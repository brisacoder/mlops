{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e282b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Get the logger for docling and set its level\n",
    "logging.getLogger('docling').setLevel(logging.INFO)\n",
    "logging.getLogger('docling_core').setLevel(logging.INFO)\n",
    "log = logging.getLogger(__name__)  # This makes your script a logging-aware application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e38cd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HTML to artifacts/doc.html\n",
      "h2: Filter replacement (page 492) - Parent H1: None\n",
      "h2: Filter replacement (page 492) - Parent H1: None\n",
      "h2: Blade replacement (page 494) - Parent H1: None\n",
      "h2: Front windshield wiper blade replacement (page 495) - Parent H1: None\n",
      "h2: Rear window wiper blade replacement (page 496) - Parent H1: None\n",
      "h2: Tire replacement (page 503) - Parent H1: None\n",
      "h2: Compact spare tire replacement (page 504) - Parent H1: None\n",
      "h2: Wheel replacement (page 504) - Parent H1: None\n",
      "h2: Instrument panel fuse replacement (page 514) - Parent H1: None\n",
      "h2: Engine compartment panel fuse replacement (page 514) - Parent H1: None\n",
      "h2: Front light replacement (page 526) - Parent H1: None\n",
      "h2: Side repeater light replacement (page 527) - Parent H1: None\n",
      "h2: Rear combination light replacement (page 527) - Parent H1: None\n",
      "h2: High mounted stop light replacement (page 528) - Parent H1: None\n",
      "h2: License plate light replacement (page 528) - Parent H1: None\n",
      "h2: Interior light replacement (page 529) - Parent H1: None\n",
      "h2: Filter replacement (page 492) - Parent H1: None\n",
      "h2: Filter replacement (page 492) - Parent H1: None\n",
      "h2: Blade replacement (page 494) - Parent H1: None\n",
      "h2: Front windshield wiper blade replacement (page 495) - Parent H1: None\n",
      "h2: Rear window wiper blade replacement (page 496) - Parent H1: None\n",
      "h2: Tire replacement (page 503) - Parent H1: None\n",
      "h2: Compact spare tire replacement (page 504) - Parent H1: None\n",
      "h2: Wheel replacement (page 504) - Parent H1: None\n",
      "h2: Instrument panel fuse replacement (page 514) - Parent H1: None\n",
      "h2: Engine compartment panel fuse replacement (page 514) - Parent H1: None\n",
      "h2: Front light replacement (page 526) - Parent H1: None\n",
      "h2: Side repeater light replacement (page 527) - Parent H1: None\n",
      "h2: Rear combination light replacement (page 527) - Parent H1: None\n",
      "h2: High mounted stop light replacement (page 528) - Parent H1: None\n",
      "h2: License plate light replacement (page 528) - Parent H1: None\n",
      "h2: Interior light replacement (page 529) - Parent H1: None\n"
     ]
    }
   ],
   "source": [
    "def find_headers_in_html(doc, html_string, word):\n",
    "    \"\"\"Find headers in HTML that contain the given word, and include page info and parent H1 from doc.\"\"\"\n",
    "    try:\n",
    "        from bs4 import BeautifulSoup\n",
    "        from docling_core.types.doc.document import SectionHeaderItem\n",
    "        soup = BeautifulSoup(html_string, 'html.parser')\n",
    "        headers = []\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            text = tag.get_text().strip()\n",
    "            if word.lower() in text.lower():\n",
    "                # Find parent H1\n",
    "                parent_h1 = None\n",
    "                current = tag\n",
    "                while current:\n",
    "                    current = current.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "                    if current and current.name == 'h1':\n",
    "                        parent_h1 = current.get_text().strip()\n",
    "                        break\n",
    "                # Find corresponding SectionHeaderItem in doc\n",
    "                page = None\n",
    "                for item in doc.texts:\n",
    "                    if isinstance(item, SectionHeaderItem) and item.text.strip() == text:\n",
    "                        prov = getattr(item, \"prov\", None)\n",
    "                        if prov:\n",
    "                            for p in prov:\n",
    "                                pg = getattr(p, \"page_no\", None)\n",
    "                                if pg is not None:\n",
    "                                    page = int(pg)\n",
    "                                    break\n",
    "                        break\n",
    "                headers.append((tag.name, text, page, parent_h1))\n",
    "        return headers\n",
    "    except ImportError:\n",
    "        print(\"BeautifulSoup not available. Install with: pip install beautifulsoup4\")\n",
    "        return []\n",
    "\n",
    "# Call the function\n",
    "html = export_doc_html(doc)\n",
    "replacement_headers = find_headers_in_html(doc, html, 'replacement')\n",
    "for level, text, page, parent_h1 in replacement_headers:\n",
    "    print(f\"{level}: {text} (page {page}) - Parent H1: {parent_h1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054b38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_semantic_similarity(headers_list, query):\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between a list of headers and a query string using TF-IDF vectors.\n",
    "    \n",
    "    Args:\n",
    "        headers_list: List of tuples (level, text, page, parent_h1) from find_headers_in_html or similar.\n",
    "        query: The query string (phrase or word).\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with 'header', 'cosine_similarity', and 'euclidean_distance'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        \n",
    "        # Extract texts from headers\n",
    "        texts = [text for _, text, _, _ in headers_list]\n",
    "        texts.append(query)\n",
    "        \n",
    "        # Vectorize using TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Query vector is the last one\n",
    "        query_vec = tfidf_matrix[-1]\n",
    "        \n",
    "        similarities = []\n",
    "        for i, header in enumerate(headers_list):\n",
    "            header_vec = tfidf_matrix[i]\n",
    "            \n",
    "            # Cosine similarity\n",
    "            cos_sim = cosine_similarity(query_vec, header_vec)[0][0]\n",
    "            \n",
    "            # Euclidean distance\n",
    "            euclidean = np.linalg.norm(query_vec.toarray() - header_vec.toarray())\n",
    "            \n",
    "            similarities.append({\n",
    "                'header': header,\n",
    "                'cosine_similarity': cos_sim,\n",
    "                'euclidean_distance': euclidean\n",
    "            })\n",
    "        \n",
    "        return similarities\n",
    "    except ImportError as e:\n",
    "        print(f\"Required libraries not available: {e}. Install scikit-learn and numpy.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75eb48ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HTML to artifacts/doc.html\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "header",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cosine_similarity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "euclidean_distance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e54a9103-9d1e-4f9c-ba35-f9956dbb0c3f",
       "rows": [
        [
         "0",
         "('h2', 'Cabin air filter', 203, None)",
         "0.6705436749433179",
         "0.81173434700853"
        ],
        [
         "1",
         "('h2', 'Cabin air filter', 203, None)",
         "0.6705436749433179",
         "0.81173434700853"
        ],
        [
         "2",
         "('h2', 'Cabin Air Filter', 493, None)",
         "0.6705436749433179",
         "0.81173434700853"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/plain": [
       "[{'header': ('h2', 'Cabin air filter', 203, None),\n",
       "  'cosine_similarity': np.float64(0.6705436749433179),\n",
       "  'euclidean_distance': np.float64(0.81173434700853)},\n",
       " {'header': ('h2', 'Cabin air filter', 203, None),\n",
       "  'cosine_similarity': np.float64(0.6705436749433179),\n",
       "  'euclidean_distance': np.float64(0.81173434700853)},\n",
       " {'header': ('h2', 'Cabin Air Filter', 493, None),\n",
       "  'cosine_similarity': np.float64(0.6705436749433179),\n",
       "  'euclidean_distance': np.float64(0.81173434700853)}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = export_doc_html(doc)\n",
    "replacement_headers = find_headers_in_html(doc, html, 'cabin air filter')\n",
    "compute_semantic_similarity(replacement_headers, 'replace cabin air filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94332cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_ref='#/texts/9550' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=492, bbox=BoundingBox(l=36.734, t=514.1930122070312, r=138.19, b=503.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 18))] orig='Filter replacement' text='Filter replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9579' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=493, bbox=BoundingBox(l=235.158, t=393.1940122070312, r=336.374, b=382.7230122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 18))] orig='Filter replacement' text='Filter replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9602' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=494, bbox=BoundingBox(l=220.985, t=276.3510122070312, r=324.841, b=265.8810122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 17))] orig='Blade replacement' text='Blade replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9611' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=495, bbox=BoundingBox(l=50.907, t=547.1660122070313, r=184.713, b=528.4450122070313, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 40))] orig='Front windshield wiper blade replacement' text='Front windshield wiper blade replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9636' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=496, bbox=BoundingBox(l=220.985, t=547.1660122070313, r=337.751, b=528.4450122070313, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 35))] orig='Rear window wiper blade replacement' text='Rear window wiper blade replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9784' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=503, bbox=BoundingBox(l=235.158, t=547.1930122070312, r=328.694, b=536.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 16))] orig='Tire replacement' text='Tire replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9798' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=504, bbox=BoundingBox(l=36.734, t=367.33301220703123, r=183.008, b=358.6110122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 30))] orig='Compact spare tire replacement' text='Compact spare tire replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9804' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=504, bbox=BoundingBox(l=220.985, t=547.1930122070312, r=328.441, b=536.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 17))] orig='Wheel replacement' text='Wheel replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10007' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=514, bbox=BoundingBox(l=36.734, t=547.1930122070312, r=159.729, b=524.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 33))] orig='Instrument panel fuse replacement' text='Instrument panel fuse replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10019' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=514, bbox=BoundingBox(l=220.985, t=448.1930122070312, r=373.019, b=425.7230122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 41))] orig='Engine compartment panel fuse replacement' text='Engine compartment panel fuse replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10108' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=526, bbox=BoundingBox(l=36.734, t=486.3510122070312, r=166.749, b=475.8800122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 23))] orig='Front light replacement' text='Front light replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10139' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=527, bbox=BoundingBox(l=235.158, t=547.1930122070312, r=338.954, b=524.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 31))] orig='Side repeater light replacement' text='Side repeater light replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10143' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=527, bbox=BoundingBox(l=235.158, t=308.6740122070312, r=362.953, b=286.2030122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 34))] orig='Rear combination light replacement' text='Rear combination light replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10154' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=528, bbox=BoundingBox(l=36.734, t=491.1930122070312, r=172.449, b=468.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 35))] orig='High mounted stop light replacement' text='High mounted stop light replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10158' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=528, bbox=BoundingBox(l=220.985, t=547.1930122070312, r=323.341, b=524.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 31))] orig='License plate light replacement' text='License plate light replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10170' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=529, bbox=BoundingBox(l=50.907, t=547.1930122070312, r=192.441, b=536.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 26))] orig='Interior light replacement' text='Interior light replacement' formatting=None hyperlink=None level=1\n"
     ]
    }
   ],
   "source": [
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "def find_headers_with_word(doc, word):\n",
    "    \"\"\"Find all SectionHeaderItem that contain the given word in their text.\"\"\"\n",
    "    matches = []\n",
    "    for text in doc.texts:\n",
    "        if isinstance(text, SectionHeaderItem):\n",
    "            if word.lower() in text.text.lower():\n",
    "                matches.append(text)\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "replacement_headers = find_headers_with_word(doc, 'replacement')\n",
    "for header in replacement_headers:\n",
    "    print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567242d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097fbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23184a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: how do i change the cabin air filter?\n"
     ]
    }
   ],
   "source": [
    "# Parameters for header-only selection\n",
    "query = \"how do i change the cabin air filter?\"  # set this per user request\n",
    "print(\"Query:\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e493e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2133 body headers.\n",
      "Indexed 1911 headers with content.\n"
     ]
    }
   ],
   "source": [
    "# Header index with spaCy lemmatization and optional WordNet synonyms\n",
    "import re\n",
    "from typing import List, Any, Tuple, Dict, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "# Load spaCy model (lightweight) and optionally WordNet\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except Exception:\n",
    "        # Try to download if missing (comment out if offline)\n",
    "        import sys, subprocess\n",
    "        subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=False)\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    nlp = None\n",
    "    print(\"spaCy not available:\", e)\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    try:\n",
    "        _ = wn.synsets(\"test\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"omw-1.4\")\n",
    "except Exception as e:\n",
    "    wn = None\n",
    "    print(\"WordNet not available:\", e)\n",
    "\n",
    "\n",
    "def is_body(x: Any) -> bool:\n",
    "    v = getattr(x, \"content_layer\", None)\n",
    "    return getattr(v, \"value\", v) == \"body\"\n",
    "\n",
    "texts: List[Any] = list(doc.texts)\n",
    "headers: List[Tuple[int, SectionHeaderItem]] = [\n",
    "    (i, t) for i, t in enumerate(texts) if isinstance(t, SectionHeaderItem) and is_body(t)\n",
    "]\n",
    "print(f\"Found {len(headers)} body headers.\")\n",
    "\n",
    "# --- Page helpers ---\n",
    "\n",
    "def item_pages(obj: Any) -> Set[int]:\n",
    "    pages: Set[int] = set()\n",
    "    prov = getattr(obj, \"prov\", None)\n",
    "    if prov:\n",
    "        for p in prov:\n",
    "            pg = getattr(p, \"page_no\", None)\n",
    "            if pg is not None:\n",
    "                try:\n",
    "                    pages.add(int(pg))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    # Fallback if no provenance: use page_ref (0-indexed in many docling builds)\n",
    "    pr = getattr(obj, \"page_ref\", None)\n",
    "    if pr is not None and not pages:\n",
    "        try:\n",
    "            pages.add(int(pr) + 1)\n",
    "        except Exception:\n",
    "            pages.add(1)\n",
    "    return pages\n",
    "\n",
    "\n",
    "def nodes_pages(nodes: List[Any]) -> Set[int]:\n",
    "    ps: Set[int] = set()\n",
    "    for n in nodes:\n",
    "        ps |= item_pages(n)\n",
    "    return ps\n",
    "\n",
    "# Slice a section and filter out empty/TOC\n",
    "\n",
    "def slice_nodes(i: int) -> Tuple[SectionHeaderItem, List[Any]]:\n",
    "    h = texts[i]\n",
    "    lvl = getattr(h, \"level\", 3)\n",
    "    nodes = []\n",
    "    for j in range(i + 1, len(texts)):\n",
    "        t = texts[j]\n",
    "        if not is_body(t):\n",
    "            continue\n",
    "        if isinstance(t, SectionHeaderItem) and getattr(t, \"level\", 3) <= lvl:\n",
    "            break\n",
    "        nodes.append(t)\n",
    "    return h, nodes\n",
    "\n",
    "\n",
    "def has_content(nodes: List[Any]) -> bool:\n",
    "    textish = 0\n",
    "    structural = 0\n",
    "    for n in nodes:\n",
    "        name = n.__class__.__name__.lower()\n",
    "        if hasattr(n, \"text\") and name != \"sectionheaderitem\":\n",
    "            if re.search(r\"\\w\", getattr(n, \"text\", \"\") or \"\"):\n",
    "                textish += 1\n",
    "        if hasattr(n, \"items\") or hasattr(n, \"num_rows\") or hasattr(n, \"caption\"):\n",
    "            structural += 1\n",
    "    return textish >= 1 or structural >= 1\n",
    "\n",
    "# Build header index with POS-tagged lemmas and document pages\n",
    "IndexItem = Dict[str, Any]\n",
    "index: List[IndexItem] = []\n",
    "\n",
    "for i, h in headers:\n",
    "    title = getattr(h, \"text\", \"\") or \"\"\n",
    "    header_ps = item_pages(h)\n",
    "    nodes = slice_nodes(i)[1]\n",
    "    if not has_content(nodes):\n",
    "        continue\n",
    "    section_ps = nodes_pages(nodes)\n",
    "\n",
    "    nouns: Set[str] = set()\n",
    "    verbs: Set[str] = set()\n",
    "\n",
    "    if nlp is not None:\n",
    "        doc_h = nlp(title)\n",
    "        for tok in doc_h:\n",
    "            if tok.is_stop or not tok.is_alpha:\n",
    "                continue\n",
    "            lemma = tok.lemma_.lower()\n",
    "            if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "                nouns.add(lemma)\n",
    "            elif tok.pos_ in (\"VERB\",):\n",
    "                verbs.add(lemma)\n",
    "    else:\n",
    "        # Fallback: simple regex tokenization, no POS\n",
    "        for t in re.findall(r\"\\b[\\w-]+\\b\", title.lower()):\n",
    "            if len(t) >= 2:\n",
    "                nouns.add(t)\n",
    "\n",
    "    # Optional synonym expansion via WordNet\n",
    "    syns_n: Set[str] = set()\n",
    "    syns_v: Set[str] = set()\n",
    "    if wn is not None:\n",
    "        for n in nouns:\n",
    "            for s in wn.synsets(n, pos=wn.NOUN):\n",
    "                for l in s.lemma_names():\n",
    "                    syns_n.add(l.replace(\"_\", \" \").lower())\n",
    "        for v in verbs:\n",
    "            for s in wn.synsets(v, pos=wn.VERB):\n",
    "                for l in s.lemma_names():\n",
    "                    syns_v.add(l.replace(\"_\", \" \").lower())\n",
    "\n",
    "    index.append({\n",
    "        \"i\": i,\n",
    "        # First page where the header is located (doc page numbers)\n",
    "        \"header_pages\": sorted(header_ps),\n",
    "        # Pages covered by the section content\n",
    "        \"section_pages\": sorted(section_ps),\n",
    "        # Convenience union of all pages for this section (header + content)\n",
    "        \"doc_pages\": sorted((header_ps | section_ps)),\n",
    "        \"level\": getattr(h, \"level\", 3),\n",
    "        \"title\": title,\n",
    "        \"nouns\": nouns,\n",
    "        \"verbs\": verbs,\n",
    "        \"syn_nouns\": syns_n,\n",
    "        \"syn_verbs\": syns_v,\n",
    "    })\n",
    "\n",
    "print(f\"Indexed {len(index)} headers with content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2517245e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top headers:\n",
      " 1. p 203 h1 score=0.580 | noun=1.00 verb=0.00 n_syn=1.00 v_syn=0.00 fuzz=0.60 :: Cabin air filter  [pages: 203]\n",
      " 2. p 212 h1 score=0.580 | noun=1.00 verb=0.00 n_syn=1.00 v_syn=0.00 fuzz=0.60 :: Cabin air filter  [pages: 212]\n",
      " 3. p 483 h1 score=0.433 | noun=0.67 verb=0.00 n_syn=0.94 v_syn=0.00 fuzz=0.51 :: Air cleaner filter  [pages: 483]\n",
      " 4. p 321 h1 score=0.421 | noun=0.00 verb=1.00 n_syn=0.00 v_syn=1.00 fuzz=0.42 :: · Changing lanes  [pages: 321,322]\n",
      " 5. p 374 h1 score=0.421 | noun=0.00 verb=1.00 n_syn=0.00 v_syn=1.00 fuzz=0.42 :: · Changing lanes  [pages: 374]\n",
      "Chosen: doc pages [203] 'Cabin air filter' score=0.580\n",
      "Saved header_syn_lemma_best_section.html\n"
     ]
    }
   ],
   "source": [
    "# Rank headers using query nouns/verbs + WordNet synonyms; pick best and save section\n",
    "from html import escape\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "\n",
    "# Extract query nouns/verbs via spaCy (or fallback)\n",
    "q_nouns, q_verbs = set(), set()\n",
    "q_text = query\n",
    "if 'nlp' in globals() and nlp is not None:\n",
    "    qdoc = nlp(q_text)\n",
    "    for tok in qdoc:\n",
    "        if tok.is_stop or not tok.is_alpha:\n",
    "            continue\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            q_nouns.add(lemma)\n",
    "        elif tok.pos_ in (\"VERB\",):\n",
    "            q_verbs.add(lemma)\n",
    "else:\n",
    "    for t in re.findall(r\"\\b[\\w-]+\\b\", q_text.lower()):\n",
    "        if len(t) >= 2:\n",
    "            q_nouns.add(t)\n",
    "\n",
    "# Expand query with WordNet if available\n",
    "q_syn_n, q_syn_v = set(), set()\n",
    "if 'wn' in globals() and wn is not None:\n",
    "    for n in q_nouns:\n",
    "        for s in wn.synsets(n, pos=wn.NOUN):\n",
    "            for l in s.lemma_names():\n",
    "                q_syn_n.add(l.replace(\"_\", \" \").lower())\n",
    "    for v in q_verbs:\n",
    "        for s in wn.synsets(v, pos=wn.VERB):\n",
    "            for l in s.lemma_names():\n",
    "                q_syn_v.add(l.replace(\"_\", \" \").lower())\n",
    "\n",
    "# Score headers: prioritize noun/verb coverage, then synonyms, then fuzzy title\n",
    "cands = []\n",
    "for h in index:\n",
    "    hn = h['nouns']; hv = h['verbs']\n",
    "    syn_n = h['syn_nouns']; syn_v = h['syn_verbs']\n",
    "\n",
    "    # Coverage\n",
    "    noun_cov = len(q_nouns & hn) / max(1, len(q_nouns))\n",
    "    verb_cov = len(q_verbs & hv) / max(1, len(q_verbs))\n",
    "\n",
    "    # Synonym coverage\n",
    "    noun_syn_cov = len(q_syn_n & (hn | syn_n)) / max(1, len(q_syn_n)) if q_syn_n else 0.0\n",
    "    verb_syn_cov = len(q_syn_v & (hv | syn_v)) / max(1, len(q_syn_v)) if q_syn_v else 0.0\n",
    "\n",
    "    fuzzy = SequenceMatcher(None, q_text.lower(), h['title'].lower()).ratio()\n",
    "\n",
    "    score = (\n",
    "        0.40 * noun_cov +\n",
    "        0.30 * verb_cov +\n",
    "        0.15 * noun_syn_cov +\n",
    "        0.10 * verb_syn_cov +\n",
    "        0.05 * fuzzy\n",
    "    )\n",
    "\n",
    "    # Derive a representative document page for display: first page where this section appears\n",
    "    # Prefer header page if available, else earliest section content page\n",
    "    doc_pages = h.get('doc_pages', [])\n",
    "    first_doc_page = doc_pages[0] if doc_pages else (h.get('header_pages') or h.get('section_pages') or [1])[0]\n",
    "\n",
    "    cands.append({**h, 'score': score, 'fuzzy': fuzzy,\n",
    "                  'noun_cov': noun_cov, 'verb_cov': verb_cov,\n",
    "                  'noun_syn_cov': noun_syn_cov, 'verb_syn_cov': verb_syn_cov,\n",
    "                  'first_doc_page': first_doc_page})\n",
    "\n",
    "cands.sort(key=lambda x: x['score'], reverse=True)\n",
    "print(\"Top headers:\")\n",
    "for r, c in enumerate(cands[:5], start=1):\n",
    "    # Show the full list of document pages for each candidate\n",
    "    pages_str = ','.join(str(p) for p in c.get('doc_pages', []) or c.get('header_pages', []) or c.get('section_pages', []) or [\"?\"])\n",
    "    print(f\"{r:>2}. p{c['first_doc_page']:>4} h{c['level']} score={c['score']:.3f} | noun={c['noun_cov']:.2f} verb={c['verb_cov']:.2f} n_syn={c['noun_syn_cov']:.2f} v_syn={c['verb_syn_cov']:.2f} fuzz={c['fuzzy']:.2f} :: {c['title']}  [pages: {pages_str}]\")\n",
    "\n",
    "best = cands[0]\n",
    "print(f\"Chosen: doc pages {best.get('doc_pages', best.get('header_pages', best.get('section_pages', ['?'])))} '{best['title']}' score={best['score']:.3f}\")\n",
    "\n",
    "# Slice and save rendered HTML\n",
    "h, nodes = slice_nodes(best['i'])\n",
    "\n",
    "# Augment HTML header with page info\n",
    "page_info = ', '.join(str(p) for p in best.get('doc_pages') or best.get('header_pages') or best.get('section_pages') or [])\n",
    "\n",
    "\n",
    "def render_nodes(nodes: List[Any]) -> str:\n",
    "    parts = []\n",
    "    for n in nodes:\n",
    "        name = n.__class__.__name__.lower()\n",
    "        if hasattr(n, 'text') and name != 'sectionheaderitem':\n",
    "            t = getattr(n, 'text', '')\n",
    "            if t:\n",
    "                parts.append(f\"<p>{escape(t)}</p>\")\n",
    "        elif hasattr(n, 'items'):\n",
    "            parts.append('<ul>')\n",
    "            for it in getattr(n, 'items', []) or []:\n",
    "                parts.append(f\"<li>{escape(getattr(it, 'text', str(it)) or '')}</li>\")\n",
    "            parts.append('</ul>')\n",
    "        elif hasattr(n, 'num_rows') and hasattr(n, 'num_cols') and hasattr(n, 'cells'):\n",
    "            rows = []\n",
    "            for r in range(n.num_rows):\n",
    "                cells = []\n",
    "                for c in range(n.num_cols):\n",
    "                    cell = n.cells[r][c]\n",
    "                    cells.append(f\"<td>{escape(getattr(cell, 'text', '') or '')}</td>\")\n",
    "                rows.append('<tr>' + ''.join(cells) + '</tr>')\n",
    "            parts.append('<table>' + ''.join(rows) + '</table>')\n",
    "        elif hasattr(n, 'caption'):\n",
    "            cap = getattr(getattr(n, 'caption', None), 'text', '') or ''\n",
    "            if cap:\n",
    "                parts.append(f\"<figure><figcaption>{escape(cap)}</figcaption></figure>\")\n",
    "    return '\\n'.join(parts)\n",
    "\n",
    "html_out = f\"\"\"\n",
    "<!doctype html>\n",
    "<html><head><meta charset='utf-8'><title>{escape(best['title'])}</title>\n",
    "<style>body{{font-family:Segoe UI, Roboto, Arial, sans-serif; line-height:1.5; padding:1rem}} ul{{margin-left:1.25rem}}</style>\n",
    "</head><body>\n",
    "<h2>{escape(best['title'])}</h2>\n",
    "<p><em>Document pages: {escape(page_info)}</em></p>\n",
    "{render_nodes(nodes)}\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "out_dir = Path('artifacts/sections')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "(out_dir / 'header_syn_lemma_best_section.html').write_text(html_out, encoding='utf-8')\n",
    "print('Saved header_syn_lemma_best_section.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8cde0e",
   "metadata": {},
   "source": [
    "# Subtree-based selection: rank a section together with its subsections\n",
    "\n",
    "This section adds a minimal, reusable, and PEP8-compliant utility layer to:\n",
    "\n",
    "- Build a header tree from `doc.texts` (parent/children relationships by header levels)\n",
    "- Compute page coverage using provenance (preferred) and page_ref fallback\n",
    "- Extract linguistic features (noun/verb lemmas and optional WordNet synonyms) for titles\n",
    "- Construct a subtree index (parent + immediate children merged semantics)\n",
    "- Rank subtrees against a query and explore the top candidates with pages and child headers\n",
    "\n",
    "Outputs:\n",
    "- Console summary of the top subtree candidates (parent + children, doc pages, coverage)\n",
    "- Saved HTML for the best subtree slice: `artifacts/sections/subtree_best_section.html`\n",
    "\n",
    "Design notes:\n",
    "- We keep this independent from existing cells. Functions are documented and PEP8-compliant.\n",
    "- We limit subtree semantics to parent + immediate children to stay fast and deterministic.\n",
    "- Rendering uses the existing structural slice: from the parent header until the next header of the same or higher level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc45806",
   "metadata": {},
   "outputs": [],
   "source": [
    "    node = HeaderNode(index=i, level=level, title=title, parent=parent, children=[], header_pages=header_pages, section_pages=list(set().union(*section_pages)), doc_pages=doc_pages)\n",
    "    node.nodes = nodes\n",
    "    nodes_map[i] = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ab8f241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top subtrees (parent + immediate children):\n",
      " 1. p203 h1 score=0.589 :: Cabin air filter\n",
      "    subtree noun matches: air, cabin, filter\n",
      " 2. p212 h1 score=0.589 :: Cabin air filter\n",
      "    subtree noun matches: air, cabin, filter\n",
      " 3. p493 h1 score=0.589 :: Cabin Air Filter\n",
      "    subtree noun matches: air, cabin, filter\n",
      " 4. p483 h1 score=0.408 :: Air cleaner filter\n",
      "    subtree noun matches: air, filter\n",
      " 5. p139 h1 score=0.376 :: Replacing the battery\n",
      "    subtree noun matches: battery\n",
      "    subtree verb matches: replace\n",
      "Saved subtree_best_section.html\n"
     ]
    }
   ],
   "source": [
    "# Minimal runner for subtree selection and action bump\n",
    "from pathlib import Path\n",
    "\n",
    "def run_subtree_selection(query: str, top_k: int = 5, out_dir: Path = Path(\"artifacts/sections\"), out_name: str = \"subtree_best_section.html\"):\n",
    "    tree = build_header_tree(doc)\n",
    "    subtrees = build_subtree_index(tree)\n",
    "    scored = [(score_subtree(query, st), st) for st in subtrees]\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    print(\"Top subtrees (parent + immediate children):\")\n",
    "    for rank, (score, st) in enumerate(scored[:top_k], start=1):\n",
    "        pages_str = \",\".join(str(p) for p in st[\"doc_pages\"])\n",
    "        print(f\"{rank:>2}. p{pages_str} h{st['level']} score={score:.3f} :: {st['title']}\")\n",
    "        if st[\"child_titles\"]:\n",
    "            print(f\"    child titles: {st['child_titles']}\")\n",
    "        pn = \", \".join(sorted(st[\"subtree_nouns\"]))\n",
    "        pv = \", \".join(sorted(st[\"subtree_verbs\"]))\n",
    "        if pn:\n",
    "            print(f\"    subtree noun matches: {pn}\")\n",
    "        if pv:\n",
    "            print(f\"    subtree verb matches: {pv}\")\n",
    "        if st[\"child_action_hits\"]:\n",
    "            print(f\"    child action hits: {st['child_action_hits']}\")\n",
    "    # Save best subtree\n",
    "    if scored:\n",
    "        best_idx = scored[0][1][\"idx\"]\n",
    "        out_path = out_dir / out_name\n",
    "        save_best_subtree_html(tree, best_idx, out_path)\n",
    "        print(f\"Saved {out_path.name}\")\n",
    "\n",
    "# Example runner\n",
    "if 'query' in globals():\n",
    "    run_subtree_selection(query=query, top_k=5)\n",
    "else:\n",
    "    print(\"Set a `query variable (str) and re-run this cell to execute subtree selection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8ca9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I replace the cabin air filter?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
