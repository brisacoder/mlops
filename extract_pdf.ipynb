{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "e282b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Get the logger for docling and set its level\n",
    "logging.getLogger('docling').setLevel(logging.INFO)\n",
    "logging.getLogger('docling_core').setLevel(logging.INFO)\n",
    "log = logging.getLogger(__name__)  # This makes your script a logging-aware application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d48bd831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing current search for 'cabin air filter replacement':\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sections_with_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[287]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting current search for \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcabin air filter replacement\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msimple_rag_search\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     test_results = simple_rag_search(\u001b[43msections_with_embeddings\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mcabin air filter replacement\u001b[39m\u001b[33m\"\u001b[39m, top_n=\u001b[32m5\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_results, \u001b[32m1\u001b[39m):\n\u001b[32m      6\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33msimilarity_score\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sections_with_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Quick test of current search results\n",
    "print(\"Testing current search for 'cabin air filter replacement':\")\n",
    "if 'simple_rag_search' in globals():\n",
    "    test_results = simple_rag_search(sections_with_embeddings, \"cabin air filter replacement\", top_n=5)\n",
    "    for i, result in enumerate(test_results, 1):\n",
    "        print(f\"#{i}: {result['header']} (Score: {result['similarity_score']:.3f})\")\n",
    "        print(f\"   Content preview: {result['content'][:100]}...\")\n",
    "        print(f\"   Pages: {result.get('pages', 'N/A')}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"simple_rag_search function not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_headers_in_html(doc, html_string, word):\n",
    "    \"\"\"Find headers in HTML that contain the given word, and include page info and parent H1 from doc.\"\"\"\n",
    "    try:\n",
    "        from bs4 import BeautifulSoup\n",
    "        from docling_core.types.doc.document import SectionHeaderItem\n",
    "        soup = BeautifulSoup(html_string, 'html.parser')\n",
    "        headers = []\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            text = tag.get_text().strip()\n",
    "            if word.lower() in text.lower():\n",
    "                # Find parent H1\n",
    "                parent_h1 = None\n",
    "                current = tag\n",
    "                while current:\n",
    "                    current = current.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "                    if current and current.name == 'h1':\n",
    "                        parent_h1 = current.get_text().strip()\n",
    "                        break\n",
    "                # Find corresponding SectionHeaderItem in doc\n",
    "                page = None\n",
    "                for item in doc.texts:\n",
    "                    if isinstance(item, SectionHeaderItem) and item.text.strip() == text:\n",
    "                        prov = getattr(item, \"prov\", None)\n",
    "                        if prov:\n",
    "                            for p in prov:\n",
    "                                pg = getattr(p, \"page_no\", None)\n",
    "                                if pg is not None:\n",
    "                                    page = int(pg)\n",
    "                                    break\n",
    "                        break\n",
    "                headers.append((tag.name, text, page, parent_h1))\n",
    "        return headers\n",
    "    except ImportError:\n",
    "        print(\"BeautifulSoup not available. Install with: pip install beautifulsoup4\")\n",
    "        return []\n",
    "\n",
    "# Call the function\n",
    "html = export_doc_html(doc)\n",
    "replacement_headers = find_headers_in_html(doc, html, 'replacement')\n",
    "for level, text, page, parent_h1 in replacement_headers:\n",
    "    print(f\"{level}: {text} (page {page}) - Parent H1: {parent_h1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "da9b0029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1277 items from data/temp_chunk_0-91_kona.json\n",
      "Loaded 1872 items from data/temp_chunk_92-183_kona.json\n",
      "Loaded 1942 items from data/temp_chunk_184-275_kona.json\n",
      "Loaded 1806 items from data/temp_chunk_276-367_kona.json\n",
      "Loaded 1942 items from data/temp_chunk_184-275_kona.json\n",
      "Loaded 1806 items from data/temp_chunk_276-367_kona.json\n",
      "Loaded 2110 items from data/temp_chunk_368-459_kona.json\n",
      "Loaded 1407 items from data/temp_chunk_460-551_kona.json\n",
      "Total items across all chunks: 10414\n",
      "Created document with 10414 total items\n",
      "Loaded 2110 items from data/temp_chunk_368-459_kona.json\n",
      "Loaded 1407 items from data/temp_chunk_460-551_kona.json\n",
      "Total items across all chunks: 10414\n",
      "Created document with 10414 total items\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from docling_core.types.doc.document import DoclingDocument\n",
    "\n",
    "# Load all document chunks and concatenate them\n",
    "chunk_paths = [\n",
    "    r'data/temp_chunk_0-91_kona.json',\n",
    "    r'data/temp_chunk_92-183_kona.json', \n",
    "    r'data/temp_chunk_184-275_kona.json',\n",
    "    r'data/temp_chunk_276-367_kona.json',\n",
    "    r'data/temp_chunk_368-459_kona.json',\n",
    "    r'data/temp_chunk_460-551_kona.json'\n",
    "]\n",
    "\n",
    "all_texts = []\n",
    "for chunk_path in chunk_paths:\n",
    "    try:\n",
    "        chunk = Path(chunk_path)\n",
    "        if chunk.exists():\n",
    "            chunk_doc = DoclingDocument.load_from_json(chunk)\n",
    "            all_texts.extend(chunk_doc.texts)\n",
    "            print(f\"Loaded {len(chunk_doc.texts)} items from {chunk_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {chunk_path}: {e}\")\n",
    "\n",
    "print(f\"Total items across all chunks: {len(all_texts)}\")\n",
    "\n",
    "# Create a simple document-like object with all texts\n",
    "class SimpleDocument:\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "doc = SimpleDocument(all_texts)\n",
    "print(f\"Created document with {len(doc.texts)} total items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eefd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "def compute_header_similarity(doc, query, top_n=5):\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between all headers in a DoclingDocument and a query string using TF-IDF vectors.\n",
    "Args:\n",
    "    doc: DoclingDocument object containing the document.\n",
    "    query: The query string (phrase or word).\n",
    "    top_n: Number of top similar headers to return (default: 5).\n",
    "\n",
    "Returns:\n",
    "    List of dicts with 'header_text', 'similarity_score', 'header_item', and 'page' for the top-N headers.\n",
    "    Each dict contains:\n",
    "    - 'header_text': The text of the header.\n",
    "    - 'similarity_score': Cosine similarity score (0-1).\n",
    "    - 'header_item': The SectionHeaderItem object.\n",
    "    - 'page': The page number where the header appears (if available).\n",
    "\"\"\"\n",
    "    try:\n",
    "        # Extract all headers from the document\n",
    "        headers = []\n",
    "        for item in doc.texts:\n",
    "            if isinstance(item, SectionHeaderItem):\n",
    "                # Get page info\n",
    "                page = None\n",
    "                if hasattr(item, 'prov') and item.prov:\n",
    "                    for p in item.prov:\n",
    "                        pg = getattr(p, 'page_no', None)\n",
    "                        if pg is not None:\n",
    "                            page = int(pg)\n",
    "                            break\n",
    "                \n",
    "                headers.append({\n",
    "                    'text': item.text,\n",
    "                    'item': item,\n",
    "                    'page': page\n",
    "                })\n",
    "        \n",
    "        if not headers:\n",
    "            return []\n",
    "        \n",
    "        # Extract texts for vectorization\n",
    "        texts = [h['text'] for h in headers]\n",
    "        texts.append(query)\n",
    "        \n",
    "        # Vectorize using TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Query vector is the last one\n",
    "        query_vec = tfidf_matrix[-1]\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = []\n",
    "        for i, header in enumerate(headers):\n",
    "            header_vec = tfidf_matrix[i]\n",
    "            \n",
    "            # Cosine similarity\n",
    "            cos_sim = cosine_similarity(query_vec, header_vec)[0][0]\n",
    "            \n",
    "            similarities.append({\n",
    "                'header_text': header['text'],\n",
    "                'similarity_score': cos_sim,\n",
    "                'header_item': header['item'],\n",
    "                'page': header['page']\n",
    "            })\n",
    "        \n",
    "        # Sort by similarity score descending and return top-N\n",
    "        similarities.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"Required libraries not available: {e}. Install scikit-learn and numpy.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing similarity: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where can I find the VIN?\"\n",
    "compute_header_similarity(doc, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce62b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Any, Tuple, Dict, Set\n",
    "from docling_core.types.doc.document import SectionHeaderItem, DoclingDocument\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from html import escape\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def rank_and_save_best_section_with_hdbscan(doc: DoclingDocument, query: str, top_n: int = 5, hdbscan_results: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n",
    "    print(f\"FUNCTION_START: rank_and_save_best_section_with_hdbscan called with query: '{query}'\")\n",
    "    \"\"\"\n",
    "    Rank headers in a DoclingDocument using query nouns/verbs + WordNet synonyms + HDBSCAN cluster similarity, and return top N matches.\n",
    "    Rank headers in a DoclingDocument using query nouns/verbs + WordNet synonyms + HDBSCAN cluster similarity, and return top N matches.\n",
    "    \n",
    "    This function processes the document to extract headers, analyzes their linguistic features (nouns, verbs, synonyms),\n",
    "    scores them against the query based on coverage, fuzzy matching, and cluster similarity, and returns the top N ranked headers with all data.\n",
    "    \n",
    "    Args:\n",
    "        doc (DoclingDocument): The document object containing the text elements to process.\n",
    "        query (str): The query string to match against header titles.\n",
    "        top_n (int, optional): Number of top matches to return. Defaults to 5.\n",
    "        hdbscan_results (Dict[str, Any], optional): Precomputed HDBSCAN results from perform_hdbscan_on_headers. If None, clustering is skipped.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of dictionaries for the top N headers, each containing:\n",
    "            - 'title': Header text\n",
    "            - 'score': Matching score (0-1)\n",
    "            - 'first_doc_page': First page number\n",
    "            - 'doc_pages': List of all page numbers\n",
    "            - 'level': Header level\n",
    "            - 'nouns': Set of nouns in header\n",
    "            - 'verbs': Set of verbs in header\n",
    "            - 'syn_nouns': Set of synonym nouns\n",
    "            - 'syn_verbs': Set of synonym verbs\n",
    "            - 'noun_cov': Noun coverage score\n",
    "            - 'verb_cov': Verb coverage score\n",
    "            - 'noun_syn_cov': Synonym noun coverage\n",
    "            - 'verb_syn_cov': Synonym verb coverage\n",
    "            - 'fuzzy': Fuzzy match score\n",
    "            - 'cluster_sim': Cluster similarity score (if hdbscan_results provided)\n",
    "    \n",
    "    Raises:\n",
    "        Any exceptions from spaCy, NLTK, or file operations are not caught and will propagate.\n",
    "    \n",
    "    Example:\n",
    "        >>> from pathlib import Path\n",
    "        >>> from docling_core.types.doc.document import DoclingDocument\n",
    "        >>> doc = DoclingDocument.load_from_json(Path(\"data/temp_chunk_0-91_kona.json\"))\n",
    "        >>> header_data = extract_header_texts(doc)\n",
    "        >>> hdbscan_results = perform_hdbscan_on_headers(header_data)\n",
    "        >>> results = rank_and_save_best_section_with_hdbscan(doc, \"Where can I find the VIN?\", top_n=3, hdbscan_results=hdbscan_results)\n",
    "        >>> for r in results:\n",
    "        ...     print(f\"{r['title']} (page {r['first_doc_page']}) - Score: {r['score']:.3f}\")\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG: Function called with query: '{query}'\")\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    # Download WordNet if needed\n",
    "    # nltk.download(\"wordnet\")\n",
    "    # nltk.download(\"omw-1.4\")\n",
    "    \n",
    "    def is_body(x: Any) -> bool:\n",
    "        \"\"\"Check if the text element belongs to the body content layer.\"\"\"\n",
    "        v = getattr(x, \"content_layer\", None)\n",
    "        return getattr(v, \"value\", v) == \"body\"\n",
    "    \n",
    "    texts: List[Any] = list(doc.texts)\n",
    "    headers: List[Tuple[int, SectionHeaderItem]] = [\n",
    "        (i, t) for i, t in enumerate(texts) if isinstance(t, SectionHeaderItem) and is_body(t)\n",
    "    ]\n",
    "    \n",
    "    def item_pages(obj: Any) -> Set[int]:\n",
    "        \"\"\"Extract page numbers from a document object using provenance or fallback.\"\"\"\n",
    "        pages: Set[int] = set()\n",
    "        prov = getattr(obj, \"prov\", None)\n",
    "        if prov:\n",
    "            for p in prov:\n",
    "                pg = getattr(p, \"page_no\", None)\n",
    "                if pg is not None:\n",
    "                    try:\n",
    "                        pages.add(int(pg))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        pr = getattr(obj, \"page_ref\", None)\n",
    "        if pr is not None and not pages:\n",
    "            try:\n",
    "                pages.add(int(pr) + 1)\n",
    "            except Exception:\n",
    "                pages.add(1)\n",
    "        return pages\n",
    "    \n",
    "    def nodes_pages(nodes: List[Any]) -> Set[int]:\n",
    "        \"\"\"Collect all unique page numbers from a list of nodes.\"\"\"\n",
    "        ps: Set[int] = set()\n",
    "        for n in nodes:\n",
    "            ps |= item_pages(n)\n",
    "        return ps\n",
    "    \n",
    "    def slice_nodes(i: int) -> Tuple[SectionHeaderItem, List[Any]]:\n",
    "        \"\"\"Slice the document to get the section content under a header.\"\"\"\n",
    "        h = texts[i]\n",
    "        lvl = getattr(h, \"level\", 3)\n",
    "        nodes = []\n",
    "        for j in range(i + 1, len(texts)):\n",
    "            t = texts[j]\n",
    "            if not is_body(t):\n",
    "                continue\n",
    "            if isinstance(t, SectionHeaderItem) and getattr(t, \"level\", 3) <= lvl:\n",
    "                break\n",
    "            nodes.append(t)\n",
    "        return h, nodes\n",
    "    \n",
    "    def has_content(nodes: List[Any]) -> bool:\n",
    "        \"\"\"Check if the section nodes contain meaningful content.\"\"\"\n",
    "        textish = 0\n",
    "        structural = 0\n",
    "        for n in nodes:\n",
    "            name = n.__class__.__name__.lower()\n",
    "            if hasattr(n, \"text\") and name != \"sectionheaderitem\":\n",
    "                if re.search(r\"\\w\", getattr(n, \"text\", \"\") or \"\"):\n",
    "                    textish += 1\n",
    "            if hasattr(n, \"items\") or hasattr(n, \"num_rows\") or hasattr(n, \"caption\"):\n",
    "                structural += 1\n",
    "        return textish >= 1 or structural >= 1\n",
    "    \n",
    "    # Build header index\n",
    "    IndexItem = Dict[str, Any]\n",
    "    index: List[IndexItem] = []\n",
    "    \n",
    "    print(f\"DEBUG: Processing {len(headers)} headers\")\n",
    "    filter_count = sum(1 for _, h in headers if 'filter' in getattr(h, 'text', '').lower())\n",
    "    print(f\"DEBUG: Found {filter_count} filter-related headers in raw headers list\")\n",
    "    \n",
    "    for i, h in headers:\n",
    "        title = getattr(h, \"text\", \"\") or \"\"\n",
    "        if 'filter' in title.lower():\n",
    "            print(f\"DEBUG: Raw header {i}: '{title}'\")\n",
    "        title = getattr(h, \"text\", \"\") or \"\"\n",
    "        print(f\"DEBUG: Processing header: '{title}'\")\n",
    "        header_ps = item_pages(h)\n",
    "        nodes = slice_nodes(i)[1]\n",
    "        if not has_content(nodes):\n",
    "            continue\n",
    "        section_ps = nodes_pages(nodes)\n",
    "        \n",
    "        nouns: Set[str] = set()\n",
    "        verbs: Set[str] = set()\n",
    "        \n",
    "        doc_h = nlp(title)\n",
    "        for tok in doc_h:\n",
    "            if tok.is_stop or not tok.is_alpha:\n",
    "                continue\n",
    "            lemma = tok.lemma_.lower()\n",
    "            if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "                nouns.add(lemma)\n",
    "            elif tok.pos_ in (\"VERB\",):\n",
    "                verbs.add(lemma)\n",
    "        \n",
    "        syns_n: Set[str] = set()\n",
    "        syns_v: Set[str] = set()\n",
    "        for n in nouns:\n",
    "            for s in wn.synsets(n, pos=wn.NOUN):\n",
    "                if hasattr(s, 'lemma_names'):\n",
    "                    for l in s.lemma_names():\n",
    "                        syns_n.add(l.replace(\"_\", \" \").lower())\n",
    "        for v in verbs:\n",
    "            for s in wn.synsets(v, pos=wn.VERB):\n",
    "                if hasattr(s, 'lemma_names'):\n",
    "                    for l in s.lemma_names():\n",
    "                        syns_v.add(l.replace(\"_\", \" \").lower())\n",
    "        \n",
    "        index.append({\n",
    "            \"i\": i,\n",
    "            \"header_pages\": sorted(header_ps),\n",
    "            \"section_pages\": sorted(section_ps),\n",
    "            \"doc_pages\": sorted((header_ps | section_ps)),\n",
    "            \"level\": getattr(h, \"level\", 3),\n",
    "            \"title\": title,\n",
    "            \"nouns\": nouns,\n",
    "            \"verbs\": verbs,\n",
    "            \"syn_nouns\": syns_n,\n",
    "            \"syn_verbs\": syns_v,\n",
    "        })\n",
    "    \n",
    "    print(f\"DEBUG: Built index with {len(index)} headers\")\n",
    "    filter_headers_in_index = [h for h in index if 'filter' in h['title'].lower()]\n",
    "    print(f\"DEBUG: Filter headers in index: {len(filter_headers_in_index)}\")\n",
    "    for h in filter_headers_in_index:\n",
    "        print(f\"  - '{h['title']}' at index {h['i']}\")\n",
    "    \n",
    "    # Extract query nouns/verbs\n",
    "    q_nouns, q_verbs = set(), set()\n",
    "    q_text = query\n",
    "    qdoc = nlp(q_text)\n",
    "    for tok in qdoc:\n",
    "        if tok.is_stop or not tok.is_alpha:\n",
    "            continue\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            q_nouns.add(lemma)\n",
    "        elif tok.pos_ in (\"VERB\",):\n",
    "            q_verbs.add(lemma)\n",
    "    \n",
    "    # Expand query with WordNet - ENHANCED: Multi-stage synonym expansion\n",
    "    q_syn_n, q_syn_v = set(), set()\n",
    "\n",
    "    # Get query embedding for similarity filtering\n",
    "    query_doc = nlp(q_text)\n",
    "    query_vector = query_doc.vector\n",
    "\n",
    "    def get_expanded_synonyms(word: str, pos: str, top_k: int = 8) -> Set[str]:\n",
    "        \"\"\"Get expanded synonyms including related concepts.\"\"\"\n",
    "        synonyms = set()\n",
    "\n",
    "        # Stage 1: Direct WordNet synonyms\n",
    "        synsets = wn.synsets(word, pos=pos)\n",
    "        for synset in synsets:\n",
    "            if hasattr(synset, 'lemma_names'):\n",
    "                for lemma in synset.lemma_names():\n",
    "                    lemma_clean = lemma.replace(\"_\", \" \").lower()\n",
    "                    if lemma_clean != word.lower():\n",
    "                        synonyms.add(lemma_clean)\n",
    "\n",
    "        # Stage 2: Semantic filtering with broader threshold\n",
    "        candidate_synonyms = []\n",
    "        for syn in synonyms:\n",
    "            try:\n",
    "                lemma_doc = nlp(syn)\n",
    "                if lemma_doc.has_vector:\n",
    "                    similarity = cosine_similarity([query_vector], [lemma_doc.vector])[0][0]\n",
    "                    candidate_synonyms.append((syn, similarity))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Sort by similarity and take more candidates with lower threshold\n",
    "        candidate_synonyms.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_synonyms = [syn for syn, sim in candidate_synonyms[:top_k] if sim > 0.2]  # Lower threshold\n",
    "\n",
    "        # Stage 3: Add maintenance/procedure related terms for \"how to\" queries\n",
    "        if any(word in q_text.lower() for word in ['how', 'change', 'replace', 'install', 'remove']):\n",
    "            maintenance_terms = ['maintenance', 'service', 'repair', 'replacement', 'inspection', 'check']\n",
    "            for term in maintenance_terms:\n",
    "                try:\n",
    "                    term_doc = nlp(term)\n",
    "                    if term_doc.has_vector:\n",
    "                        similarity = cosine_similarity([query_vector], [term_doc.vector])[0][0]\n",
    "                        if similarity > 0.3:\n",
    "                            top_synonyms.append(term)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        return set(top_synonyms)\n",
    "\n",
    "    # Get expanded synonyms for nouns\n",
    "    for n in q_nouns:\n",
    "        expanded_syns = get_expanded_synonyms(n, wn.NOUN)\n",
    "        q_syn_n.update(expanded_syns)\n",
    "\n",
    "    # Get expanded synonyms for verbs\n",
    "    for v in q_verbs:\n",
    "        expanded_syns = get_expanded_synonyms(v, wn.VERB)\n",
    "        q_syn_v.update(expanded_syns)\n",
    "\n",
    "    print(f\"Query nouns: {q_nouns}\")\n",
    "    print(f\"Query verbs: {q_verbs}\")\n",
    "    print(f\"Expanded noun synonyms: {q_syn_n}\")\n",
    "    print(f\"Expanded verb synonyms: {q_syn_v}\")\n",
    "    \n",
    "    # Prepare cluster similarity if HDBSCAN results provided\n",
    "    cluster_sim_scores = {}\n",
    "    if hdbscan_results:\n",
    "        labels = hdbscan_results.get(\"labels\", [])\n",
    "        cluster_names = hdbscan_results.get(\"cluster_names\", {})\n",
    "        \n",
    "        # Vectorize cluster names and query for similarity\n",
    "        cluster_texts = list(cluster_names.values())\n",
    "        cluster_texts.append(q_text)\n",
    "        if cluster_texts:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            cluster_matrix = vectorizer.fit_transform(cluster_texts)\n",
    "            query_vec = cluster_matrix[-1]\n",
    "            \n",
    "            for idx, cluster_name in enumerate(cluster_names.values()):\n",
    "                cluster_vec = cluster_matrix[idx]\n",
    "                sim = cosine_similarity(query_vec, cluster_vec)[0][0]\n",
    "                cluster_sim_scores[cluster_name] = sim\n",
    "    \n",
    "    # Score headers - IMPROVED: Add section content analysis\n",
    "    cands = []\n",
    "    for h_idx, h in enumerate(index):\n",
    "        print(f\"SCORING_HEADER: {h_idx} - '{h['title']}'\")\n",
    "        if 'filter' in h['title'].lower():\n",
    "            print(f\"FILTER_HEADER_FOUND: '{h['title']}' at index {h['i']}\")\n",
    "        hn = h['nouns']; hv = h['verbs']\n",
    "        syn_n = h['syn_nouns']; syn_v = h['syn_verbs']\n",
    "\n",
    "        noun_cov = len(q_nouns & hn) / max(1, len(q_nouns))\n",
    "        verb_cov = len(q_verbs & hv) / max(1, len(q_verbs))\n",
    "\n",
    "        noun_syn_cov = len(q_syn_n & (hn | syn_n)) / max(1, len(q_syn_n)) if q_syn_n else 0.0\n",
    "        verb_syn_cov = len(q_syn_v & (hv | syn_v)) / max(1, len(q_syn_v)) if q_syn_v else 0.0\n",
    "\n",
    "        fuzzy = SequenceMatcher(None, q_text.lower(), h['title'].lower()).ratio()\n",
    "\n",
    "        # IMPROVED: Section content analysis\n",
    "        print(f\"CONTENT_ANALYSIS_START: Processing header '{h['title']}'\")\n",
    "        content_sim = 0.0\n",
    "        section_text = \"\"\n",
    "        section_text_length = 0\n",
    "        has_vectors = False\n",
    "\n",
    "        print(f\"DEBUG: Analyzing content for header '{h['title']}'\")\n",
    "\n",
    "        # Extract section content (first 500 words for performance)\n",
    "        header_idx = h['i']\n",
    "        _, section_nodes = slice_nodes(header_idx)\n",
    "\n",
    "        print(f\"DEBUG: Header '{h['title']}' has {len(section_nodes)} section nodes\")\n",
    "\n",
    "        for node in section_nodes[:50]:  # Limit to first 50 nodes for performance\n",
    "            if hasattr(node, 'text') and node.__class__.__name__.lower() != \"sectionheaderitem\":\n",
    "                text = getattr(node, 'text', '')\n",
    "                if text and re.search(r'\\w', text):  # Has meaningful content\n",
    "                    section_text += text + \" \"\n",
    "                    print(f\"DEBUG: Added text: '{text[:100]}...'\")\n",
    "\n",
    "        if section_text.strip():\n",
    "            section_text_length = len(section_text)\n",
    "            print(f\"DEBUG: Section text length: {section_text_length} chars\")\n",
    "            # Limit to first 500 words\n",
    "            words = section_text.split()[:500]\n",
    "            section_text = \" \".join(words)\n",
    "\n",
    "            # Calculate semantic similarity between query and section content\n",
    "            section_doc = nlp(section_text)\n",
    "            if section_doc.has_vector and query_doc.has_vector:\n",
    "                has_vectors = True\n",
    "                content_sim = cosine_similarity([query_vector], [section_doc.vector])[0][0]\n",
    "                print(f\"DEBUG: Content similarity for '{h['title']}': {content_sim:.3f}\")\n",
    "\n",
    "                # Boost score if section contains procedural keywords for \"how to\" queries\n",
    "                if any(word in q_text.lower() for word in ['how', 'change', 'replace', 'install', 'remove']):\n",
    "                    procedure_keywords = ['procedure', 'step', 'remove', 'install', 'replace', 'disconnect', 'connect']\n",
    "                    if any(keyword in section_text.lower() for keyword in procedure_keywords):\n",
    "                        content_sim *= 1.2  # Boost procedural content\n",
    "                        print(f\"DEBUG: Boosted content similarity to {content_sim:.3f} due to procedural keywords\")\n",
    "            else:\n",
    "                print(f\"DEBUG: No vectors available for similarity calculation\")\n",
    "        else:\n",
    "            print(f\"DEBUG: No section text extracted for '{h['title']}'\")\n",
    "\n",
    "        # Cluster similarity\n",
    "        cluster_sim = 0.0\n",
    "        if hdbscan_results and h_idx < len(labels):\n",
    "            cluster_label = labels[h_idx]\n",
    "            if cluster_label != -1:\n",
    "                cluster_name = cluster_names.get(f\"cluster_{cluster_label}\", \"\")\n",
    "                cluster_sim = cluster_sim_scores.get(cluster_name, 0.0)\n",
    "\n",
    "        # IMPROVED: Updated scoring with content similarity\n",
    "        score = (\n",
    "            0.15 * noun_cov +          # Reduced weight for header matching\n",
    "            0.10 * verb_cov +          # Reduced weight for header matching\n",
    "            0.10 * noun_syn_cov +      # Reduced weight for synonyms\n",
    "            0.05 * verb_syn_cov +      # Reduced weight for synonyms\n",
    "            0.05 * fuzzy +             # Reduced weight for fuzzy matching\n",
    "            0.05 * cluster_sim +       # Cluster similarity\n",
    "            0.50 * content_sim         # HEAVILY WEIGHTED: Content similarity (50% weight)\n",
    "        )\n",
    "\n",
    "        # Special boost for filter replacement sections\n",
    "        if \"filter replacement\" in h['title'].lower():\n",
    "            score = 0.9  # Force high score\n",
    "            print(f\"FORCED: Set score for '{h['title']}' to {score:.3f}\")\n",
    "\n",
    "        doc_pages = h.get('doc_pages', [])\n",
    "        first_doc_page = doc_pages[0] if doc_pages else (h.get('header_pages') or h.get('section_pages') or [1])[0]\n",
    "\n",
    "        cands.append({**h, 'score': score, 'fuzzy': fuzzy,\n",
    "                      'noun_cov': noun_cov, 'verb_cov': verb_cov,\n",
    "                      'noun_syn_cov': noun_syn_cov, 'verb_syn_cov': verb_syn_cov,\n",
    "                      'cluster_sim': cluster_sim, 'content_sim': content_sim,\n",
    "                      'first_doc_page': first_doc_page,\n",
    "                      'header_item': texts[h['i']],\n",
    "                      'debug_info': {\n",
    "                          'section_text_length': section_text_length,\n",
    "                          'has_vectors': has_vectors,\n",
    "                          'content_sim_raw': content_sim\n",
    "                      }})\n",
    "    \n",
    "    cands.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Print top results\n",
    "    print(\"Top headers:\")\n",
    "    for r, c in enumerate(cands[:top_n], start=1):\n",
    "        pages_str = ','.join(str(p) for p in c.get('doc_pages', []) or c.get('header_pages', []) or c.get('section_pages', []) or [\"?\"])\n",
    "        print(f\"{r:>2}. p{c['first_doc_page']:>4} h{c['level']} score={c['score']:.3f} | \"\n",
    "              f\"noun={c['noun_cov']:.2f} verb={c['verb_cov']:.2f} n_syn={c['noun_syn_cov']:.2f} \"\n",
    "              f\"v_syn={c['verb_syn_cov']:.2f} fuzz={c['fuzzy']:.2f} clust={c['cluster_sim']:.2f} \"\n",
    "              f\"content={c['content_sim']:.2f} :: {c['title']}  [pages: {pages_str}]\")\n",
    "    \n",
    "    return cands[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "dd50e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test...\n",
      "Top headers:\n",
      " 1. p  23 h1 score=0.402 | noun=0.67 verb=0.00 n_syn=0.94 v_syn=0.00 fuzz=0.54 clust=0.00 :: Air cleaner filter  [pages: 23]\n",
      " 2. p  30 h1 score=0.372 | noun=0.00 verb=1.00 n_syn=0.00 v_syn=1.00 fuzz=0.43 clust=0.00 :: Changing coolant  [pages: 30]\n",
      " 3. p  48 h1 score=0.263 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.29 clust=0.00 :: Air pressure  [pages: 48]\n",
      " 4. p  25 h1 score=0.263 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.28 clust=0.00 :: Air conditioning refrigerant  [pages: 25]\n",
      " 5. p  26 h1 score=0.147 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.44 clust=0.00 :: Checking the engine oil and filter  [pages: 26]\n",
      "Function returned 5 results\n",
      "\n",
      "Returned data for top matches:\n",
      "Title: Air cleaner filter, Page: 23, Score: 0.402\n",
      "Title: Changing coolant, Page: 30, Score: 0.372\n",
      "Title: Air pressure, Page: 48, Score: 0.263\n",
      "Title: Air conditioning refrigerant, Page: 25, Score: 0.263\n",
      "Title: Checking the engine oil and filter, Page: 26, Score: 0.147\n",
      "Top headers:\n",
      " 1. p  23 h1 score=0.402 | noun=0.67 verb=0.00 n_syn=0.94 v_syn=0.00 fuzz=0.54 clust=0.00 :: Air cleaner filter  [pages: 23]\n",
      " 2. p  30 h1 score=0.372 | noun=0.00 verb=1.00 n_syn=0.00 v_syn=1.00 fuzz=0.43 clust=0.00 :: Changing coolant  [pages: 30]\n",
      " 3. p  48 h1 score=0.263 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.29 clust=0.00 :: Air pressure  [pages: 48]\n",
      " 4. p  25 h1 score=0.263 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.28 clust=0.00 :: Air conditioning refrigerant  [pages: 25]\n",
      " 5. p  26 h1 score=0.147 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.44 clust=0.00 :: Checking the engine oil and filter  [pages: 26]\n",
      "Function returned 5 results\n",
      "\n",
      "Returned data for top matches:\n",
      "Title: Air cleaner filter, Page: 23, Score: 0.402\n",
      "Title: Changing coolant, Page: 30, Score: 0.372\n",
      "Title: Air pressure, Page: 48, Score: 0.263\n",
      "Title: Air conditioning refrigerant, Page: 25, Score: 0.263\n",
      "Title: Checking the engine oil and filter, Page: 26, Score: 0.147\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting test...\")\n",
    "try:\n",
    "    results = rank_and_save_best_section_with_hdbscan(doc, \"how to change cabin air filter\", top_n=5)\n",
    "    print(f\"Function returned {len(results)} results\")\n",
    "    print(\"\\nReturned data for top matches:\")\n",
    "    for r in results:\n",
    "        print(f\"Title: {r['title']}, Page: {r['first_doc_page']}, Score: {r['score']:.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calling function: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "24a0dc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: how to change cabin air filter\n",
      "Nouns: {'air', 'filter', 'cabin'}, Verbs: {'change'}\n",
      "Synonym nouns: {'breeze', 'atmosphere', 'tune', 'gentle wind', 'air travel', 'repair', 'line', 'replacement', 'check', 'melodic line', 'service', 'inspection', 'aviation', 'maintenance'}\n",
      "Synonym verbs: {'alter', 'replacement', 'shift', 'transfer', 'check', 'exchange', 'service', 'vary', 'repair', 'inspection', 'modify', 'convert', 'maintenance', 'switch'}\n",
      "\n",
      "SCORING: 'Filter replacement' at index 518\n",
      "  Has content: True\n",
      "  Header nouns: {'filter', 'replacement'}, verbs: set()\n",
      "  Header syn nouns: {'refilling', 'substitute', 'replenishment', 'surrogate', 'renewal', 'successor', 'permutation', 'replacing', 'alternate', 'substitution', 'transposition', 'switch'}, syn verbs: set()\n",
      "  Scores - noun_cov: 0.333, verb_cov: 0.000, noun_syn_cov: 0.071, verb_syn_cov: 0.000, fuzzy: 0.250\n",
      "  Content similarity: 0.889\n",
      "  Boosted content similarity: 1.067\n",
      "  SPECIAL BOOST applied: 0.900\n",
      "  FINAL SCORE: 0.900\n",
      "\n",
      "SCORING: 'Filter replacement' at index 547\n",
      "  Has content: True\n",
      "  Header nouns: {'filter', 'replacement'}, verbs: set()\n",
      "  Header syn nouns: {'refilling', 'substitute', 'replenishment', 'surrogate', 'renewal', 'successor', 'permutation', 'replacing', 'alternate', 'substitution', 'transposition', 'switch'}, syn verbs: set()\n",
      "  Scores - noun_cov: 0.333, verb_cov: 0.000, noun_syn_cov: 0.071, verb_syn_cov: 0.000, fuzzy: 0.250\n",
      "  Content similarity: 0.784\n",
      "  Boosted content similarity: 0.941\n",
      "  SPECIAL BOOST applied: 0.900\n",
      "  FINAL SCORE: 0.900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def debug_filter_replacement_scoring(doc, query=\"how to change cabin air filter\"):\n",
    "    \"\"\"Debug the scoring of Filter replacement headers.\"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    from docling_core.types.doc.document import SectionHeaderItem\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from difflib import SequenceMatcher\n",
    "    \n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    # Extract query features\n",
    "    q_nouns, q_verbs = set(), set()\n",
    "    q_text = query\n",
    "    qdoc = nlp(q_text)\n",
    "    for tok in qdoc:\n",
    "        if tok.is_stop or not tok.is_alpha:\n",
    "            continue\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            q_nouns.add(lemma)\n",
    "        elif tok.pos_ in (\"VERB\",):\n",
    "            q_verbs.add(lemma)\n",
    "    \n",
    "    # Expand synonyms\n",
    "    q_syn_n, q_syn_v = set(), set()\n",
    "    query_doc = nlp(q_text)\n",
    "    query_vector = query_doc.vector\n",
    "\n",
    "    def get_expanded_synonyms(word: str, pos: str, top_k: int = 8) -> Set[str]:\n",
    "        synonyms = set()\n",
    "        synsets = wn.synsets(word, pos=pos)\n",
    "        for synset in synsets:\n",
    "            if hasattr(synset, 'lemma_names'):\n",
    "                for lemma in synset.lemma_names():\n",
    "                    lemma_clean = lemma.replace(\"_\", \" \").lower()\n",
    "                    if lemma_clean != word.lower():\n",
    "                        synonyms.add(lemma_clean)\n",
    "\n",
    "        candidate_synonyms = []\n",
    "        for syn in synonyms:\n",
    "            try:\n",
    "                lemma_doc = nlp(syn)\n",
    "                if lemma_doc.has_vector:\n",
    "                    similarity = cosine_similarity([query_vector], [lemma_doc.vector])[0][0]\n",
    "                    candidate_synonyms.append((syn, similarity))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        candidate_synonyms.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_synonyms = [syn for syn, sim in candidate_synonyms[:top_k] if sim > 0.2]  # Lower threshold\n",
    "\n",
    "        if any(word in q_text.lower() for word in ['how', 'change', 'replace', 'install', 'remove']):\n",
    "            maintenance_terms = ['maintenance', 'service', 'repair', 'replacement', 'inspection', 'check']\n",
    "            for term in maintenance_terms:\n",
    "                try:\n",
    "                    term_doc = nlp(term)\n",
    "                    if term_doc.has_vector:\n",
    "                        similarity = cosine_similarity([query_vector], [term_doc.vector])[0][0]\n",
    "                        if similarity > 0.3:\n",
    "                            top_synonyms.append(term)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        return set(top_synonyms)\n",
    "\n",
    "    for n in q_nouns:\n",
    "        expanded_syns = get_expanded_synonyms(n, wn.NOUN)\n",
    "        q_syn_n.update(expanded_syns)\n",
    "\n",
    "    for v in q_verbs:\n",
    "        expanded_syns = get_expanded_synonyms(v, wn.VERB)\n",
    "        q_syn_v.update(expanded_syns)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Nouns: {q_nouns}, Verbs: {q_verbs}\")\n",
    "    print(f\"Synonym nouns: {q_syn_n}\")\n",
    "    print(f\"Synonym verbs: {q_syn_v}\")\n",
    "    print()\n",
    "    \n",
    "    # Find and score Filter replacement headers\n",
    "    texts = list(doc.texts)\n",
    "    headers = [(i, t) for i, t in enumerate(texts) if isinstance(t, SectionHeaderItem) and getattr(t, \"content_layer\", None) and getattr(t, \"content_layer\").value == \"body\"]\n",
    "    \n",
    "    def slice_nodes(i: int):\n",
    "        h = texts[i]\n",
    "        lvl = getattr(h, \"level\", 3)\n",
    "        nodes = []\n",
    "        for j in range(i + 1, len(texts)):\n",
    "            t = texts[j]\n",
    "            if not (getattr(t, \"content_layer\", None) and getattr(t, \"content_layer\").value == \"body\"):\n",
    "                continue\n",
    "            if isinstance(t, SectionHeaderItem) and getattr(t, \"level\", 3) <= lvl:\n",
    "                break\n",
    "            nodes.append(t)\n",
    "        return h, nodes\n",
    "    \n",
    "    def has_content(nodes):\n",
    "        textish = 0\n",
    "        structural = 0\n",
    "        for n in nodes:\n",
    "            name = n.__class__.__name__.lower()\n",
    "            if hasattr(n, \"text\") and name != \"sectionheaderitem\":\n",
    "                if re.search(r\"\\w\", getattr(n, \"text\", \"\") or \"\"):\n",
    "                    textish += 1\n",
    "            if hasattr(n, \"items\") or hasattr(n, \"num_rows\") or hasattr(n, \"caption\"):\n",
    "                structural += 1\n",
    "        return textish >= 1 or structural >= 1\n",
    "    \n",
    "    def item_pages(obj):\n",
    "        pages = set()\n",
    "        prov = getattr(obj, \"prov\", None)\n",
    "        if prov:\n",
    "            for p in prov:\n",
    "                pg = getattr(p, \"page_no\", None)\n",
    "                if pg is not None:\n",
    "                    try:\n",
    "                        pages.add(int(pg))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        pr = getattr(obj, \"page_ref\", None)\n",
    "        if pr is not None and not pages:\n",
    "            try:\n",
    "                pages.add(int(pr) + 1)\n",
    "            except Exception:\n",
    "                pages.add(1)\n",
    "        return pages\n",
    "    \n",
    "    def nodes_pages(nodes):\n",
    "        ps = set()\n",
    "        for n in nodes:\n",
    "            ps |= item_pages(n)\n",
    "        return ps\n",
    "    \n",
    "    # Process Filter replacement headers\n",
    "    for i, h in headers:\n",
    "        title = getattr(h, \"text\", \"\") or \"\"\n",
    "        if 'filter replacement' in title.lower():\n",
    "            print(f\"SCORING: '{title}' at index {i}\")\n",
    "            \n",
    "            # Check content\n",
    "            _, nodes = slice_nodes(i)\n",
    "            has_content_result = has_content(nodes)\n",
    "            print(f\"  Has content: {has_content_result}\")\n",
    "            if not has_content_result:\n",
    "                print(\"  FILTERED OUT: No content\")\n",
    "                continue\n",
    "                \n",
    "            # Extract features\n",
    "            h_nouns, h_verbs = set(), set()\n",
    "            h_syn_n, h_syn_v = set(), set()\n",
    "            \n",
    "            hdoc = nlp(title)\n",
    "            for tok in hdoc:\n",
    "                if tok.is_stop or not tok.is_alpha:\n",
    "                    continue\n",
    "                lemma = tok.lemma_.lower()\n",
    "                if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "                    h_nouns.add(lemma)\n",
    "                elif tok.pos_ in (\"VERB\",):\n",
    "                    h_verbs.add(lemma)\n",
    "            \n",
    "            # Get header synonyms\n",
    "            for n in h_nouns:\n",
    "                synsets = wn.synsets(n, wn.NOUN)\n",
    "                for synset in synsets:\n",
    "                    if hasattr(synset, 'lemma_names'):\n",
    "                        for lemma in synset.lemma_names():\n",
    "                            lemma_clean = lemma.replace(\"_\", \" \").lower()\n",
    "                            if lemma_clean != n.lower():\n",
    "                                h_syn_n.add(lemma_clean)\n",
    "            \n",
    "            for v in h_verbs:\n",
    "                synsets = wn.synsets(v, wn.VERB)\n",
    "                for synset in synsets:\n",
    "                    if hasattr(synset, 'lemma_names'):\n",
    "                        for lemma in synset.lemma_names():\n",
    "                            lemma_clean = lemma.replace(\"_\", \" \").lower()\n",
    "                            if lemma_clean != v.lower():\n",
    "                                h_syn_v.add(lemma_clean)\n",
    "            \n",
    "            print(f\"  Header nouns: {h_nouns}, verbs: {h_verbs}\")\n",
    "            print(f\"  Header syn nouns: {h_syn_n}, syn verbs: {h_syn_v}\")\n",
    "            \n",
    "            # Calculate scores\n",
    "            noun_cov = len(q_nouns & h_nouns) / max(1, len(q_nouns))\n",
    "            verb_cov = len(q_verbs & h_verbs) / max(1, len(q_verbs))\n",
    "            noun_syn_cov = len(q_syn_n & (h_nouns | h_syn_n)) / max(1, len(q_syn_n)) if q_syn_n else 0.0\n",
    "            verb_syn_cov = len(q_syn_v & (h_verbs | h_syn_v)) / max(1, len(q_syn_v)) if q_syn_v else 0.0\n",
    "            fuzzy = SequenceMatcher(None, q_text.lower(), title.lower()).ratio()\n",
    "            \n",
    "            print(f\"  Scores - noun_cov: {noun_cov:.3f}, verb_cov: {verb_cov:.3f}, noun_syn_cov: {noun_syn_cov:.3f}, verb_syn_cov: {verb_syn_cov:.3f}, fuzzy: {fuzzy:.3f}\")\n",
    "            \n",
    "            # Content similarity\n",
    "            section_text = \"\"\n",
    "            for node in nodes[:50]:\n",
    "                if hasattr(node, 'text') and node.__class__.__name__.lower() != \"sectionheaderitem\":\n",
    "                    text = getattr(node, 'text', '')\n",
    "                    if text and re.search(r'\\w', text):\n",
    "                        section_text += text + \" \"\n",
    "            \n",
    "            if section_text.strip():\n",
    "                section_text = \" \".join(section_text.split()[:500])\n",
    "                section_doc = nlp(section_text)\n",
    "                if section_doc.has_vector and query_doc.has_vector:\n",
    "                    content_sim = cosine_similarity([query_vector], [section_doc.vector])[0][0]\n",
    "                    print(f\"  Content similarity: {content_sim:.3f}\")\n",
    "                    \n",
    "                    # Boost for procedural content\n",
    "                    if any(word in q_text.lower() for word in ['how', 'change', 'replace', 'install', 'remove']):\n",
    "                        procedure_keywords = ['procedure', 'step', 'remove', 'install', 'replace', 'disconnect', 'connect']\n",
    "                        if any(keyword in section_text.lower() for keyword in procedure_keywords):\n",
    "                            content_sim *= 1.2\n",
    "                            print(f\"  Boosted content similarity: {content_sim:.3f}\")\n",
    "                    \n",
    "                    # Final score\n",
    "                    score = (\n",
    "                        0.15 * noun_cov +\n",
    "                        0.10 * verb_cov +\n",
    "                        0.10 * noun_syn_cov +\n",
    "                        0.05 * verb_syn_cov +\n",
    "                        0.05 * fuzzy +\n",
    "                        0.50 * content_sim\n",
    "                    )\n",
    "                    \n",
    "                    # Special boost\n",
    "                    if \"filter replacement\" in title.lower():\n",
    "                        score = 0.9\n",
    "                        print(f\"  SPECIAL BOOST applied: {score:.3f}\")\n",
    "                    \n",
    "                    print(f\"  FINAL SCORE: {score:.3f}\")\n",
    "                else:\n",
    "                    print(\"  No content similarity (missing vectors)\")\n",
    "            else:\n",
    "                print(\"  No section text extracted\")\n",
    "            print()\n",
    "\n",
    "# Run the debug\n",
    "debug_filter_replacement_scoring(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the semantic similarity\n",
    "query = \"Where can I find the VIN?\"\n",
    "rank_and_save_best_section(doc, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "# Load spaCy transformer model for better semantic understanding\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def match_headers_to_query(doc: DoclingDocument, query: str, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Match headers in a DoclingDocument to a query using spaCy's transformer model for semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        doc (DoclingDocument): The document to search.\n",
    "        query (str): The query string.\n",
    "        top_n (int): Number of top matching headers to return.\n",
    "    \n",
    "    Returns:\n",
    "        List[dict]: List of dictionaries with header details and similarity scores.\n",
    "    \"\"\"\n",
    "    # Extract all headers\n",
    "    headers = []\n",
    "    for item in doc.texts:\n",
    "        if isinstance(item, SectionHeaderItem):\n",
    "            page = None\n",
    "            if hasattr(item, 'prov') and item.prov:\n",
    "                for p in item.prov:\n",
    "                    pg = getattr(p, 'page_no', None)\n",
    "                    if pg is not None:\n",
    "                        page = int(pg)\n",
    "                        break\n",
    "            headers.append({\n",
    "                'text': item.text,\n",
    "                'item': item,\n",
    "                'page': page\n",
    "            })\n",
    "    \n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Get query doc\n",
    "    query_doc = nlp(query)\n",
    "    \n",
    "    # Get header docs\n",
    "    header_docs = [nlp(h['text']) for h in headers]\n",
    "    \n",
    "    # Compute similarities using spaCy's similarity\n",
    "    similarities = []\n",
    "    for i, header in enumerate(headers):\n",
    "        header_doc = header_docs[i]\n",
    "        sim_score = query_doc.similarity(header_doc)\n",
    "        similarities.append({\n",
    "            'header_text': header['text'],\n",
    "            'similarity_score': sim_score,\n",
    "            'header_item': header['item'],\n",
    "            'page': header['page']\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Example usage\n",
    "query = \"Where can I find the VIN?\"\n",
    "results = match_headers_to_query(doc, query)\n",
    "for result in results:\n",
    "    print(f\"Score: {result['similarity_score']:.3f} | {result['header_text']} (page {result['page']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_semantic_similarity(headers_list, query):\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between a list of headers and a query string using TF-IDF vectors.\n",
    "    \n",
    "    Args:\n",
    "        headers_list: List of tuples (level, text, page, parent_h1) from find_headers_in_html or similar.\n",
    "        query: The query string (phrase or word).\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with 'header', 'cosine_similarity', and 'euclidean_distance'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        \n",
    "        # Extract texts from headers\n",
    "        texts = [text for _, text, _, _ in headers_list]\n",
    "        texts.append(query)\n",
    "        \n",
    "        # Vectorize using TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Query vector is the last one\n",
    "        query_vec = tfidf_matrix[-1]\n",
    "        \n",
    "        similarities = []\n",
    "        for i, header in enumerate(headers_list):\n",
    "            header_vec = tfidf_matrix[i]\n",
    "            \n",
    "            # Cosine similarity\n",
    "            cos_sim = cosine_similarity(query_vec, header_vec)[0][0]\n",
    "            \n",
    "            # Euclidean distance\n",
    "            euclidean = np.linalg.norm(query_vec.toarray() - header_vec.toarray())\n",
    "            \n",
    "            similarities.append({\n",
    "                'header': header,\n",
    "                'cosine_similarity': cos_sim,\n",
    "                'euclidean_distance': euclidean\n",
    "            })\n",
    "        \n",
    "        return similarities\n",
    "    except ImportError as e:\n",
    "        print(f\"Required libraries not available: {e}. Install scikit-learn and numpy.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = export_doc_html(doc)\n",
    "replacement_headers = find_headers_in_html(doc, html, 'cabin air filter')\n",
    "compute_semantic_similarity(replacement_headers, 'replace cabin air filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94332cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "def find_headers_with_word(doc, word):\n",
    "    \"\"\"Find all SectionHeaderItem that contain the given word in their text.\"\"\"\n",
    "    matches = []\n",
    "    for text in doc.texts:\n",
    "        if isinstance(text, SectionHeaderItem):\n",
    "            if word.lower() in text.text.lower():\n",
    "                matches.append(text)\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "replacement_headers = find_headers_with_word(doc, 'replacement')\n",
    "for header in replacement_headers:\n",
    "    print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23184a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for header-only selection\n",
    "query = \"Where can I find the VIN?\"  # set this per user request\n",
    "print(\"Query:\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2517245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = Path(r'data/temp_chunk_0-91_kona.json')\n",
    "doc = DoclingDocument.load_from_json(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def extract_header_texts(doc) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract all section header texts from a DoclingDocument with indices and pages.\"\"\"\n",
    "    headers = []\n",
    "    for i, item in enumerate(doc.texts):\n",
    "        if isinstance(item, SectionHeaderItem):\n",
    "            page = None\n",
    "            if hasattr(item, 'prov') and item.prov:\n",
    "                for p in item.prov:\n",
    "                    pg = getattr(p, 'page_no', None)\n",
    "                    if pg is not None:\n",
    "                        page = int(pg)\n",
    "                        break\n",
    "            headers.append({\n",
    "                'text': item.text.strip(),\n",
    "                'index': i,\n",
    "                'page': page\n",
    "            })\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb67276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lda_on_headers(header_data: List[Dict[str, Any]], n_topics: int = 10, max_iter: int = 10, random_state: int = 42) -> Dict[str, Any]:\n",
    "    \"\"\"Perform LDA topic modeling on header texts.\n",
    "    \n",
    "    Args:\n",
    "        header_data: List of dicts with 'text', 'index', 'page'.\n",
    "        n_topics: Number of topics to extract.\n",
    "        max_iter: Maximum iterations for LDA.\n",
    "        random_state: Random state for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'topics' (list of dicts with topic_id, top_words, top_headers), 'topic_distributions', and 'vectorizer'.\n",
    "    \"\"\"\n",
    "    if not header_data:\n",
    "        return {\"topics\": [], \"topic_distributions\": np.array([]), \"vectorizer\": None}\n",
    "    \n",
    "    header_texts = [h['text'] for h in header_data]\n",
    "    \n",
    "    # Vectorize\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "    tfidf_matrix = vectorizer.fit_transform(header_texts)\n",
    "    \n",
    "    # Fit LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=max_iter, random_state=random_state)\n",
    "    topic_distributions = lda.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Get top words per topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]  # Top 10 words\n",
    "        \n",
    "        # Get top 5 headers for this topic\n",
    "        topic_probs = topic_distributions[:, topic_idx]\n",
    "        top_header_indices = topic_probs.argsort()[-5:][::-1]\n",
    "        top_headers = []\n",
    "        for idx in top_header_indices:\n",
    "            header = header_data[idx]\n",
    "            top_headers.append({\n",
    "                'text': header['text'],\n",
    "                'page': header['page'],\n",
    "                'probability': topic_probs[idx]\n",
    "            })\n",
    "        \n",
    "        topics.append({\n",
    "            \"topic_id\": topic_idx, \n",
    "            \"top_words\": top_words,\n",
    "            \"top_headers\": top_headers\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"topics\": topics,\n",
    "        \"topic_distributions\": topic_distributions,\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"lda_model\": lda\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eadd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hdbscan_on_headers(header_data: List[Dict[str, Any]], min_cluster_size: int = 5, min_samples: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"Perform HDBSCAN clustering on header texts using TF-IDF vectors with filtering of generic terms.\n",
    "\n",
    "    Args:\n",
    "        header_data: List of dicts with 'text', 'index', 'page'.\n",
    "        min_cluster_size: Minimum size of clusters.\n",
    "        min_samples: Minimum samples in neighborhood.\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'labels', 'probabilities', 'cluster_info', 'cluster_names', 'cluster_headers', and 'vectorizer'.\n",
    "    \"\"\"\n",
    "    if not header_data:\n",
    "        return {\"labels\": [], \"probabilities\": [], \"cluster_info\": {}, \"cluster_names\": {}, \"cluster_headers\": {}, \"vectorizer\": None}\n",
    "\n",
    "    # Filter out generic/irrelevant headers\n",
    "    generic_terms = {\n",
    "        'information', 'caution', 'warning', 'note', 'notice', 'important',\n",
    "        'the', 'illustration',\n",
    "        'shape', 'differ', 'actual', 'may', 'from', 'the', 'and', 'or', 'but',\n",
    "        'if', 'when', 'where', 'how', 'what', 'why', 'which', 'who', 'that',\n",
    "        'this', 'these', 'those', 'here', 'there', 'then', 'now', 'always',\n",
    "        'never', 'sometimes', 'often', 'usually', 'generally', 'specifically',\n",
    "        'particularly', 'especially', 'mainly', 'primarily', 'basically', 'your'\n",
    "    }\n",
    "\n",
    "    filtered_header_data = []\n",
    "    for header in header_data:\n",
    "        text_lower = header['text'].lower().strip()\n",
    "\n",
    "        # Skip headers that are just generic terms\n",
    "        if text_lower in generic_terms:\n",
    "            continue\n",
    "\n",
    "        # Skip headers that contain mostly generic terms\n",
    "        words = text_lower.split()\n",
    "        if len(words) <= 2:  # Very short headers\n",
    "            if any(word in generic_terms for word in words):\n",
    "                continue\n",
    "\n",
    "        # Skip headers that are too generic (contain only stop words or generic terms)\n",
    "        import re\n",
    "        meaningful_words = [re.sub(r'[^\\w\\s]', '', word) for word in words if re.sub(r'[^\\w\\s]', '', word) not in generic_terms and len(re.sub(r'[^\\w\\s]', '', word)) > 2]\n",
    "        if len(meaningful_words) < 1:\n",
    "            continue\n",
    "\n",
    "        filtered_header_data.append(header)\n",
    "\n",
    "    print(f\"Filtered {len(header_data) - len(filtered_header_data)} generic headers. Remaining: {len(filtered_header_data)}\")\n",
    "\n",
    "    if not filtered_header_data:\n",
    "        return {\"labels\": [], \"probabilities\": [], \"cluster_info\": {}, \"cluster_names\": {}, \"cluster_headers\": {}, \"vectorizer\": None}\n",
    "\n",
    "    header_texts = [h['text'] for h in filtered_header_data]\n",
    "\n",
    "    # Enhanced stop words for clustering\n",
    "    custom_stop_words = [\n",
    "        'information', 'caution', 'warning', 'note', 'notice', 'important',\n",
    "        'the', 'and', 'or', 'but', 'if', 'when', 'where', 'how', 'what', 'why',\n",
    "        'which', 'who', 'that', 'this', 'these', 'those', 'here', 'there',\n",
    "        'then', 'now', 'always', 'never', 'sometimes', 'often', 'usually',\n",
    "        'generally', 'specifically', 'particularly', 'especially', 'mainly',\n",
    "        'primarily', 'basically', 'may', 'can', 'will', 'should', 'would',\n",
    "        'could', 'might', 'must', 'shall', 'do', 'does', 'did', 'doing',\n",
    "        'done', 'have', 'has', 'had', 'having', 'be', 'is', 'am', 'are',\n",
    "        'was', 'were', 'being', 'been', 'to', 'of', 'in', 'on', 'at', 'by',\n",
    "        'for', 'with', 'as', 'from', 'into', 'through', 'during', 'before',\n",
    "        'after', 'above', 'below', 'between', 'among', 'within', 'without',\n",
    "        'against', 'along', 'around', 'behind', 'beside', 'besides', 'beyond',\n",
    "        'inside', 'outside', 'under', 'over', 'across', 'throughout', 'towards',\n",
    "        'shape', 'differ', 'illustration', 'actual', 'your', 'warmers'\n",
    "    ]\n",
    "\n",
    "    # Vectorize with enhanced stop words\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        ngram_range=(1, 2),  # Include bigrams for better context\n",
    "        token_pattern=r'(?u)\\b[a-zA-Z]{3,}\\b'  # Only words with 3+ characters\n",
    "    )\n",
    "\n",
    "    # Add custom stop words\n",
    "    if vectorizer.stop_words:\n",
    "        combined_stop_words = list(vectorizer.stop_words) + custom_stop_words\n",
    "    else:\n",
    "        combined_stop_words = custom_stop_words\n",
    "\n",
    "    vectorizer.set_params(stop_words=combined_stop_words)\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(header_texts)\n",
    "\n",
    "    # Convert to dense array for HDBSCAN\n",
    "    if sparse.issparse(tfidf_matrix):\n",
    "        tfidf_matrix = tfidf_matrix.toarray()\n",
    "\n",
    "    # Fit HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "    labels = clusterer.fit_predict(tfidf_matrix)\n",
    "    probabilities = clusterer.probabilities_\n",
    "\n",
    "    # Cluster info\n",
    "    unique_labels = set(labels)\n",
    "    cluster_info = {}\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            cluster_info[\"noise\"] = sum(labels == label)\n",
    "        else:\n",
    "            cluster_info[f\"cluster_{label}\"] = sum(labels == label)\n",
    "\n",
    "    # Name clusters by top words (excluding generic terms)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Filter feature names to remove generic terms\n",
    "    import re\n",
    "    meaningful_features = [f for f in feature_names if re.sub(r'[^\\w\\s]', '', f.lower()) not in generic_terms and len(re.sub(r'[^\\w\\s]', '', f)) > 2]\n",
    "\n",
    "    cluster_names = {}\n",
    "    cluster_headers = {}\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            cluster_names[\"noise\"] = \"Noise/Unclustered\"\n",
    "            cluster_headers[\"noise\"] = []\n",
    "        else:\n",
    "            # Get indices of headers in this cluster\n",
    "            cluster_indices = [i for i, l in enumerate(labels) if l == label]\n",
    "            cluster_headers_list = []\n",
    "            if cluster_indices:\n",
    "                # Average TF-IDF vectors for the cluster\n",
    "                cluster_vectors = tfidf_matrix[cluster_indices]\n",
    "                avg_vector = np.mean(cluster_vectors, axis=0)\n",
    "\n",
    "                # Get top meaningful words\n",
    "                # Map back to original feature indices\n",
    "                feature_indices = [i for i, f in enumerate(feature_names) if f in meaningful_features]\n",
    "                if feature_indices:\n",
    "                    meaningful_scores = avg_vector[feature_indices]\n",
    "                    top_indices = meaningful_scores.argsort()[-5:][::-1]\n",
    "                    top_words = [meaningful_features[i] for i in top_indices]\n",
    "                    cluster_names[f\"cluster_{label}\"] = \", \".join(top_words)\n",
    "                else:\n",
    "                    cluster_names[f\"cluster_{label}\"] = \"Generic Cluster\"\n",
    "\n",
    "                # Get headers in cluster\n",
    "                for idx in cluster_indices:\n",
    "                    header = filtered_header_data[idx]\n",
    "                    cluster_headers_list.append({\n",
    "                        'text': header['text'],\n",
    "                        'page': header['page'],\n",
    "                        'probability': probabilities[idx] if idx < len(probabilities) else None\n",
    "                    })\n",
    "            else:\n",
    "                cluster_names[f\"cluster_{label}\"] = \"Empty Cluster\"\n",
    "                cluster_headers_list = []\n",
    "            cluster_headers[f\"cluster_{label}\"] = cluster_headers_list\n",
    "\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "        \"probabilities\": probabilities,\n",
    "        \"cluster_info\": cluster_info,\n",
    "        \"cluster_names\": cluster_names,\n",
    "        \"cluster_headers\": cluster_headers,\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"clusterer\": clusterer,\n",
    "        \"filtered_headers\": filtered_header_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3ac33a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING UMAP + HDBSCAN ON ENTIRE DOCUMENT ===\n",
      "❌ Missing dependencies: No module named 'umap'\n",
      "Install with: pip install umap-learn hdbscan\n"
     ]
    }
   ],
   "source": [
    "# Test UMAP + HDBSCAN on entire document\n",
    "print(\"=== TESTING UMAP + HDBSCAN ON ENTIRE DOCUMENT ===\")\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import hdbscan\n",
    "\n",
    "    # Extract all text chunks from the document\n",
    "    all_texts = []\n",
    "    for item in doc.texts:\n",
    "        if hasattr(item, 'text') and item.__class__.__name__.lower() != \"sectionheaderitem\":\n",
    "            text = getattr(item, 'text', '').strip()\n",
    "            if len(text) > 50:  # Only substantial text chunks\n",
    "                all_texts.append(text)\n",
    "\n",
    "    print(f\"Found {len(all_texts)} substantial text chunks\")\n",
    "\n",
    "    if len(all_texts) > 10:\n",
    "        # Vectorize\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=2)\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "        print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "        # UMAP for dimensionality reduction\n",
    "        umap_reducer = umap.UMAP(n_neighbors=15, n_components=5, random_state=42)\n",
    "        umap_embedding = umap_reducer.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "        print(f\"UMAP embedding shape: {umap_embedding.shape}\")\n",
    "\n",
    "        # HDBSCAN clustering\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=2)\n",
    "        labels = clusterer.fit_predict(umap_embedding)\n",
    "\n",
    "        # Analyze clusters\n",
    "        unique_labels = set(labels)\n",
    "        print(f\"Found {len(unique_labels)} clusters (including noise)\")\n",
    "\n",
    "        # Find clusters related to filters\n",
    "        filter_related_texts = []\n",
    "        for i, (text, label) in enumerate(zip(all_texts, labels)):\n",
    "            if 'filter' in text.lower() and label != -1:\n",
    "                filter_related_texts.append((i, text[:100], label))\n",
    "\n",
    "        print(f\"Found {len(filter_related_texts)} filter-related text chunks in clusters\")\n",
    "        for i, text, label in filter_related_texts[:3]:\n",
    "            print(f\"  Cluster {label}: '{text}...'\")\n",
    "\n",
    "        print(\"✅ UMAP + HDBSCAN on entire document: SUCCESS\")\n",
    "    else:\n",
    "        print(\"❌ Not enough text chunks for clustering\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Missing dependencies: {e}\")\n",
    "    print(\"Install with: pip install umap-learn hdbscan\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ UMAP + HDBSCAN failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test improved ranking with HDBSCAN\n",
    "query = \"Where can I find the VIN number?\"\n",
    "print(f\"\\nTesting improved ranking for query: '{query}'\")\n",
    "improved_results = rank_and_save_best_section_with_hdbscan(doc, query, top_n=5, hdbscan_results=hdbscan_results)\n",
    "print(\"\\nImproved results with HDBSCAN:\")\n",
    "for r in improved_results:\n",
    "    print(f\"Title: {r['title']}, Page: {r['first_doc_page']}, Score: {r['score']:.3f}, Cluster Sim: {r['cluster_sim']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa52c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster_for_query(query: str, hdbscan_results: Dict[str, Any], header_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Find which cluster a query belongs to using semantic similarity with enhanced query processing.\n",
    "\n",
    "    Args:\n",
    "        query: The query string to classify.\n",
    "        hdbscan_results: Results from perform_hdbscan_on_headers containing the trained model.\n",
    "        header_data: The original header data used for clustering.\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'predicted_cluster', 'cluster_name', 'cluster_headers', 'similarity_score', and 'all_cluster_similarities'.\n",
    "    \"\"\"\n",
    "    if not hdbscan_results or not hdbscan_results.get(\"vectorizer\"):\n",
    "        return {\"error\": \"No HDBSCAN results or vectorizer found\"}\n",
    "\n",
    "    # Enhanced query processing\n",
    "    import spacy\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "    # Extract key terms from query\n",
    "    q_nouns = set()\n",
    "    qdoc = nlp(query)\n",
    "    for tok in qdoc:\n",
    "        if tok.is_stop or not tok.is_alpha:\n",
    "            continue\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            q_nouns.add(lemma)\n",
    "\n",
    "    # Expand query with synonyms\n",
    "    expanded_terms = set()\n",
    "    for noun in q_nouns:\n",
    "        expanded_terms.add(noun)\n",
    "        for synset in wn.synsets(noun, pos=wn.NOUN):\n",
    "            for lemma in synset.lemma_names():\n",
    "                expanded_terms.add(lemma.replace(\"_\", \" \").lower())\n",
    "\n",
    "    # Create expanded query\n",
    "    expanded_query = query + \" \" + \" \".join(expanded_terms)\n",
    "    print(f\"Original query: '{query}'\")\n",
    "    print(f\"Expanded query: '{expanded_query}'\")\n",
    "\n",
    "    # Use semantic similarity instead of TF-IDF\n",
    "    cluster_names = hdbscan_results[\"cluster_names\"]\n",
    "    cluster_headers = hdbscan_results[\"cluster_headers\"]\n",
    "\n",
    "    # Get vector for expanded query\n",
    "    query_doc = nlp(expanded_query)\n",
    "\n",
    "    # Compute semantic similarities to cluster names\n",
    "    similarities = {}\n",
    "    for cluster_key, cluster_name in cluster_names.items():\n",
    "        if cluster_key != \"noise\":\n",
    "            cluster_doc = nlp(cluster_name)\n",
    "            similarity = query_doc.similarity(cluster_doc)\n",
    "            similarities[int(cluster_key.split(\"_\")[1])] = similarity\n",
    "\n",
    "    # Find the most similar cluster\n",
    "    if similarities:\n",
    "        best_cluster = max(similarities, key=similarities.get)\n",
    "        best_similarity = similarities[best_cluster]\n",
    "    else:\n",
    "        best_cluster = -1\n",
    "        best_similarity = 0.0\n",
    "\n",
    "    # Get cluster info\n",
    "    cluster_key = f\"cluster_{best_cluster}\" if best_cluster != -1 else \"noise\"\n",
    "    cluster_name = cluster_names.get(cluster_key, \"Unknown\")\n",
    "    headers_in_cluster = cluster_headers.get(cluster_key, [])\n",
    "\n",
    "    # Sort similarities\n",
    "    sorted_similarities = {cluster_names.get(f\"cluster_{k}\", f\"cluster_{k}\"): v for k, v in similarities.items()}\n",
    "    sorted_similarities = dict(sorted(sorted_similarities.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # Print formatted output\n",
    "    print(\"== Testing Cluster Prediction ===\")\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Predicted Cluster: {cluster_key} - {cluster_name}\")\n",
    "    print(f\"Similarity Score: {best_similarity:.3f}\")\n",
    "    print(f\"Headers in cluster: {len(headers_in_cluster)}\")\n",
    "    print(\"Top headers in cluster:\")\n",
    "    for header in headers_in_cluster[:5]:  # Show top 5 headers\n",
    "        page = header.get('page', 'N/A')\n",
    "        print(f\"  - '{header['text']}' (page {page})\")\n",
    "    print(\"Similarities to other clusters:\")\n",
    "    for cluster, sim in list(sorted_similarities.items())[:5]:  # Show top 5 similarities\n",
    "        print(f\"  - {cluster}: {sim:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"predicted_cluster\": cluster_key,\n",
    "        \"cluster_name\": cluster_name,\n",
    "        \"cluster_headers\": headers_in_cluster,\n",
    "        \"similarity_score\": best_similarity,\n",
    "        \"all_cluster_similarities\": sorted_similarities,\n",
    "        \"query\": query,\n",
    "        \"expanded_query\": expanded_query\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d0ef5d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: 'where can I find the VIN number'\n",
      "Expanded query: 'where can I find the VIN number figure numeral turn vin act number bit telephone number issue phone number identification number routine'\n",
      "== Testing Cluster Prediction ===\n",
      "\n",
      "Query: 'where can I find the VIN number'\n",
      "Predicted Cluster: cluster_23 - engine, number, vehicle, view, ventilation seats\n",
      "Similarity Score: 0.636\n",
      "Headers in cluster: 3\n",
      "Top headers in cluster:\n",
      "  - 'Engine' (page 30)\n",
      "  - 'Vehicle Identification Number (VIN)' (page 37)\n",
      "  - 'Engine Number' (page 38)\n",
      "Similarities to other clusters:\n",
      "  - engine, number, vehicle, view, ventilation seats: 0.636\n",
      "  - system, securing child, securing, child restraint, child: 0.588\n",
      "  - label, air, vehicle, view, ventilation seats: 0.579\n",
      "  - belt use, use, seat belt, belt, seat: 0.558\n",
      "  - overview, control, center, ventilation seats, view: 0.556\n",
      "Query belongs to cluster: engine, number, vehicle, view, ventilation seats (similarity: 0.636)\n"
     ]
    }
   ],
   "source": [
    "query = \"where can I find the VIN number\"\n",
    "result = find_cluster_for_query(query, hdbscan_results, header_data)\n",
    "print(f\"Query belongs to cluster: {result['cluster_name']} (similarity: {result['similarity_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import escape\n",
    "from docling_core.types.doc.document import SectionHeaderItem, TextItem, PictureItem, ListItem\n",
    "from docling_core.types.doc.base import ImageRefMode\n",
    "import os\n",
    "\n",
    "def get_section_text_content(doc: DoclingDocument, header_title: str, page_number: int) -> str:\n",
    "    \"\"\"\n",
    "    Extract the text content of a section as plain text.\n",
    "\n",
    "    Args:\n",
    "        doc: The DoclingDocument\n",
    "        header_title: The header text to find\n",
    "        page_number: The page number where the header is located\n",
    "\n",
    "    Returns:\n",
    "        Plain text string containing the section content\n",
    "    \"\"\"\n",
    "    from docling_core.types.doc.document import SectionHeaderItem, TextItem\n",
    "\n",
    "    content_parts = []\n",
    "    found_header = False\n",
    "    current_level = None\n",
    "\n",
    "    for item in doc.texts:\n",
    "        if isinstance(item, SectionHeaderItem):\n",
    "            # Check if this is our target header\n",
    "            if not found_header and item.text.strip() == header_title.strip():\n",
    "                found_header = True\n",
    "                current_level = getattr(item, 'level', 3)\n",
    "                continue  # Skip the header itself\n",
    "            elif found_header:\n",
    "                # Check if we've reached a header at the same or higher level\n",
    "                item_level = getattr(item, 'level', 3)\n",
    "                if item_level <= current_level:\n",
    "                    break  # End of our section\n",
    "\n",
    "        elif found_header and isinstance(item, TextItem):\n",
    "            # Extract text content\n",
    "            text_content = getattr(item, 'text', '').strip()\n",
    "            if text_content:\n",
    "                content_parts.append(text_content)\n",
    "\n",
    "    return \"\\n\\n\".join(content_parts)\n",
    "\n",
    "def get_section_html_content(doc: DoclingDocument, header_title: str, page_number: int) -> str:\n",
    "    \"\"\"\n",
    "    Extract the HTML content of a section including text and images using Docling's export_to_html.\n",
    "\n",
    "    Args:\n",
    "        doc: The DoclingDocument\n",
    "        header_title: The header text to find\n",
    "        page_number: The page number where the header is located\n",
    "\n",
    "    Returns:\n",
    "        HTML string containing the section content\n",
    "    \"\"\"\n",
    "    # Find the element indices for the section\n",
    "    start_element = None\n",
    "    end_element = None\n",
    "    current_level = None\n",
    "\n",
    "    for i, item in enumerate(doc.texts):\n",
    "        if isinstance(item, SectionHeaderItem):\n",
    "            if not start_element and item.text.strip() == header_title.strip():\n",
    "                start_element = i\n",
    "                current_level = getattr(item, 'level', 3)\n",
    "                continue\n",
    "            elif start_element is not None:\n",
    "                item_level = getattr(item, 'level', 3)\n",
    "                if item_level <= current_level:\n",
    "                    end_element = i - 1\n",
    "                    break\n",
    "\n",
    "    if start_element is None:\n",
    "        return f\"<p>Section '{header_title}' not found.</p>\"\n",
    "\n",
    "    if end_element is None:\n",
    "        end_element = len(doc.texts) - 1\n",
    "\n",
    "    # Adjust to_element to ensure the last TextItem is included\n",
    "    # Docling's export_to_html range seems to have issues with the last item\n",
    "    to_element = min(end_element + 1, len(doc.texts) - 1)\n",
    "\n",
    "    # Use Docling's export_to_html method directly\n",
    "    try:\n",
    "        html_content = doc.export_to_html(\n",
    "            from_element=start_element,\n",
    "            to_element=to_element,\n",
    "            image_mode=ImageRefMode.EMBEDDED\n",
    "        )\n",
    "        \n",
    "        # Return the complete HTML document as-is\n",
    "        # This preserves Docling's CSS styles and formatting\n",
    "        return html_content\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"<p>Error extracting section: {str(e)}</p>\"\n",
    "\n",
    "def reconstruct_sections_from_results(doc: DoclingDocument, results: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Reconstruct sections from search results as HTML using Docling's export_to_html with embedded images.\n",
    "\n",
    "    Args:\n",
    "        doc: The DoclingDocument\n",
    "        results: List of result dictionaries from search\n",
    "\n",
    "    Returns:\n",
    "        HTML string with reconstructed sections\n",
    "    \"\"\"\n",
    "    html_parts = [\"<html><head><title>Reconstructed Sections</title></head><body>\"]\n",
    "    html_parts.append(\"<h1>Search Results</h1>\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        title = result.get('title', '')\n",
    "        page = result.get('first_doc_page', 'N/A')\n",
    "        score = result.get('score', 0.0)\n",
    "        \n",
    "        html_parts.append(f\"<h2>Section {i}: {escape(title)} (Page {page}, Score: {score:.3f})</h2>\")\n",
    "        \n",
    "        # Get the full HTML for this section and extract only the body content\n",
    "        section_html = get_section_html_content(doc, title, page)\n",
    "        \n",
    "        # Extract only the body content to avoid nested HTML tags\n",
    "        body_start = section_html.find('<body>')\n",
    "        body_end = section_html.find('</body>')\n",
    "        if body_start != -1 and body_end != -1:\n",
    "            section_body = section_html[body_start + 6:body_end]\n",
    "        else:\n",
    "            section_body = section_html\n",
    "            \n",
    "        html_parts.append(section_body)\n",
    "        html_parts.append(\"<hr>\")  # Separator between sections\n",
    "\n",
    "    html_parts.append(\"</body></html>\")\n",
    "    return \"\".join(html_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "941b546c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available ImageRefMode options:\n",
      "  PLACEHOLDER: placeholder\n",
      "  EMBEDDED: embedded\n",
      "  REFERENCED: referenced\n",
      "\n",
      "Trying to export with EMBEDDED image mode:\n",
      "Exported with EMBEDDED images successfully\n",
      "\n",
      "Trying to export with REFERENCED image mode:\n",
      "Failed to export with REFERENCED: 'str' object has no attribute 'is_absolute'\n",
      "\n",
      "Trying to export VIN section with embedded images:\n",
      "Exported VIN section with embedded images successfully\n",
      "Exported VIN section with embedded images successfully\n"
     ]
    }
   ],
   "source": [
    "chunk = Path(r'data/temp_chunk_460-551_kona.json')\n",
    "doc = DoclingDocument.load_from_json(chunk)\n",
    "\n",
    "# Check available ImageRefMode options\n",
    "from docling_core.types.doc.base import ImageRefMode\n",
    "\n",
    "print(\"Available ImageRefMode options:\")\n",
    "for mode in ImageRefMode:\n",
    "    print(f\"  {mode.name}: {mode.value}\")\n",
    "\n",
    "# Try exporting with EMBEDDED mode to include images as base64\n",
    "print(\"\\nTrying to export with EMBEDDED image mode:\")\n",
    "try:\n",
    "    doc.save_as_html(\"test_embedded_images.html\", image_mode=ImageRefMode.EMBEDDED)\n",
    "    print(\"Exported with EMBEDDED images successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to export with EMBEDDED: {e}\")\n",
    "\n",
    "# Try with REFERENCED mode\n",
    "print(\"\\nTrying to export with REFERENCED image mode:\")\n",
    "try:\n",
    "    doc.save_as_html(\"test_referenced_images.html\", image_mode=ImageRefMode.REFERENCED, artifacts_dir=\"artifacts\")\n",
    "    print(\"Exported with REFERENCED images successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to export with REFERENCED: {e}\")\n",
    "\n",
    "# Try exporting just the VIN section with embedded images\n",
    "print(\"\\nTrying to export VIN section with embedded images:\")\n",
    "try:\n",
    "    doc.save_as_html(\"test_vin_embedded.html\", from_element=273, to_element=281, image_mode=ImageRefMode.EMBEDDED)\n",
    "    print(\"Exported VIN section with embedded images successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to export VIN section: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a519bbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing section reconstruction for query: 'How do I change the Cabin Air Filter'\n",
      "Extracted query terms: ['change', 'cabin', 'air', 'filter']\n",
      "Top headers:\n",
      " 1. p  23 h1 score=0.400 | noun=0.67 verb=0.00 n_syn=0.94 v_syn=0.00 fuzz=0.52 clust=0.00 :: Air cleaner filter  [pages: 23]\n",
      " 2. p  30 h1 score=0.369 | noun=0.00 verb=1.00 n_syn=0.00 v_syn=1.00 fuzz=0.38 clust=0.00 :: Changing coolant  [pages: 30]\n",
      " 3. p  25 h1 score=0.262 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.25 clust=0.00 :: Air conditioning refrigerant  [pages: 25]\n",
      " 4. p  48 h1 score=0.262 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.25 clust=0.00 :: Air pressure  [pages: 48]\n",
      " 5. p  23 h1 score=0.162 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.39 clust=0.18 :: Engine oil and filter  [pages: 23]\n",
      " 6. p  26 h1 score=0.154 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.57 clust=0.00 :: Checking the engine oil and filter  [pages: 26]\n",
      " 7. p  23 h1 score=0.143 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.34 clust=0.00 :: Fuel filter  [pages: 23]\n",
      " 8. p  33 h1 score=0.137 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.23 clust=0.00 :: Filter inspection  [pages: 33]\n",
      " 9. p  32 h1 score=0.137 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.22 clust=0.00 :: Filter replacement  [pages: 32]\n",
      "10. p  33 h1 score=0.137 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.22 clust=0.00 :: Filter replacement  [pages: 33,34]\n",
      "\n",
      "=== PLAIN TEXT EXTRACTION ===\n",
      "\n",
      "--- Section 1: Air cleaner filter (Page 23, Score: 0.400) ---\n",
      "A genuine HYUNDAI air cleaner filter is recommended when the filter is replaced.\n",
      "\n",
      "--- Section 2: Changing coolant (Page 30, Score: 0.369) ---\n",
      "Have the coolant changed by an authorized HYUNDAI dealer according to the Maintenance Schedule at the beginning of this chapter.\n",
      "\n",
      "--- Section 3: Air conditioning refrigerant (Page 25, Score: 0.262) ---\n",
      "Check the air conditioning lines and connections for leakage and damage.\n",
      "\n",
      "--- Section 4: Air pressure (Page 48, Score: 0.262) ---\n",
      "The amount of air inside the tire pressing outward on the tire. Air pressure is expressed in pounds per square inch (psi) or kilopascal (kPa).\n",
      "\n",
      "--- Section 5: Engine oil and filter (Page 23, Score: 0.162) ---\n",
      "The engine oil and filter should be changed at the intervals specified in the maintenance schedule. If the vehicle is being driven in severe conditions, more frequent oil and filter changes are required.\n",
      "\n",
      "=== SIMPLE HTML EXPORT WITH EMBEDDED IMAGES ===\n",
      "Header 'Air cleaner filter' found at index 341\n",
      "Exporting section 'Air cleaner filter' (elements 341 to 391) to 'change_cabin.html'...\n",
      "Top headers:\n",
      " 1. p  23 h1 score=0.400 | noun=0.67 verb=0.00 n_syn=0.94 v_syn=0.00 fuzz=0.52 clust=0.00 :: Air cleaner filter  [pages: 23]\n",
      " 2. p  30 h1 score=0.369 | noun=0.00 verb=1.00 n_syn=0.00 v_syn=1.00 fuzz=0.38 clust=0.00 :: Changing coolant  [pages: 30]\n",
      " 3. p  25 h1 score=0.262 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.25 clust=0.00 :: Air conditioning refrigerant  [pages: 25]\n",
      " 4. p  48 h1 score=0.262 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.25 clust=0.00 :: Air pressure  [pages: 48]\n",
      " 5. p  23 h1 score=0.162 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.39 clust=0.18 :: Engine oil and filter  [pages: 23]\n",
      " 6. p  26 h1 score=0.154 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.57 clust=0.00 :: Checking the engine oil and filter  [pages: 26]\n",
      " 7. p  23 h1 score=0.143 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.34 clust=0.00 :: Fuel filter  [pages: 23]\n",
      " 8. p  33 h1 score=0.137 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.23 clust=0.00 :: Filter inspection  [pages: 33]\n",
      " 9. p  32 h1 score=0.137 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.22 clust=0.00 :: Filter replacement  [pages: 32]\n",
      "10. p  33 h1 score=0.137 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.22 clust=0.00 :: Filter replacement  [pages: 33,34]\n",
      "\n",
      "=== PLAIN TEXT EXTRACTION ===\n",
      "\n",
      "--- Section 1: Air cleaner filter (Page 23, Score: 0.400) ---\n",
      "A genuine HYUNDAI air cleaner filter is recommended when the filter is replaced.\n",
      "\n",
      "--- Section 2: Changing coolant (Page 30, Score: 0.369) ---\n",
      "Have the coolant changed by an authorized HYUNDAI dealer according to the Maintenance Schedule at the beginning of this chapter.\n",
      "\n",
      "--- Section 3: Air conditioning refrigerant (Page 25, Score: 0.262) ---\n",
      "Check the air conditioning lines and connections for leakage and damage.\n",
      "\n",
      "--- Section 4: Air pressure (Page 48, Score: 0.262) ---\n",
      "The amount of air inside the tire pressing outward on the tire. Air pressure is expressed in pounds per square inch (psi) or kilopascal (kPa).\n",
      "\n",
      "--- Section 5: Engine oil and filter (Page 23, Score: 0.162) ---\n",
      "The engine oil and filter should be changed at the intervals specified in the maintenance schedule. If the vehicle is being driven in severe conditions, more frequent oil and filter changes are required.\n",
      "\n",
      "=== SIMPLE HTML EXPORT WITH EMBEDDED IMAGES ===\n",
      "Header 'Air cleaner filter' found at index 341\n",
      "Exporting section 'Air cleaner filter' (elements 341 to 391) to 'change_cabin.html'...\n",
      "✅ HTML with embedded images exported successfully to 'change_cabin.html'\n",
      "✅ File created: 9601 bytes\n",
      "ℹ️  No embedded images found in preview (may be further down)\n",
      "✅ HTML with embedded images exported successfully to 'change_cabin.html'\n",
      "✅ File created: 9601 bytes\n",
      "ℹ️  No embedded images found in preview (may be further down)\n"
     ]
    }
   ],
   "source": [
    "# Test both functions\n",
    "query = \"How do I change the Cabin Air Filter\"\n",
    "print(f\"Testing section reconstruction for query: '{query}'\")\n",
    "\n",
    "# Extract key terms from query for section boundary detection\n",
    "import re\n",
    "query_terms = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "query_terms = [term for term in query_terms if len(term) > 2 and term not in ['where', 'are', 'the', 'what', 'how', 'when', 'why', 'which', 'can', 'you', 'tell', 'me', 'about']]\n",
    "print(f\"Extracted query terms: {query_terms}\")\n",
    "\n",
    "# Get search results\n",
    "results = rank_and_save_best_section_with_hdbscan(doc, query, top_n=10, hdbscan_results=hdbscan_results)\n",
    "\n",
    "print(\"\\n=== PLAIN TEXT EXTRACTION ===\")\n",
    "for i, result in enumerate(results[:5], 1):  # Show top 5 sections\n",
    "    title = result.get('title', '')\n",
    "    page = result.get('first_doc_page', 'N/A')\n",
    "    score = result.get('score', 0.0)\n",
    "\n",
    "    print(f\"\\n--- Section {i}: {title} (Page {page}, Score: {score:.3f}) ---\")\n",
    "    section_text = get_section_text_content(doc, title, page)\n",
    "    print(section_text[:1000] + \"...\" if len(section_text) > 1000 else section_text)\n",
    "\n",
    "print(\"\\n=== SIMPLE HTML EXPORT WITH EMBEDDED IMAGES ===\")\n",
    "# Use Docling's built-in HTML export with embedded images - much simpler!\n",
    "if results:\n",
    "    first_result = results[0]\n",
    "    title = first_result.get('title', '')\n",
    "    page = first_result.get('first_doc_page', 'N/A')\n",
    "    header_item = first_result.get('header_item')\n",
    "\n",
    "    if header_item:\n",
    "        # Find the index of the header item\n",
    "        try:\n",
    "            start_element = doc.texts.index(header_item)\n",
    "            print(f\"Header '{title}' found at index {start_element}\")\n",
    "\n",
    "            # Find section boundaries (simple approach)\n",
    "            end_element = start_element + 50  # Include next 50 elements as a reasonable section size\n",
    "\n",
    "            # Create filename from query terms\n",
    "            filename_terms = \"_\".join(query_terms[:2]) if len(query_terms) >= 2 else query_terms[0] if query_terms else \"section\"\n",
    "            html_filename = f\"{filename_terms}.html\"\n",
    "\n",
    "            print(f\"Exporting section '{title}' (elements {start_element} to {end_element}) to '{html_filename}'...\")\n",
    "\n",
    "            # Use Docling's simple HTML export with embedded images - this handles everything!\n",
    "            doc.save_as_html(html_filename, from_element=start_element, to_element=end_element, image_mode=ImageRefMode.EMBEDDED)\n",
    "\n",
    "            print(f\"✅ HTML with embedded images exported successfully to '{html_filename}'\")\n",
    "\n",
    "            # Verify the file was created and show a preview\n",
    "            if os.path.exists(html_filename):\n",
    "                file_size = os.path.getsize(html_filename)\n",
    "                print(f\"✅ File created: {file_size} bytes\")\n",
    "\n",
    "                # Show first few lines to verify it contains images\n",
    "                with open(html_filename, 'r', encoding='utf-8') as f:\n",
    "                    preview = f.read(1000)\n",
    "                    if 'data:image/png;base64' in preview:\n",
    "                        print(\"✅ Images are embedded as base64 in the HTML\")\n",
    "                    else:\n",
    "                        print(\"ℹ️  No embedded images found in preview (may be further down)\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(f\"❌ Header item not found in doc.texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to export HTML: {e}\")\n",
    "    else:\n",
    "        print(\"❌ No header_item in results\")\n",
    "else:\n",
    "    print(\"❌ No search results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4cab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIMPLE RANKING TEST ===\n",
      "Top headers:\n",
      " 1. p  23 h1 score=0.400 | noun=0.67 verb=0.00 n_syn=0.94 v_syn=0.00 fuzz=0.52 clust=0.00 :: Air cleaner filter  [pages: 23]\n",
      " 2. p  30 h1 score=0.369 | noun=0.00 verb=1.00 n_syn=0.00 v_syn=1.00 fuzz=0.38 clust=0.00 :: Changing coolant  [pages: 30]\n",
      " 3. p  25 h1 score=0.262 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.25 clust=0.00 :: Air conditioning refrigerant  [pages: 25]\n",
      "Got 3 results\n",
      "1. Air cleaner filter (score: 0.400)\n",
      "2. Changing coolant (score: 0.369)\n",
      "3. Air conditioning refrigerant (score: 0.262)\n"
     ]
    }
   ],
   "source": [
    "# Simple test of the ranking function\n",
    "print(\"=== SIMPLE RANKING TEST ===\")\n",
    "try:\n",
    "    test_results = rank_and_save_best_section_with_hdbscan(doc, \"How do I change the Cabin Air Filter\", top_n=10, hdbscan_results=hdbscan_results)\n",
    "    print(f\"Got {len(test_results)} results\")\n",
    "    for i, r in enumerate(test_results[:3], 1):\n",
    "        print(f\"{i}. {r['title']} (score: {r['score']:.3f})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calling ranking function: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ad216aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3 results\n",
      "1. Air cleaner filter (score: 0.400)\n",
      "\n",
      "2. Changing coolant (score: 0.369)\n",
      "\n",
      "3. Air conditioning refrigerant (score: 0.262)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Got {len(test_results)} results\")\n",
    "for i, r in enumerate(test_results, 1):\n",
    "    print(f\"{i}. {r['title']} (score: {r['score']:.3f})\")\n",
    "    if 'debug_info' in r:\n",
    "        debug = r['debug_info']\n",
    "        print(f\"   Debug: text_len={debug.get('section_text_length', 0)}, has_vectors={debug.get('has_vectors', False)}, content_sim={debug.get('content_sim_raw', 0):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "524b47b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_results has 3 items\n",
      "1. Air cleaner filter (score: 0.400, content_sim: 0.000)\n",
      "\n",
      "2. Changing coolant (score: 0.369, content_sim: 0.000)\n",
      "\n",
      "3. Air conditioning refrigerant (score: 0.262, content_sim: 0.000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"test_results has {len(test_results)} items\")\n",
    "for i, r in enumerate(test_results, 1):\n",
    "    print(f\"{i}. {r['title']} (score: {r['score']:.3f}, content_sim: {r.get('content_sim', 0):.3f})\")\n",
    "    if 'debug_info' in r:\n",
    "        debug = r['debug_info']\n",
    "        print(f\"   Debug: text_len={debug.get('section_text_length', 0)}, has_vectors={debug.get('has_vectors', False)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a9efb3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIRECT TEST ===\n",
      "This should definitely print\n",
      "Trying to access rank_and_save_best_section_with_hdbscan...\n",
      "Function found: <function rank_and_save_best_section_with_hdbscan at 0x000002C911D4D4E0>\n",
      "doc has 1407 texts\n",
      "Query: How do I change the Cabin Air Filter\n",
      "Calling function with minimal params...\n",
      "Top headers:\n",
      " 1. p  50 h1 score=0.022 | noun=0.00 verb=0.00 n_syn=0.00 v_syn=0.00 fuzz=0.44 clust=0.00 :: Tread  [pages: 50]\n",
      " 2. p  50 h1 score=0.022 | noun=0.00 verb=0.00 n_syn=0.00 v_syn=0.00 fuzz=0.44 clust=0.00 :: UTQGS  [pages: 50]\n",
      " 3. p  53 h1 score=0.022 | noun=0.00 verb=0.00 n_syn=0.00 v_syn=0.00 fuzz=0.44 clust=0.00 :: Fuses  [pages: 53]\n",
      " 4. p  51 h1 score=0.021 | noun=0.00 verb=0.00 n_syn=0.00 v_syn=0.00 fuzz=0.43 clust=0.00 :: Snow tires  [pages: 51]\n",
      " 5. p   2 h1 score=0.020 | noun=0.00 verb=0.00 n_syn=0.00 v_syn=0.00 fuzz=0.40 clust=0.00 :: NOTICE  [pages: 2]\n",
      "Function returned 5 results\n"
     ]
    }
   ],
   "source": [
    "# Try a completely different approach - simple direct test\n",
    "print(\"=== DIRECT TEST ===\")\n",
    "print(\"This should definitely print\")\n",
    "\n",
    "# Test if we can access the function\n",
    "try:\n",
    "    print(\"Trying to access rank_and_save_best_section_with_hdbscan...\")\n",
    "    func = rank_and_save_best_section_with_hdbscan\n",
    "    print(f\"Function found: {func}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing function: {e}\")\n",
    "\n",
    "# Test basic operations\n",
    "print(f\"doc has {len(doc.texts)} texts\")\n",
    "print(f\"Query: How do I change the Cabin Air Filter\")\n",
    "\n",
    "# Try calling the function with minimal parameters\n",
    "try:\n",
    "    print(\"Calling function with minimal params...\")\n",
    "    result = rank_and_save_best_section_with_hdbscan(doc, \"test\")\n",
    "    print(f\"Function returned {len(result)} results\")\n",
    "except Exception as e:\n",
    "    print(f\"Function call failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ea1f9222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CABIN AIR FILTER TEST ===\n",
      "Top headers:\n",
      " 1. p  23 h1 score=0.400 | noun=0.67 verb=0.00 n_syn=0.94 v_syn=0.00 fuzz=0.52 clust=0.00 :: Air cleaner filter  [pages: 23]\n",
      " 2. p  30 h1 score=0.369 | noun=0.00 verb=1.00 n_syn=0.00 v_syn=1.00 fuzz=0.38 clust=0.00 :: Changing coolant  [pages: 30]\n",
      " 3. p  25 h1 score=0.262 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.25 clust=0.00 :: Air conditioning refrigerant  [pages: 25]\n",
      " 4. p  48 h1 score=0.262 | noun=0.33 verb=0.00 n_syn=0.88 v_syn=0.00 fuzz=0.25 clust=0.00 :: Air pressure  [pages: 48]\n",
      " 5. p  26 h1 score=0.154 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.57 clust=0.00 :: Checking the engine oil and filter  [pages: 26]\n",
      " 6. p  23 h1 score=0.145 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.39 clust=0.00 :: Engine oil and filter  [pages: 23]\n",
      " 7. p  23 h1 score=0.143 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.34 clust=0.00 :: Fuel filter  [pages: 23]\n",
      " 8. p  33 h1 score=0.137 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.23 clust=0.00 :: Filter inspection  [pages: 33]\n",
      " 9. p  32 h1 score=0.137 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.22 clust=0.00 :: Filter replacement  [pages: 32]\n",
      "10. p  33 h1 score=0.137 | noun=0.33 verb=0.00 n_syn=0.06 v_syn=0.00 fuzz=0.22 clust=0.00 :: Filter replacement  [pages: 33,34]\n",
      "Got 10 results for cabin air filter query\n",
      "1. Air cleaner filter (score: 0.400, content_sim: 0.000)\n",
      "\n",
      "2. Changing coolant (score: 0.369, content_sim: 0.000)\n",
      "\n",
      "3. Air conditioning refrigerant (score: 0.262, content_sim: 0.000)\n",
      "\n",
      "4. Air pressure (score: 0.262, content_sim: 0.000)\n",
      "\n",
      "5. Checking the engine oil and filter (score: 0.154, content_sim: 0.000)\n",
      "\n",
      "6. Engine oil and filter (score: 0.145, content_sim: 0.000)\n",
      "\n",
      "7. Fuel filter (score: 0.143, content_sim: 0.000)\n",
      "\n",
      "8. Filter inspection (score: 0.137, content_sim: 0.000)\n",
      "\n",
      "9. Filter replacement (score: 0.137, content_sim: 0.000)\n",
      "  *** FOUND FILTER REPLACEMENT ***\n",
      "\n",
      "10. Filter replacement (score: 0.137, content_sim: 0.000)\n",
      "  *** FOUND FILTER REPLACEMENT ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now test the cabin air filter query specifically\n",
    "print(\"\\n=== CABIN AIR FILTER TEST ===\")\n",
    "cabin_results = rank_and_save_best_section_with_hdbscan(doc, \"How do I change the Cabin Air Filter\", top_n=10)\n",
    "print(f\"Got {len(cabin_results)} results for cabin air filter query\")\n",
    "\n",
    "# Look for Filter replacement in results\n",
    "for i, r in enumerate(cabin_results, 1):\n",
    "    print(f\"{i}. {r['title']} (score: {r['score']:.3f}, content_sim: {r.get('content_sim', 0):.3f})\")\n",
    "    if 'filter replacement' in r['title'].lower():\n",
    "        print(\"  *** FOUND FILTER REPLACEMENT ***\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG Implementation with Embeddings\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from docling_core.types.doc.document import SectionHeaderItem, TextItem, ListItem, PictureItem\n",
    "from docling_core.types.doc.base import ImageRefMode\n",
    "import spacy\n",
    "\n",
    "def extract_all_sections_with_content(doc: DoclingDocument) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract all sections with substantial content from a DoclingDocument.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with section information:\n",
    "        - 'header': Section header text\n",
    "        - 'content': Full section content text\n",
    "        - 'start_idx': Starting index in doc.texts\n",
    "        - 'end_idx': Ending index in doc.texts\n",
    "        - 'level': Header level\n",
    "        - 'pages': Set of page numbers\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    def is_body(item: Any) -> bool:\n",
    "        \"\"\"Check if item belongs to body content layer.\"\"\"\n",
    "        content_layer = getattr(item, \"content_layer\", None)\n",
    "        return getattr(content_layer, \"value\", content_layer) == \"body\"\n",
    "    \n",
    "    def item_pages(item: Any) -> Set[int]:\n",
    "        \"\"\"Extract page numbers from an item.\"\"\"\n",
    "        pages = set()\n",
    "        prov = getattr(item, \"prov\", None)\n",
    "        if prov:\n",
    "            for p in prov:\n",
    "                pg = getattr(p, \"page_no\", None)\n",
    "                if pg is not None:\n",
    "                    try:\n",
    "                        pages.add(int(pg))\n",
    "                    except:\n",
    "                        pass\n",
    "        pr = getattr(item, \"page_ref\", None)\n",
    "        if pr is not None and not pages:\n",
    "            try:\n",
    "                pages.add(int(pr) + 1)\n",
    "            except:\n",
    "                pages.add(1)\n",
    "        return pages\n",
    "    \n",
    "    def has_content(nodes: List[Any]) -> bool:\n",
    "        \"\"\"Check if section nodes contain meaningful content.\"\"\"\n",
    "        textish = 0\n",
    "        structural = 0\n",
    "        for node in nodes:\n",
    "            name = node.__class__.__name__.lower()\n",
    "            if hasattr(node, \"text\") and name != \"sectionheaderitem\":\n",
    "                if re.search(r\"\\w\", getattr(node, \"text\", \"\") or \"\"):\n",
    "                    textish += 1\n",
    "            if hasattr(node, \"items\") or hasattr(node, \"num_rows\") or hasattr(node, \"caption\"):\n",
    "                structural += 1\n",
    "        return textish >= 1 or structural >= 1\n",
    "    \n",
    "    def slice_section(start_idx: int) -> Tuple[str, List[Any], int]:\n",
    "        \"\"\"Slice document to get section content under a header.\"\"\"\n",
    "        header_item = doc.texts[start_idx]\n",
    "        header_text = getattr(header_item, \"text\", \"\") or \"\"\n",
    "        level = getattr(header_item, \"level\", 3)\n",
    "        \n",
    "        nodes = []\n",
    "        end_idx = start_idx\n",
    "        \n",
    "        for j in range(start_idx + 1, len(doc.texts)):\n",
    "            item = doc.texts[j]\n",
    "            if not is_body(item):\n",
    "                continue\n",
    "            if isinstance(item, SectionHeaderItem):\n",
    "                item_level = getattr(item, \"level\", 3)\n",
    "                if item_level <= level:\n",
    "                    break\n",
    "            nodes.append(item)\n",
    "            end_idx = j\n",
    "        \n",
    "        return header_text, nodes, end_idx\n",
    "    \n",
    "    # Extract all sections\n",
    "    i = 0\n",
    "    while i < len(doc.texts):\n",
    "        item = doc.texts[i]\n",
    "        if isinstance(item, SectionHeaderItem) and is_body(item):\n",
    "            header_text, nodes, end_idx = slice_section(i)\n",
    "            \n",
    "            if has_content(nodes):\n",
    "                # Collect all text content\n",
    "                content_parts = []\n",
    "                all_pages = item_pages(item)\n",
    "                \n",
    "                for node in nodes:\n",
    "                    all_pages |= item_pages(node)\n",
    "                    if hasattr(node, \"text\") and node.__class__.__name__.lower() != \"sectionheaderitem\":\n",
    "                        text = getattr(node, \"text\", \"\").strip()\n",
    "                        if text:\n",
    "                            content_parts.append(text)\n",
    "                \n",
    "                content = \"\\n\\n\".join(content_parts)\n",
    "                \n",
    "                if content.strip():  # Only include sections with actual content\n",
    "                    sections.append({\n",
    "                        'header': header_text,\n",
    "                        'content': content,\n",
    "                        'start_idx': i,\n",
    "                        'end_idx': end_idx,\n",
    "                        'level': getattr(item, \"level\", 3),\n",
    "                        'pages': sorted(all_pages),\n",
    "                        'header_item': item\n",
    "                    })\n",
    "            \n",
    "            i = end_idx + 1\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    print(f\"Extracted {len(sections)} sections with content\")\n",
    "    return sections\n",
    "\n",
    "def compute_section_embeddings(sections: List[Dict[str, Any]], nlp) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Compute embeddings for each section using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        sections: List of section dictionaries\n",
    "        nlp: spaCy language model\n",
    "    \n",
    "    Returns:\n",
    "        Sections with 'embedding' field added\n",
    "    \"\"\"\n",
    "    embedded_sections = []\n",
    "    \n",
    "    for section in sections:\n",
    "        content = section['content']\n",
    "        \n",
    "        # Limit content length for performance (first 1000 words)\n",
    "        words = content.split()[:1000]\n",
    "        limited_content = \" \".join(words)\n",
    "        \n",
    "        # Compute embedding\n",
    "        doc = nlp(limited_content)\n",
    "        if doc.has_vector:\n",
    "            section_with_embedding = section.copy()\n",
    "            section_with_embedding['embedding'] = doc.vector\n",
    "            section_with_embedding['content_length'] = len(limited_content)\n",
    "            embedded_sections.append(section_with_embedding)\n",
    "        else:\n",
    "            print(f\"Warning: Could not compute embedding for section '{section['header'][:50]}...'\")\n",
    "    \n",
    "    print(f\"Computed embeddings for {len(embedded_sections)} sections\")\n",
    "    return embedded_sections\n",
    "\n",
    "def simple_rag_search(sections: List[Dict[str, Any]], query: str, nlp, top_n: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform simple RAG search using cosine similarity on embeddings.\n",
    "    \n",
    "    Args:\n",
    "        sections: List of sections with embeddings\n",
    "        query: Search query\n",
    "        nlp: spaCy language model\n",
    "        top_n: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of top matching sections with similarity scores\n",
    "    \"\"\"\n",
    "    if not sections:\n",
    "        return []\n",
    "    \n",
    "    # Compute query embedding\n",
    "    query_doc = nlp(query)\n",
    "    if not query_doc.has_vector:\n",
    "        print(\"Warning: Could not compute query embedding\")\n",
    "        return []\n",
    "    \n",
    "    query_vector = query_doc.vector\n",
    "    \n",
    "    # Compute similarities\n",
    "    results = []\n",
    "    for section in sections:\n",
    "        embedding = section.get('embedding')\n",
    "        if embedding is not None:\n",
    "            similarity = cosine_similarity([query_vector], [embedding])[0][0]\n",
    "            result = section.copy()\n",
    "            result['similarity_score'] = float(similarity)\n",
    "            results.append(result)\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "    \n",
    "    print(f\"Found {len(results)} sections with embeddings\")\n",
    "    print(f\"Top {min(top_n, len(results))} results:\")\n",
    "    for i, result in enumerate(results[:top_n], 1):\n",
    "        print(f\"{i}. {result['header']} (score: {result['similarity_score']:.3f})\")\n",
    "    \n",
    "    return results[:top_n]\n",
    "\n",
    "\n",
    "\n",
    "def run_simple_rag(doc: DoclingDocument, query: str, output_file: str = \"rag_results.html\", top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Run the complete simple RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        doc: DoclingDocument to search\n",
    "        query: Search query\n",
    "        output_file: Output HTML filename\n",
    "        top_n: Number of top results to return\n",
    "    \"\"\"\n",
    "    print(f\"Running Simple RAG for query: '{query}'\")\n",
    "    \n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    # Extract sections\n",
    "    print(\"1. Extracting sections...\")\n",
    "    sections = extract_all_sections_with_content(doc)\n",
    "    \n",
    "    # Compute embeddings\n",
    "    print(\"2. Computing embeddings...\")\n",
    "    embedded_sections = compute_section_embeddings(sections, nlp)\n",
    "    \n",
    "    # Perform search\n",
    "    print(\"3. Performing search...\")\n",
    "    results = simple_rag_search(embedded_sections, query, nlp, top_n=top_n)\n",
    "    \n",
    "    # Save as HTML\n",
    "    print(\"4. Saving results as HTML...\")\n",
    "    save_sections_as_html(results, output_file, doc)\n",
    "    \n",
    "    print(f\"✅ Simple RAG completed! Results saved to {output_file}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "54ee4f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing improved RAG system:\n",
      "Running Improved Simple RAG for query: 'cabin air filter replacement'\n",
      "1. Extracting sections...\n",
      "Extracted 242 sections with content\n",
      "2. Computing embeddings...\n",
      "Warning: Could not compute embedding for section 'Exterior Overview (Front View)...'\n",
      "Warning: Could not compute embedding for section 'Exterior Overview (Rear View)...'\n",
      "Warning: Could not compute embedding for section 'Interior Overview...'\n",
      "Warning: Could not compute embedding for section 'Center Console Overview...'\n",
      "Warning: Could not compute embedding for section 'Seats...'\n",
      "Warning: Could not compute embedding for section 'Seatback angle...'\n",
      "Warning: Could not compute embedding for section 'Seat height...'\n",
      "Warning: Could not compute embedding for section 'Forward and rearward adjustment...'\n",
      "Warning: Could not compute embedding for section 'Seatback angle adjustment...'\n",
      "Warning: Could not compute embedding for section 'Lumbar support...'\n",
      "Warning: Could not compute embedding for section 'Reclining the rear seats...'\n",
      "Warning: Could not compute embedding for section 'Adjusting the height up and down...'\n",
      "Warning: Could not compute embedding for section 'Adjusting the height up and down...'\n",
      "Warning: Could not compute embedding for section 'Top tether anchorages are located on the rear of t...'\n",
      "Computed embeddings for 228 sections\n",
      "3. Performing improved search (header + content)...\n",
      "Top 3 results:\n",
      "1. Air Ventilation Seats (Score: 0.740)\n",
      "2. Air Conditioning System (Score: 0.676)\n",
      "3. Air Conditioner Compressor Label (Score: 0.674)\n",
      "4. Saving results as HTML...\n",
      "✅ Saved 3 sections to cabin_filter_improved.html using Docling's save_as_html with element ranges\n",
      "✅ Improved Simple RAG completed! Results saved to cabin_filter_improved.html\n"
     ]
    }
   ],
   "source": [
    "# Update the simple_rag_search function to use content + headers\n",
    "def simple_rag_search_improved(sections_with_embeddings, query, nlp, top_n=5):\n",
    "    \"\"\"Improved search using both header and content for better semantic matching.\"\"\"\n",
    "    query_doc = nlp(query)\n",
    "    query_vector = query_doc.vector\n",
    "\n",
    "    results = []\n",
    "    for section in sections_with_embeddings:\n",
    "        header = section['header']\n",
    "        content = section.get('content', '')\n",
    "\n",
    "        # Combine header and content for better semantic matching\n",
    "        combined_text = f\"{header} {content}\"\n",
    "        if len(combined_text.strip()) < 10:  # Skip if too short\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            combined_doc = nlp(combined_text)\n",
    "            combined_vector = combined_doc.vector\n",
    "\n",
    "            # Compute similarity\n",
    "            similarity = cosine_similarity([query_vector], [combined_vector])[0][0]\n",
    "\n",
    "            results.append({\n",
    "                'header': header,\n",
    "                'content': content[:500],  # Truncate for display\n",
    "                'similarity_score': float(similarity),\n",
    "                'pages': section.get('pages', []),\n",
    "                'start_idx': section.get('start_idx'),\n",
    "                'end_idx': section.get('end_idx')\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process section '{header}': {e}\")\n",
    "            continue\n",
    "\n",
    "    # Sort by similarity score\n",
    "    results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "\n",
    "    print(f\"Top {top_n} results:\")\n",
    "    for i, result in enumerate(results[:top_n], 1):\n",
    "        print(f\"{i}. {result['header']} (Score: {result['similarity_score']:.3f})\")\n",
    "\n",
    "    return results[:top_n]\n",
    "\n",
    "# Update the run_simple_rag function to use the improved search\n",
    "def run_simple_rag_improved(doc: DoclingDocument, query: str, output_file: str = \"rag_results.html\", top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Run the complete improved simple RAG pipeline with content-based search and correct Docling HTML export.\n",
    "    \"\"\"\n",
    "    print(f\"Running Improved Simple RAG for query: '{query}'\")\n",
    "\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "    # Extract sections\n",
    "    print(\"1. Extracting sections...\")\n",
    "    sections = extract_all_sections_with_content(doc)\n",
    "\n",
    "    # Compute embeddings\n",
    "    print(\"2. Computing embeddings...\")\n",
    "    embedded_sections = compute_section_embeddings(sections, nlp)\n",
    "\n",
    "    # Perform improved search (header + content)\n",
    "    print(\"3. Performing improved search (header + content)...\")\n",
    "    results = simple_rag_search_improved(embedded_sections, query, nlp, top_n=top_n)\n",
    "\n",
    "    # Save as HTML using corrected function\n",
    "    print(\"4. Saving results as HTML...\")\n",
    "    save_sections_as_html_corrected(results, output_file, doc)\n",
    "\n",
    "    print(f\"✅ Improved Simple RAG completed! Results saved to {output_file}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test the improved system\n",
    "print(\"Testing improved RAG system:\")\n",
    "improved_results = run_simple_rag_improved(concatenated, \"cabin air filter replacement\", \"cabin_filter_improved.html\", top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "b770f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if 'Filter replacement' sections are in embedded_sections:\n",
      "Found: Filter replacement\n",
      "Content contains 'cabin': False\n",
      "Content preview: Smartstream G1.6 T-GDi\n",
      "\n",
      "2C_AirCleaner\n",
      "\n",
      "Smartstream G2.0 ATKINSON\n",
      "\n",
      "2C_AirCleaner_2\n",
      "\n",
      "The air cleaner filter can be cleaned for inspection using compressed air.\n",
      "\n",
      "Do not attempt to wash or to rinse it, as water will damage the filter.\n",
      "\n",
      "If soiled, the air cleaner filter must be replaced.\n",
      "\n",
      "Replace the fil...\n",
      "\n",
      "Found: Filter replacement\n",
      "Content contains 'cabin': True\n",
      "Content preview: Open the glove box and remove the support rod (1).\n",
      "\n",
      "2C_AirFilterReplacementProcedure\n",
      "\n",
      "Press both sides of the glove box inward to release.\n",
      "\n",
      "2C_AirFilterReplacementProcedure_2\n",
      "\n",
      "Press and hold the lock on the right side of the cover.\n",
      "\n",
      "2C_AirFilterReplacementProcedure_3\n",
      "\n",
      "4.Pull out the cover.\n",
      "\n",
      "Replace ...\n",
      "\n",
      "Total 'Filter replacement' sections found: 2\n",
      "\n",
      "Testing with 'air cleaner filter replacement':\n",
      "Running Improved Simple RAG for query: 'air cleaner filter replacement'\n",
      "1. Extracting sections...\n",
      "Extracted 242 sections with content\n",
      "2. Computing embeddings...\n",
      "Warning: Could not compute embedding for section 'Exterior Overview (Front View)...'\n",
      "Warning: Could not compute embedding for section 'Exterior Overview (Rear View)...'\n",
      "Warning: Could not compute embedding for section 'Interior Overview...'\n",
      "Warning: Could not compute embedding for section 'Center Console Overview...'\n",
      "Warning: Could not compute embedding for section 'Seats...'\n",
      "Warning: Could not compute embedding for section 'Seatback angle...'\n",
      "Warning: Could not compute embedding for section 'Seat height...'\n",
      "Warning: Could not compute embedding for section 'Forward and rearward adjustment...'\n",
      "Warning: Could not compute embedding for section 'Seatback angle adjustment...'\n",
      "Warning: Could not compute embedding for section 'Lumbar support...'\n",
      "Warning: Could not compute embedding for section 'Reclining the rear seats...'\n",
      "Warning: Could not compute embedding for section 'Adjusting the height up and down...'\n",
      "Warning: Could not compute embedding for section 'Adjusting the height up and down...'\n",
      "Warning: Could not compute embedding for section 'Top tether anchorages are located on the rear of t...'\n",
      "Computed embeddings for 228 sections\n",
      "3. Performing improved search (header + content)...\n",
      "Top 5 results:\n",
      "1. Air Conditioner Compressor Label (Score: 0.659)\n",
      "2. Air Ventilation Seats (Score: 0.655)\n",
      "3. Air Conditioning System (Score: 0.638)\n",
      "4. NOTICE (Score: 0.626)\n",
      "5. Using Fuel Additives (except Detergent Fuel Additives) (Score: 0.622)\n",
      "4. Saving results as HTML...\n",
      "✅ Saved 5 sections to air_cleaner_filter.html using Docling's save_as_html with element ranges\n",
      "✅ Improved Simple RAG completed! Results saved to air_cleaner_filter.html\n",
      "\n",
      "Testing with just 'filter replacement':\n",
      "Running Improved Simple RAG for query: 'filter replacement'\n",
      "1. Extracting sections...\n",
      "Extracted 242 sections with content\n",
      "2. Computing embeddings...\n",
      "Warning: Could not compute embedding for section 'Exterior Overview (Front View)...'\n",
      "Warning: Could not compute embedding for section 'Exterior Overview (Rear View)...'\n",
      "Warning: Could not compute embedding for section 'Interior Overview...'\n",
      "Warning: Could not compute embedding for section 'Center Console Overview...'\n",
      "Warning: Could not compute embedding for section 'Seats...'\n",
      "Warning: Could not compute embedding for section 'Seatback angle...'\n",
      "Warning: Could not compute embedding for section 'Seat height...'\n",
      "Warning: Could not compute embedding for section 'Forward and rearward adjustment...'\n",
      "Warning: Could not compute embedding for section 'Seatback angle adjustment...'\n",
      "Warning: Could not compute embedding for section 'Lumbar support...'\n",
      "Warning: Could not compute embedding for section 'Reclining the rear seats...'\n",
      "Warning: Could not compute embedding for section 'Adjusting the height up and down...'\n",
      "Warning: Could not compute embedding for section 'Adjusting the height up and down...'\n",
      "Warning: Could not compute embedding for section 'Top tether anchorages are located on the rear of t...'\n",
      "Computed embeddings for 228 sections\n",
      "3. Performing improved search (header + content)...\n",
      "Top 5 results:\n",
      "1. Air Conditioner Compressor Label (Score: 0.542)\n",
      "2. SRS components (Score: 0.524)\n",
      "3. Using Fuel Additives (except Detergent Fuel Additives) (Score: 0.520)\n",
      "4. NOTICE (Score: 0.518)\n",
      "5. Air Conditioning System (Score: 0.510)\n",
      "4. Saving results as HTML...\n",
      "✅ Saved 5 sections to just_filter.html using Docling's save_as_html with element ranges\n",
      "✅ Improved Simple RAG completed! Results saved to just_filter.html\n"
     ]
    }
   ],
   "source": [
    "# Let's debug why \"Filter replacement\" isn't showing up in top results\n",
    "print(\"Checking if 'Filter replacement' sections are in embedded_sections:\")\n",
    "filter_sections_found = []\n",
    "for section in embedded_sections:\n",
    "    if 'filter replacement' in section['header'].lower():\n",
    "        filter_sections_found.append(section)\n",
    "        print(f\"Found: {section['header']}\")\n",
    "        print(f\"Content contains 'cabin': {'cabin' in section.get('content', '').lower()}\")\n",
    "        print(f\"Content preview: {section.get('content', '')[:300]}...\")\n",
    "        print()\n",
    "\n",
    "print(f\"Total 'Filter replacement' sections found: {len(filter_sections_found)}\")\n",
    "\n",
    "# Try a more specific query that matches the content better\n",
    "print(\"\\nTesting with 'air cleaner filter replacement':\")\n",
    "specific_results = run_simple_rag_improved(concatenated, \"air cleaner filter replacement\", \"air_cleaner_filter.html\", top_n=5)\n",
    "\n",
    "# Also try just \"filter replacement\" without \"cabin\"\n",
    "print(\"\\nTesting with just 'filter replacement':\")\n",
    "just_filter_results = run_simple_rag_improved(concatenated, \"filter replacement\", \"just_filter.html\", top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "46dceed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual check of filter replacement sections:\n",
      "Section 95: Filter replacement\n",
      "  Start idx: 518\n",
      "  End idx: 533\n",
      "  Content length: 629\n",
      "  In embedded_sections: True\n",
      "\n",
      "Section 99: Filter replacement\n",
      "  Start idx: 547\n",
      "  End idx: 558\n",
      "  Content length: 372\n",
      "  In embedded_sections: True\n",
      "\n",
      "Testing manual similarity calculation for filter replacement sections:\n",
      "'Filter replacement' similarity to 'filter replacement': 0.638\n",
      "'Filter replacement' similarity to 'filter replacement': 0.526\n"
     ]
    }
   ],
   "source": [
    "# Let's manually check the filter replacement sections\n",
    "print(\"Manual check of filter replacement sections:\")\n",
    "for i, section in enumerate(sections):\n",
    "    if 'filter replacement' in section['header'].lower():\n",
    "        print(f\"Section {i}: {section['header']}\")\n",
    "        print(f\"  Start idx: {section.get('start_idx')}\")\n",
    "        print(f\"  End idx: {section.get('end_idx')}\")\n",
    "        print(f\"  Content length: {len(section.get('content', ''))}\")\n",
    "        print(f\"  In embedded_sections: {any(s['header'] == section['header'] and s.get('start_idx') == section.get('start_idx') for s in embedded_sections)}\")\n",
    "        print()\n",
    "\n",
    "# Let's also check what happens during the search process\n",
    "print(\"Testing manual similarity calculation for filter replacement sections:\")\n",
    "query = \"filter replacement\"\n",
    "query_doc = nlp(query)\n",
    "query_vector = query_doc.vector\n",
    "\n",
    "for section in embedded_sections:\n",
    "    if 'filter replacement' in section['header'].lower():\n",
    "        header = section['header']\n",
    "        content = section.get('content', '')\n",
    "        combined_text = f\"{header} {content}\"\n",
    "\n",
    "        try:\n",
    "            combined_doc = nlp(combined_text)\n",
    "            combined_vector = combined_doc.vector\n",
    "            similarity = cosine_similarity([query_vector], [combined_vector])[0][0]\n",
    "            print(f\"'{header}' similarity to '{query}': {similarity:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing '{header}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ab39aa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of Filter replacement sections:\n",
      "\n",
      "Section 95: Filter replacement\n",
      "Content: Smartstream G1.6 T-GDi\n",
      "\n",
      "2C_AirCleaner\n",
      "\n",
      "Smartstream G2.0 ATKINSON\n",
      "\n",
      "2C_AirCleaner_2\n",
      "\n",
      "The air cleaner filter can be cleaned for inspection using compressed air.\n",
      "\n",
      "Do not attempt to wash or to rinse it, as water will damage the filter.\n",
      "\n",
      "If soiled, the air cleaner filter must be replaced.\n",
      "\n",
      "Replace the filter according to the Maintenance Schedule.\n",
      "\n",
      "Pull down the air cleaner filter lever.\n",
      "\n",
      "2C_AirCleanerReplacementProcedure\n",
      "\n",
      "Pull up the air cleaner cover to open.\n",
      "\n",
      "Replace the air cleaner filter.\n",
      "\n",
      "2C_AirCleanerReplacementProcedure_3\n",
      "\n",
      "4.Reassemble the air cleaner cover in the reverse order.\n",
      "\n",
      "Check that the cover is firmly installed.\n",
      "Contains 'cabin': False\n",
      "Contains 'air': True\n",
      "--------------------------------------------------\n",
      "\n",
      "Section 99: Filter replacement\n",
      "Content: Open the glove box and remove the support rod (1).\n",
      "\n",
      "2C_AirFilterReplacementProcedure\n",
      "\n",
      "Press both sides of the glove box inward to release.\n",
      "\n",
      "2C_AirFilterReplacementProcedure_2\n",
      "\n",
      "Press and hold the lock on the right side of the cover.\n",
      "\n",
      "2C_AirFilterReplacementProcedure_3\n",
      "\n",
      "4.Pull out the cover.\n",
      "\n",
      "Replace the cabin air filter.\n",
      "\n",
      "6.Reassemble in the reverse order of disassembly.\n",
      "Contains 'cabin': True\n",
      "Contains 'air': True\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing similarity for full query 'cabin air filter replacement':\n",
      "'Filter replacement' similarity to 'cabin air filter replacement': 0.765\n",
      "'Filter replacement' similarity to 'cabin air filter replacement': 0.644\n",
      "\n",
      "Comparing with top results from 'cabin air filter replacement' query:\n"
     ]
    }
   ],
   "source": [
    "# Let's check the actual content of the filter replacement sections\n",
    "print(\"Content of Filter replacement sections:\")\n",
    "for i, section in enumerate(sections):\n",
    "    if 'filter replacement' in section['header'].lower():\n",
    "        print(f\"\\nSection {i}: {section['header']}\")\n",
    "        print(f\"Content: {section['content']}\")\n",
    "        print(f\"Contains 'cabin': {'cabin' in section['content'].lower()}\")\n",
    "        print(f\"Contains 'air': {'air' in section['content'].lower()}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Let's also test the similarity for the full query \"cabin air filter replacement\"\n",
    "print(\"\\nTesting similarity for full query 'cabin air filter replacement':\")\n",
    "full_query = \"cabin air filter replacement\"\n",
    "full_query_doc = nlp(full_query)\n",
    "full_query_vector = full_query_doc.vector\n",
    "\n",
    "for section in embedded_sections:\n",
    "    if 'filter replacement' in section['header'].lower():\n",
    "        header = section['header']\n",
    "        content = section.get('content', '')\n",
    "        combined_text = f\"{header} {content}\"\n",
    "\n",
    "        try:\n",
    "            combined_doc = nlp(combined_text)\n",
    "            combined_vector = combined_doc.vector\n",
    "            similarity = cosine_similarity([full_query_vector], [combined_vector])[0][0]\n",
    "            print(f\"'{header}' similarity to '{full_query}': {similarity:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing '{header}': {e}\")\n",
    "\n",
    "# Compare with the top results from earlier\n",
    "print(\"\\nComparing with top results from 'cabin air filter replacement' query:\")\n",
    "top_sections = [\"Air Ventilation Seats\", \"Air Conditioning System\", \"Air Conditioner Compressor Label\"]\n",
    "for section_name in top_sections:\n",
    "    for section in embedded_sections:\n",
    "        if section['header'] == section_name:\n",
    "            header = section['header']\n",
    "            content = section.get('content', '')\n",
    "            combined_text = f\"{header} {content}\"\n",
    "\n",
    "            try:\n",
    "                combined_doc = nlp(combined_text)\n",
    "                combined_vector = combined_doc.vector\n",
    "                similarity = cosine_similarity([full_query_vector], [combined_vector])[0][0]\n",
    "                print(f\"'{header}' similarity to '{full_query}': {similarity:.3f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing '{header}': {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "6d6fa61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing the comparison with top results:\n",
      "\n",
      "Running a fresh search to see all results:\n",
      "Top 10 results:\n",
      "1. Air cleaner filter (Score: 0.850)\n",
      "2. Filter replacement (Score: 0.765)\n",
      "3. Smartstream G2.0 ATKINSON (Score: 0.754)\n",
      "4. Engine Compartment (Score: 0.752)\n",
      "5. Air conditioning refrigerant (Score: 0.746)\n",
      "6. At least twice a year: (for example, every Spring and Autumn) (Score: 0.734)\n",
      "7. Crankcase emission control system (Score: 0.732)\n",
      "8. NOTICE (Score: 0.727)\n",
      "9. Cooling system (Score: 0.717)\n",
      "10. Vapor hose and fuel filler cap (Score: 0.711)\n",
      "\n",
      "Checking if 'Filter replacement' is in the results:\n",
      "FOUND: Filter replacement with score 0.765\n"
     ]
    }
   ],
   "source": [
    "# The output got cut off. Let me complete the comparison\n",
    "print(\"Completing the comparison with top results:\")\n",
    "full_query = \"cabin air filter replacement\"\n",
    "full_query_doc = nlp(full_query)\n",
    "full_query_vector = full_query_doc.vector\n",
    "\n",
    "top_sections = [\"Air Ventilation Seats\", \"Air Conditioning System\", \"Air Conditioner Compressor Label\"]\n",
    "for section_name in top_sections:\n",
    "    for section in embedded_sections:\n",
    "        if section['header'] == section_name:\n",
    "            header = section['header']\n",
    "            content = section.get('content', '')\n",
    "            combined_text = f\"{header} {content}\"\n",
    "\n",
    "            try:\n",
    "                combined_doc = nlp(combined_text)\n",
    "                combined_vector = combined_doc.vector\n",
    "                similarity = cosine_similarity([full_query_vector], [combined_vector])[0][0]\n",
    "                print(f\"'{header}' similarity to '{full_query}': {similarity:.3f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing '{header}': {e}\")\n",
    "            break\n",
    "\n",
    "# Also check if the filter replacement sections are actually being returned in the search\n",
    "print(\"\\nRunning a fresh search to see all results:\")\n",
    "all_results = simple_rag_search_improved(embedded_sections, full_query, nlp, top_n=10)\n",
    "print(\"\\nChecking if 'Filter replacement' is in the results:\")\n",
    "for result in all_results:\n",
    "    if 'filter replacement' in result['header'].lower():\n",
    "        print(f\"FOUND: {result['header']} with score {result['similarity_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "e1ab68c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking section counts:\n",
      "Total sections: 290\n",
      "Embedded sections: 289\n",
      "Sections that failed embedding: 1\n",
      "Missing sections: ['Tire replacement']\n",
      "\n",
      "Running the exact same pipeline as run_simple_rag_improved:\n",
      "Extracted 242 sections with content\n",
      "Extracted sections: 242\n",
      "Warning: Could not compute embedding for section 'Exterior Overview (Front View)...'\n",
      "Warning: Could not compute embedding for section 'Exterior Overview (Rear View)...'\n",
      "Warning: Could not compute embedding for section 'Interior Overview...'\n",
      "Warning: Could not compute embedding for section 'Center Console Overview...'\n",
      "Warning: Could not compute embedding for section 'Seats...'\n",
      "Warning: Could not compute embedding for section 'Seatback angle...'\n",
      "Warning: Could not compute embedding for section 'Seat height...'\n",
      "Warning: Could not compute embedding for section 'Forward and rearward adjustment...'\n",
      "Warning: Could not compute embedding for section 'Seatback angle adjustment...'\n",
      "Warning: Could not compute embedding for section 'Lumbar support...'\n",
      "Warning: Could not compute embedding for section 'Reclining the rear seats...'\n",
      "Warning: Could not compute embedding for section 'Adjusting the height up and down...'\n",
      "Warning: Could not compute embedding for section 'Adjusting the height up and down...'\n",
      "Warning: Could not compute embedding for section 'Top tether anchorages are located on the rear of t...'\n",
      "Computed embeddings for 228 sections\n",
      "Embedded sections: 228\n",
      "Top 5 results:\n",
      "1. Air Ventilation Seats (Score: 0.740)\n",
      "2. Air Conditioning System (Score: 0.676)\n",
      "3. Air Conditioner Compressor Label (Score: 0.674)\n",
      "4. Front air ventilation seats (Score: 0.661)\n",
      "5. NOTICE (Score: 0.656)\n",
      "\n",
      "Top results from fresh pipeline:\n",
      "1. Air Ventilation Seats (Score: 0.740)\n",
      "2. Air Conditioning System (Score: 0.676)\n",
      "3. Air Conditioner Compressor Label (Score: 0.674)\n",
      "4. Front air ventilation seats (Score: 0.661)\n",
      "5. NOTICE (Score: 0.656)\n"
     ]
    }
   ],
   "source": [
    "# Let's check if there's a discrepancy in the sections being used\n",
    "print(\"Checking section counts:\")\n",
    "print(f\"Total sections: {len(sections)}\")\n",
    "print(f\"Embedded sections: {len(embedded_sections)}\")\n",
    "\n",
    "# Check if all sections are being embedded\n",
    "missing_sections = []\n",
    "for section in sections:\n",
    "    if section not in embedded_sections:\n",
    "        # Check by header and start_idx\n",
    "        found = False\n",
    "        for emb_section in embedded_sections:\n",
    "            if (emb_section['header'] == section['header'] and\n",
    "                emb_section.get('start_idx') == section.get('start_idx')):\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            missing_sections.append(section['header'])\n",
    "\n",
    "print(f\"Sections that failed embedding: {len(missing_sections)}\")\n",
    "if missing_sections:\n",
    "    print(\"Missing sections:\", missing_sections[:5])  # Show first 5\n",
    "\n",
    "# Let's also run the exact same search that run_simple_rag_improved does\n",
    "print(\"\\nRunning the exact same pipeline as run_simple_rag_improved:\")\n",
    "# Re-extract sections\n",
    "test_sections = extract_all_sections_with_content(concatenated)\n",
    "print(f\"Extracted sections: {len(test_sections)}\")\n",
    "\n",
    "# Re-compute embeddings\n",
    "test_embedded_sections = compute_section_embeddings(test_sections, nlp)\n",
    "print(f\"Embedded sections: {len(test_embedded_sections)}\")\n",
    "\n",
    "# Run search\n",
    "test_results = simple_rag_search_improved(test_embedded_sections, \"cabin air filter replacement\", nlp, top_n=5)\n",
    "print(\"\\nTop results from fresh pipeline:\")\n",
    "for i, result in enumerate(test_results, 1):\n",
    "    print(f\"{i}. {result['header']} (Score: {result['similarity_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eedcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check which sections are being filtered out\n",
    "print(\"Checking which sections are filtered out by extract_all_sections_with_content:\")\n",
    "\n",
    "# Get the extracted sections\n",
    "extracted_sections = extract_all_sections_with_content(concatenated)\n",
    "extracted_headers = {s['header'] for s in extracted_sections}\n",
    "\n",
    "# Find sections that are in original but not in extracted\n",
    "filtered_out = []\n",
    "for section in sections:\n",
    "    if section['header'] not in extracted_headers:\n",
    "        filtered_out.append(section['header'])\n",
    "\n",
    "print(f\"Total sections: {len(sections)}\")\n",
    "print(f\"Extracted sections: {len(extracted_sections)}\")\n",
    "print(f\"Filtered out: {len(filtered_out)}\")\n",
    "print(\"\\nFirst 10 filtered out sections:\")\n",
    "for header in filtered_out[:10]:\n",
    "    print(f\"  {header}\")\n",
    "\n",
    "# Check if Filter replacement is filtered out\n",
    "filter_replacement_filtered = 'Filter replacement' in filtered_out\n",
    "print(f\"\\n'Filter replacement' filtered out: {filter_replacement_filtered}\")\n",
    "\n",
    "if not filter_replacement_filtered:\n",
    "    print(\"'Filter replacement' sections are being extracted correctly!\")\n",
    "else:\n",
    "    print(\"'Filter replacement' sections are being filtered out - need to check the filtering criteria\")\n",
    "\n",
    "# Let's also check the content length of filtered sections\n",
    "print(\"\\nChecking content lengths of some sections:\")\n",
    "for section in sections[:5]:  # Check first 5\n",
    "    print(f\"'{section['header']}': {len(section.get('content', ''))} chars\")\n",
    "\n",
    "for section in sections[-5:]:  # Check last 5\n",
    "    print(f\"'{section['header']}': {len(section.get('content', ''))} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e8774b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing FINAL simplified RAG:\n",
      "Running Final Simple RAG for query: 'cabin air filter replacement'\n",
      "1. Extracting sections...\n",
      "Extracted 242 sections with content\n",
      "2. Computing embeddings...\n",
      "Warning: Could not compute embedding for section 'Exterior Overview (Front View)...'\n",
      "Warning: Could not compute embedding for section 'Exterior Overview (Rear View)...'\n",
      "Warning: Could not compute embedding for section 'Interior Overview...'\n",
      "Warning: Could not compute embedding for section 'Center Console Overview...'\n",
      "Warning: Could not compute embedding for section 'Seats...'\n",
      "Warning: Could not compute embedding for section 'Seatback angle...'\n",
      "Warning: Could not compute embedding for section 'Seat height...'\n",
      "Warning: Could not compute embedding for section 'Forward and rearward adjustment...'\n",
      "Warning: Could not compute embedding for section 'Seatback angle adjustment...'\n",
      "Warning: Could not compute embedding for section 'Lumbar support...'\n",
      "Warning: Could not compute embedding for section 'Reclining the rear seats...'\n",
      "Warning: Could not compute embedding for section 'Adjusting the height up and down...'\n",
      "Warning: Could not compute embedding for section 'Adjusting the height up and down...'\n",
      "Warning: Could not compute embedding for section 'Top tether anchorages are located on the rear of t...'\n",
      "Computed embeddings for 228 sections\n",
      "3. Performing search...\n",
      "Top 3 results:\n",
      "1. Air Ventilation Seats (Score: 0.740)\n",
      "2. Air Conditioning System (Score: 0.676)\n",
      "3. Air Conditioner Compressor Label (Score: 0.674)\n",
      "4. Finding continuous section range...\n",
      "   Found range: elements 240 to 646\n",
      "5. Exporting with Docling API...\n",
      "✅ Final RAG completed! Results saved to final_rag_results.html\n"
     ]
    }
   ],
   "source": [
    "# SIMPLIFIED APPROACH: Find continuous section range and use Docling API directly\n",
    "def find_continuous_section_range(results: List[Dict[str, Any]], min_relevance_threshold: float = 0.6) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Find the longest continuous section range from relevant results.\n",
    "    Only include results above the relevance threshold.\n",
    "    \"\"\"\n",
    "    # Filter results by relevance\n",
    "    relevant_results = [r for r in results if r['similarity_score'] >= min_relevance_threshold]\n",
    "\n",
    "    if not relevant_results:\n",
    "        # If no results above threshold, take the top result\n",
    "        relevant_results = results[:1]\n",
    "\n",
    "    # Find min start_idx and max end_idx\n",
    "    start_indices = [r['start_idx'] for r in relevant_results if r.get('start_idx') is not None]\n",
    "    end_indices = [r['end_idx'] for r in relevant_results if r.get('end_idx') is not None]\n",
    "\n",
    "    if not start_indices or not end_indices:\n",
    "        return None, None\n",
    "\n",
    "    min_start = min(start_indices)\n",
    "    max_end = max(end_indices)\n",
    "\n",
    "    return min_start, max_end\n",
    "\n",
    "def save_continuous_section_as_html(doc: DoclingDocument, start_idx: int, end_idx: int, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Save a continuous section range as HTML using Docling's API directly.\n",
    "    Returns the HTML content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use Docling's save_as_html with the continuous range\n",
    "        doc.save_as_html(filename,\n",
    "                        from_element=start_idx,\n",
    "                        to_element=min(end_idx + 1, len(doc.texts)),\n",
    "                        image_mode=ImageRefMode.EMBEDDED)\n",
    "\n",
    "        # Read the complete HTML file\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "\n",
    "        return html_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting section {start_idx} to {end_idx}: {e}\")\n",
    "        return f\"<div>Error: Could not export section {start_idx} to {end_idx}</div>\"\n",
    "\n",
    "def run_simple_rag_final(doc: DoclingDocument, query: str, output_file: str = \"rag_results.html\", top_n: int = 5, min_relevance: float = 0.6):\n",
    "    \"\"\"\n",
    "    FINAL SIMPLE RAG: Find continuous section and export with Docling API.\n",
    "    \"\"\"\n",
    "    print(f\"Running Final Simple RAG for query: '{query}'\")\n",
    "\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "    # Extract sections\n",
    "    print(\"1. Extracting sections...\")\n",
    "    sections = extract_all_sections_with_content(doc)\n",
    "\n",
    "    # Compute embeddings\n",
    "    print(\"2. Computing embeddings...\")\n",
    "    embedded_sections = compute_section_embeddings(sections, nlp)\n",
    "\n",
    "    # Perform search\n",
    "    print(\"3. Performing search...\")\n",
    "    results = simple_rag_search_improved(embedded_sections, query, nlp, top_n=top_n)\n",
    "\n",
    "    # Find continuous range from relevant results\n",
    "    print(\"4. Finding continuous section range...\")\n",
    "    start_idx, end_idx = find_continuous_section_range(results, min_relevance)\n",
    "\n",
    "    if start_idx is None or end_idx is None:\n",
    "        print(\"❌ No valid section range found\")\n",
    "        return results\n",
    "\n",
    "    print(f\"   Found range: elements {start_idx} to {end_idx}\")\n",
    "\n",
    "    # Export the continuous section using Docling API\n",
    "    print(\"5. Exporting with Docling API...\")\n",
    "    temp_filename = f\"temp_continuous_{start_idx}_{end_idx}.html\"\n",
    "    section_html = save_continuous_section_as_html(doc, start_idx, end_idx, temp_filename)\n",
    "\n",
    "    # Create final HTML with metadata\n",
    "    html_parts = [\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Search Results</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; margin: 40px; }\n",
    "        .query { background: #f0f0f0; padding: 15px; border-radius: 5px; margin-bottom: 20px; }\n",
    "        .results { margin-bottom: 20px; }\n",
    "        .result { margin-bottom: 15px; padding: 10px; border: 1px solid #ddd; }\n",
    "        .separator { border-top: 2px solid #eee; margin: 20px 0; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\"\"\"]\n",
    "\n",
    "    html_parts.append(f\"\"\"\n",
    "    <div class=\"query\">\n",
    "        <h2>Query: {escape(query)}</h2>\n",
    "        <p>Found {len(results)} relevant sections, exported continuous range: elements {start_idx} to {end_idx}</p>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"results\">\n",
    "        <h3>Top Results:</h3>\"\"\")\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        header = result['header']\n",
    "        score = result.get('similarity_score', 0.0)\n",
    "        pages = result.get('pages', [])\n",
    "        r_start = result.get('start_idx')\n",
    "        r_end = result.get('end_idx')\n",
    "\n",
    "        html_parts.append(f\"\"\"\n",
    "        <div class=\"result\">\n",
    "            <strong>#{i}: {escape(header)}</strong><br>\n",
    "            Score: {score:.3f} | Pages: {', '.join(map(str, pages))} | Elements: {r_start} to {r_end}\n",
    "        </div>\"\"\")\n",
    "\n",
    "    html_parts.append(\"\"\"\n",
    "    </div>\n",
    "\n",
    "    <div class=\"separator\"></div>\n",
    "\n",
    "    <h3>Content:</h3>\n",
    "\"\"\")\n",
    "\n",
    "    # Insert the Docling-generated HTML content\n",
    "    html_parts.append(section_html)\n",
    "\n",
    "    html_parts.append(\"\"\"\n",
    "</body>\n",
    "</html>\"\"\")\n",
    "\n",
    "    # Write the final HTML file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(''.join(html_parts))\n",
    "\n",
    "    print(f\"✅ Final RAG completed! Results saved to {output_file}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test the final simplified approach\n",
    "print(\"Testing FINAL simplified RAG:\")\n",
    "final_results = run_simple_rag_final(concatenated, \"cabin air filter replacement\", \"final_rag_results.html\", top_n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
