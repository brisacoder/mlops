{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f7eb749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 16:21:55,932 - INFO - Downloading layout model...\n",
      "2025-09-19 16:21:56,325 - INFO - Downloading tableformer model...\n",
      "2025-09-19 16:21:56,460 - INFO - Downloading picture classifier model...\n",
      "2025-09-19 16:21:56,582 - INFO - Downloading code formula model...\n",
      "2025-09-19 16:21:56,725 - INFO - Downloading easyocr models...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/dev/.cache/docling/models')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docling.utils.model_downloader import download_models\n",
    "download_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6f687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def split_pdf_into_chunks(input_pdf_path: Path, num_chunks: int) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Splits a PDF file into a specified number of chunks and returns the paths to the chunk files.\n",
    "\n",
    "    Args:\n",
    "        input_pdf_path: Path to the input PDF file.\n",
    "        num_chunks: The number of chunks to split the PDF into.\n",
    "\n",
    "    Returns:\n",
    "        A list of Path objects, where each Path points to a temporary PDF chunk file.\n",
    "    \"\"\"\n",
    "    temp_chunk_paths = []\n",
    "\n",
    "    try:\n",
    "        input_doc = fitz.open(input_pdf_path)\n",
    "        total_pages = input_doc.page_count\n",
    "        pages_per_chunk = total_pages // num_chunks\n",
    "        remaining_pages = total_pages % num_chunks\n",
    "\n",
    "        start_page = 0\n",
    "        for i in range(num_chunks):\n",
    "            end_page = start_page + pages_per_chunk\n",
    "            if i < remaining_pages:\n",
    "                end_page += 1\n",
    "\n",
    "            chunk_doc = fitz.open()  # Create a new empty PDF for the chunk\n",
    "\n",
    "            # Copy pages from the input PDF to the chunk document\n",
    "            chunk_doc.insert_pdf(input_doc, from_page=start_page, to_page=end_page - 1)\n",
    "\n",
    "            # Define a temporary file path for the chunk\n",
    "            # Using a temporary directory might be better for cleanup in a real scenario\n",
    "            temp_chunk_path = input_pdf_path.parent / f\"temp_chunk_{start_page}-{end_page-1}_{input_pdf_path.name}\"\n",
    "            temp_chunk_paths.append(temp_chunk_path)\n",
    "\n",
    "            # Save the temporary chunk document\n",
    "            chunk_doc.save(temp_chunk_path)\n",
    "\n",
    "            # Close the temporary chunk document\n",
    "            chunk_doc.close()\n",
    "\n",
    "            start_page = end_page\n",
    "\n",
    "        # Close the input PDF document\n",
    "        input_doc.close()\n",
    "\n",
    "        print(f\"Split {total_pages} pages into {len(temp_chunk_paths)} chunks.\")\n",
    "        for path in temp_chunk_paths:\n",
    "            print(f\"Created: {path}\")\n",
    "\n",
    "        return temp_chunk_paths\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input PDF not found at {input_pdf_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF splitting: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff316904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 552 pages into 5 chunks.\n",
      "Created: data\\temp_chunk_0-110_kona_manual.pdf\n",
      "Created: data\\temp_chunk_111-221_kona_manual.pdf\n",
      "Created: data\\temp_chunk_222-331_kona_manual.pdf\n",
      "Created: data\\temp_chunk_332-441_kona_manual.pdf\n",
      "Created: data\\temp_chunk_442-551_kona_manual.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "0467adbd-c0cd-4522-8a3f-0517b19cc7ff",
       "rows": [
        [
         "0",
         "data\\temp_chunk_0-110_kona_manual.pdf"
        ],
        [
         "1",
         "data\\temp_chunk_111-221_kona_manual.pdf"
        ],
        [
         "2",
         "data\\temp_chunk_222-331_kona_manual.pdf"
        ],
        [
         "3",
         "data\\temp_chunk_332-441_kona_manual.pdf"
        ],
        [
         "4",
         "data\\temp_chunk_442-551_kona_manual.pdf"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "[WindowsPath('data/temp_chunk_0-110_kona_manual.pdf'),\n",
       " WindowsPath('data/temp_chunk_111-221_kona_manual.pdf'),\n",
       " WindowsPath('data/temp_chunk_222-331_kona_manual.pdf'),\n",
       " WindowsPath('data/temp_chunk_332-441_kona_manual.pdf'),\n",
       " WindowsPath('data/temp_chunk_442-551_kona_manual.pdf')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_pdf_into_chunks((Path('./data/kona_manual.pdf')), num_chunks=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e282b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Get the logger for docling and set its level\n",
    "logging.getLogger('docling').setLevel(logging.INFO)\n",
    "logging.getLogger('docling_core').setLevel(logging.INFO)\n",
    "log = logging.getLogger(__name__)  # This makes your script a logging-aware application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10a75d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    LayoutOptions\n",
    ")\n",
    "from docling.datamodel.settings import settings\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.layout_model_specs import (\n",
    "    DOCLING_LAYOUT_EGRET_LARGE,\n",
    "    DOCLING_LAYOUT_EGRET_MEDIUM,\n",
    "    DOCLING_LAYOUT_EGRET_XLARGE,\n",
    "    DOCLING_LAYOUT_HERON,\n",
    "    DOCLING_LAYOUT_HERON_101,\n",
    "    DOCLING_LAYOUT_V2,\n",
    "    LayoutModelConfig,\n",
    ")\n",
    "\n",
    "data_folder = Path(\"data\")\n",
    "input_doc_path = data_folder / \"kona_manual.pdf\"\n",
    "\n",
    "# Explicitly set the accelerator\n",
    "# accelerator_options = AcceleratorOptions(\n",
    "#     num_threads=8, device=AcceleratorDevice.AUTO\n",
    "# )\n",
    "#accelerator_options = AcceleratorOptions(\n",
    "#    num_threads=8, device=AcceleratorDevice.CPU\n",
    "#)\n",
    "# accelerator_options = AcceleratorOptions(\n",
    "#     num_threads=8, device=AcceleratorDevice.MPS\n",
    "# )\n",
    "accelerator_options = AcceleratorOptions(\n",
    "        num_threads=8, device=AcceleratorDevice.CUDA\n",
    ")\n",
    "\n",
    "# easyocr doesnt support cuda:N allocation, defaults to cuda:0\n",
    "# accelerator_options = AcceleratorOptions(num_threads=8, device=\"cuda:1\")\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.accelerator_options = accelerator_options\n",
    "pipeline_options.do_ocr = False\n",
    "pipeline_options.do_table_structure = True\n",
    "pipeline_options.table_structure_options.do_cell_matching = True\n",
    "pipeline_options.generate_page_images = False\n",
    "pipeline_options.generate_picture_images = True\n",
    "pipeline_options.generate_parsed_pages = True\n",
    "pipeline_options.layout_options = LayoutOptions(create_orphan_clusters=True, keep_empty_clusters=True, model_spec=DOCLING_LAYOUT_HERON)\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Enable the profiling to measure the time spent\n",
    "settings.debug.profile_pipeline_timings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7551947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.accelerator_options import AcceleratorOptions, AcceleratorDevice\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "# def process_pdf_chunk(chunk_path: str, converter):\n",
    "#     \"\"\"\n",
    "#     Processes a single PDF chunk or a full PDF using the provided docling converter.\n",
    "\n",
    "#     Args:\n",
    "#         chunk_path: Path to the PDF file (chunk or full).\n",
    "#         converter: An instance of docling.document_converter.DocumentConverter.\n",
    "\n",
    "#     Returns:\n",
    "#         The result of the converter.convert() method call.\n",
    "#     \"\"\"\n",
    "#     # Assuming converter is already initialized with desired options outside this function\n",
    "#     conversion_result = converter.convert(chunk_path)\n",
    "#     return conversion_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ccd8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 16:41:36,533 - INFO - Downloading layout model...\n",
      "2025-09-19 16:41:36,642 - INFO - Downloading tableformer model...\n",
      "2025-09-19 16:41:36,642 - INFO - Downloading tableformer model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading models in main process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 16:41:36,749 - INFO - Downloading picture classifier model...\n",
      "2025-09-19 16:41:36,861 - INFO - Downloading code formula model...\n",
      "2025-09-19 16:41:36,861 - INFO - Downloading code formula model...\n",
      "2025-09-19 16:41:36,964 - INFO - Downloading easyocr models...\n",
      "2025-09-19 16:41:36,964 - INFO - Downloading easyocr models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models preloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 16:41:42,219 - INFO - \n",
      "Processing 2 chunks with ProcessPoolExecutor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 552 pages into 2 chunks.\n",
      "Created: temp_chunk_0-275_kona.pdf\n",
      "Created: temp_chunk_276-551_kona.pdf\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A child process terminated abruptly, the process pool is not usable anymore",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenProcessPool\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i > \u001b[32m0\u001b[39m:\n\u001b[32m    104\u001b[39m         time.sleep(\u001b[32m5\u001b[39m)  \u001b[38;5;66;03m# 5 second delay between submissions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     future = \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_pdf_chunk_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     future_to_chunk[future] = chunk_path\n\u001b[32m    108\u001b[39m parallel_results = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\concurrent\\futures\\process.py:786\u001b[39m, in \u001b[36mProcessPoolExecutor.submit\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown_lock:\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._broken:\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m BrokenProcessPool(\u001b[38;5;28mself\u001b[39m._broken)\n\u001b[32m    787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown_thread:\n\u001b[32m    788\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mcannot schedule new futures after shutdown\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mBrokenProcessPool\u001b[39m: A child process terminated abruptly, the process pool is not usable anymore"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.accelerator_options import AcceleratorOptions, AcceleratorDevice\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    LayoutOptions\n",
    ")\n",
    "from docling.datamodel.settings import settings\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.layout_model_specs import (\n",
    "    DOCLING_LAYOUT_EGRET_LARGE,\n",
    "    DOCLING_LAYOUT_EGRET_MEDIUM,\n",
    "    DOCLING_LAYOUT_EGRET_XLARGE,\n",
    "    DOCLING_LAYOUT_HERON,\n",
    "    DOCLING_LAYOUT_HERON_101,\n",
    "    DOCLING_LAYOUT_V2,\n",
    "    LayoutModelConfig,\n",
    ")\n",
    "from docling.datamodel.settings import settings\n",
    "\n",
    "\n",
    "# Preload models in main process\n",
    "print(\"Preloading models in main process...\")\n",
    "from docling.utils.model_downloader import download_models\n",
    "download_models()\n",
    "print(\"Models preloaded.\")\n",
    "\n",
    "\n",
    "def process_pdf_chunk_process(chunk_path: str):\n",
    "    \"\"\"\n",
    "    Processes a single PDF chunk using a new docling converter instance.\n",
    "    Designed for use with ProcessPoolExecutor.\n",
    "    \"\"\"\n",
    "    print(f\"Starting subprocess for {chunk_path}\")\n",
    "    \n",
    "    # Initialize converter with minimal options to avoid memory issues\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.do_ocr = False\n",
    "    pipeline_options.do_table_structure = False  # Keep disabled for stability\n",
    "    pipeline_options.generate_page_images = False\n",
    "    pipeline_options.generate_picture_images = False  # Keep disabled for stability\n",
    "    pipeline_options.generate_parsed_pages = False\n",
    "    pipeline_options.layout_options = LayoutOptions(create_orphan_clusters=True, keep_empty_clusters=True, model_spec=DOCLING_LAYOUT_HERON)\n",
    "    pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "        num_threads=1, device=AcceleratorDevice.CPU\n",
    "    )\n",
    "\n",
    "    print(f\"Initializing converter for {chunk_path}\")\n",
    "    try:\n",
    "        converter = DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(\n",
    "                    pipeline_options=pipeline_options,\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        print(f\"Converter initialized for {chunk_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Converter initialization failed for {chunk_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        conversion_result = converter.convert(chunk_path)\n",
    "        print(f\"Successfully processed chunk: {chunk_path}\")\n",
    "        return conversion_result\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error processing chunk {chunk_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "data_folder = Path(\".\")\n",
    "\n",
    "# Define the number of chunks for sequential processing\n",
    "num_chunks = 2  # Adjust as needed for optimal sequential processing\n",
    "\n",
    "# Define the input PDF file path (assuming data_folder is defined)\n",
    "input_pdf_path = data_folder / \"kona.pdf\"\n",
    "\n",
    "# Split the PDF into chunks\n",
    "temp_chunk_paths = split_pdf_into_chunks(input_pdf_path, num_chunks)\n",
    "\n",
    "# Try ProcessPoolExecutor with careful resource management\n",
    "if temp_chunk_paths:\n",
    "    log.info(f\"\\nProcessing {len(temp_chunk_paths)} chunks with ProcessPoolExecutor...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use ProcessPoolExecutor with limited workers\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=min(num_chunks, 2)) as executor:  # Limit to 2 workers max\n",
    "        # Submit processing tasks with staggered start to avoid resource contention\n",
    "        future_to_chunk = {}\n",
    "        for i, chunk_path in enumerate(temp_chunk_paths):\n",
    "            if i > 0:\n",
    "                time.sleep(5)  # 5 second delay between submissions\n",
    "            future = executor.submit(process_pdf_chunk_process, str(chunk_path))\n",
    "            future_to_chunk[future] = chunk_path\n",
    "\n",
    "        parallel_results = {}\n",
    "        for future in concurrent.futures.as_completed(future_to_chunk):\n",
    "            chunk_path = future_to_chunk[future]\n",
    "            try:\n",
    "                result = future.result(timeout=600)  # 10 minute timeout per chunk\n",
    "                if result is not None:\n",
    "                    parallel_results[chunk_path] = result\n",
    "                    log.info(f\"Processed {chunk_path}\")\n",
    "                else:\n",
    "                    log.warning(f\"Failed to process {chunk_path}\")\n",
    "            except concurrent.futures.TimeoutError:\n",
    "                log.error(f'{chunk_path} timed out after 10 minutes')\n",
    "            except Exception as exc:\n",
    "                log.error(f'{chunk_path} generated an exception: {exc}')\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "    end_time = time.time()\n",
    "    parallel_processing_time = end_time - start_time\n",
    "\n",
    "    log.info(f\"\\nParallel processing of {len(temp_chunk_paths)} chunks took {parallel_processing_time:.4f} seconds.\")\n",
    "    log.info(f\"Results: {len(parallel_results)} successful out of {len(temp_chunk_paths)}\")\n",
    "\n",
    "else:\n",
    "    log.warning(\"No chunks were created for processing.\")\n",
    "\n",
    "# Remember to clean up temporary files after you are done\n",
    "# for chunk_path in temp_chunk_paths:\n",
    "#     if chunk_path.exists():\n",
    "#         os.remove(chunk_path)\n",
    "#         print(f\"Cleaned up: {chunk_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ffa778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 16:24:48,835 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-19 16:24:48,844 - INFO - Going to convert document batch...\n",
      "2025-09-19 16:24:48,844 - INFO - Going to convert document batch...\n",
      "2025-09-19 16:24:48,845 - INFO - Initializing pipeline for StandardPdfPipeline with options hash f8e39f5f832f6e3e4aa7af81ec9458de\n",
      "2025-09-19 16:24:48,846 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-09-19 16:24:48,845 - INFO - Initializing pipeline for StandardPdfPipeline with options hash f8e39f5f832f6e3e4aa7af81ec9458de\n",
      "2025-09-19 16:24:48,846 - INFO - Accelerator device: 'cuda:0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single chunk: temp_chunk_0-91_kona.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 16:24:49,995 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-09-19 16:24:50,322 - INFO - Processing document temp_chunk_0-91_kona.pdf\n",
      "2025-09-19 16:24:50,322 - INFO - Processing document temp_chunk_0-91_kona.pdf\n",
      "Exception ignored on calling ctypes callback function <pypdfium2.internal.utils._buffer_reader object at 0x00000274CF7F6AD0>:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\pypdfium2\\internal\\utils.py\", line 47, in __call__\n",
      "    def __call__(self, _, position, p_buf, size):\n",
      "KeyboardInterrupt: \n",
      "Exception ignored on calling ctypes callback function <pypdfium2.internal.utils._buffer_reader object at 0x00000274CF7F6AD0>:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\pypdfium2\\internal\\utils.py\", line 47, in __call__\n",
      "    def __call__(self, _, position, p_buf, size):\n",
      "KeyboardInterrupt: \n",
      "2025-09-19 16:24:50,840 - WARNING - Encountered an error during conversion of document 45cfdadcb6f54414155ba899e92f2210c2bdc9d8fc50c1ed89e721fef56fae85:\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\pipeline\\base_pipeline.py\", line 230, in _build_document\n",
      "    for p in pipeline_pages:  # Must exhaust!\n",
      "             ^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\pipeline\\base_pipeline.py\", line 195, in _apply_on_pages\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\models\\page_assemble_model.py\", line 70, in __call__\n",
      "    for page in page_batch:\n",
      "                ^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\models\\table_structure_model.py\", line 177, in __call__\n",
      "    for page in page_batch:\n",
      "                ^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\models\\layout_model.py\", line 152, in __call__\n",
      "    pages = list(page_batch)\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\models\\easyocr_model.py\", line 133, in __call__\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\models\\page_preprocessing_model.py\", line 40, in __call__\n",
      "    for page in page_batch:\n",
      "                ^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\pipeline\\standard_pdf_pipeline.py\", line 128, in initialize_page\n",
      "    page._backend = conv_res.input._backend.load_page(page.page_no)  # type: ignore\n",
      "                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\backend\\docling_parse_v4_backend.py\", line 221, in load_page\n",
      "    ppage = self._pdoc[page_no]\n",
      "            ~~~~~~~~~~^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\pypdfium2\\_helpers\\document.py\", line 121, in __getitem__\n",
      "    return self.get_page(i)\n",
      "           ~~~~~~~~~~~~~^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\pypdfium2\\_helpers\\document.py\", line 369, in get_page\n",
      "    raise PdfiumError(\"Failed to load page.\")\n",
      "\n",
      "pypdfium2._helpers.misc.PdfiumError: Failed to load page.\n",
      "\n",
      "2025-09-19 16:24:50,840 - WARNING - Encountered an error during conversion of document 45cfdadcb6f54414155ba899e92f2210c2bdc9d8fc50c1ed89e721fef56fae85:\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\pipeline\\base_pipeline.py\", line 230, in _build_document\n",
      "    for p in pipeline_pages:  # Must exhaust!\n",
      "             ^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\pipeline\\base_pipeline.py\", line 195, in _apply_on_pages\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\models\\page_assemble_model.py\", line 70, in __call__\n",
      "    for page in page_batch:\n",
      "                ^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\models\\table_structure_model.py\", line 177, in __call__\n",
      "    for page in page_batch:\n",
      "                ^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\models\\layout_model.py\", line 152, in __call__\n",
      "    pages = list(page_batch)\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\models\\easyocr_model.py\", line 133, in __call__\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\models\\page_preprocessing_model.py\", line 40, in __call__\n",
      "    for page in page_batch:\n",
      "                ^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\pipeline\\standard_pdf_pipeline.py\", line 128, in initialize_page\n",
      "    page._backend = conv_res.input._backend.load_page(page.page_no)  # type: ignore\n",
      "                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\docling\\backend\\docling_parse_v4_backend.py\", line 221, in load_page\n",
      "    ppage = self._pdoc[page_no]\n",
      "            ~~~~~~~~~~^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\pypdfium2\\_helpers\\document.py\", line 121, in __getitem__\n",
      "    return self.get_page(i)\n",
      "           ~~~~~~~~~~~~~^^^\n",
      "\n",
      "  File \"c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\pypdfium2\\_helpers\\document.py\", line 369, in get_page\n",
      "    raise PdfiumError(\"Failed to load page.\")\n",
      "\n",
      "pypdfium2._helpers.misc.PdfiumError: Failed to load page.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing chunk temp_chunk_0-91_kona.pdf: Failed to load page.\n",
      "Single chunk failed.\n"
     ]
    }
   ],
   "source": [
    "# Test processing a single chunk sequentially\n",
    "if temp_chunk_paths:\n",
    "    test_chunk = temp_chunk_paths[0]\n",
    "    print(f\"Testing single chunk: {test_chunk}\")\n",
    "    # Check if file exists and is readable\n",
    "    if test_chunk.exists():\n",
    "        print(f\"Chunk file exists: {test_chunk}\")\n",
    "        try:\n",
    "            with open(test_chunk, 'rb') as f:\n",
    "                f.read(100)  # Try to read first 100 bytes\n",
    "            print(\"Chunk file is readable.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Chunk file read error: {e}\")\n",
    "    else:\n",
    "        print(f\"Chunk file does not exist: {test_chunk}\")\n",
    "    \n",
    "    try:\n",
    "        result = process_pdf_chunk_process(str(test_chunk))\n",
    "        if result:\n",
    "            print(\"Single chunk processed successfully.\")\n",
    "        else:\n",
    "            print(\"Single chunk failed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Single chunk error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02bd4150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 14:37:12,844 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-19 14:37:12,874 - INFO - Going to convert document batch...\n",
      "2025-09-19 14:37:12,875 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 369dbe88aeb3474e269aaedef841528c\n",
      "2025-09-19 14:37:12,876 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-09-19 14:37:12,874 - INFO - Going to convert document batch...\n",
      "2025-09-19 14:37:12,875 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 369dbe88aeb3474e269aaedef841528c\n",
      "2025-09-19 14:37:12,876 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-09-19 14:37:14,609 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-09-19 14:37:14,609 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-09-19 14:37:15,285 - INFO - Processing document kona_manual.pdf\n",
      "2025-09-19 14:37:15,285 - INFO - Processing document kona_manual.pdf\n",
      "2025-09-19 14:38:36,684 - INFO - Finished converting document kona_manual.pdf in 83.85 sec.\n",
      "2025-09-19 14:38:36,684 - INFO - Finished converting document kona_manual.pdf in 83.85 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion secs: [81.37313710000035]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    LayoutOptions\n",
    ")\n",
    "from docling.datamodel.settings import settings\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.layout_model_specs import (\n",
    "    DOCLING_LAYOUT_EGRET_LARGE,\n",
    "    DOCLING_LAYOUT_EGRET_MEDIUM,\n",
    "    DOCLING_LAYOUT_EGRET_XLARGE,\n",
    "    DOCLING_LAYOUT_HERON,\n",
    "    DOCLING_LAYOUT_HERON_101,\n",
    "    DOCLING_LAYOUT_V2,\n",
    "    LayoutModelConfig,\n",
    ")\n",
    "\n",
    "data_folder = Path(\"data\")\n",
    "input_doc_path = data_folder / \"kona_manual.pdf\"\n",
    "\n",
    "# Explicitly set the accelerator\n",
    "# accelerator_options = AcceleratorOptions(\n",
    "#     num_threads=8, device=AcceleratorDevice.AUTO\n",
    "# )\n",
    "#accelerator_options = AcceleratorOptions(\n",
    "#    num_threads=8, device=AcceleratorDevice.CPU\n",
    "#)\n",
    "# accelerator_options = AcceleratorOptions(\n",
    "#     num_threads=8, device=AcceleratorDevice.MPS\n",
    "# )\n",
    "accelerator_options = AcceleratorOptions(\n",
    "        num_threads=8, device=AcceleratorDevice.CUDA\n",
    ")\n",
    "\n",
    "# easyocr doesnt support cuda:N allocation, defaults to cuda:0\n",
    "# accelerator_options = AcceleratorOptions(num_threads=8, device=\"cuda:1\")\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.accelerator_options = accelerator_options\n",
    "pipeline_options.do_ocr = False\n",
    "pipeline_options.do_table_structure = True\n",
    "pipeline_options.table_structure_options.do_cell_matching = True\n",
    "pipeline_options.generate_page_images = False\n",
    "pipeline_options.generate_picture_images = True\n",
    "pipeline_options.generate_parsed_pages = True\n",
    "pipeline_options.layout_options = LayoutOptions(create_orphan_clusters=True, keep_empty_clusters=True, model_spec=DOCLING_LAYOUT_HERON)\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Enable the profiling to measure the time spent\n",
    "settings.debug.profile_pipeline_timings = True\n",
    "\n",
    "# Convert the document\n",
    "conversion_result = converter.convert(input_doc_path)\n",
    "doc = conversion_result.document\n",
    "\n",
    "# List with total time per document\n",
    "doc_conversion_secs = conversion_result.timings[\"pipeline_total\"].times\n",
    "\n",
    "md = doc.export_to_markdown()\n",
    "# print(md)\n",
    "print(f\"Conversion secs: {doc_conversion_secs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e38cd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HTML to artifacts/doc.html\n",
      "h2: Filter replacement (page 492) - Parent H1: None\n",
      "h2: Filter replacement (page 492) - Parent H1: None\n",
      "h2: Blade replacement (page 494) - Parent H1: None\n",
      "h2: Front windshield wiper blade replacement (page 495) - Parent H1: None\n",
      "h2: Rear window wiper blade replacement (page 496) - Parent H1: None\n",
      "h2: Tire replacement (page 503) - Parent H1: None\n",
      "h2: Compact spare tire replacement (page 504) - Parent H1: None\n",
      "h2: Wheel replacement (page 504) - Parent H1: None\n",
      "h2: Instrument panel fuse replacement (page 514) - Parent H1: None\n",
      "h2: Engine compartment panel fuse replacement (page 514) - Parent H1: None\n",
      "h2: Front light replacement (page 526) - Parent H1: None\n",
      "h2: Side repeater light replacement (page 527) - Parent H1: None\n",
      "h2: Rear combination light replacement (page 527) - Parent H1: None\n",
      "h2: High mounted stop light replacement (page 528) - Parent H1: None\n",
      "h2: License plate light replacement (page 528) - Parent H1: None\n",
      "h2: Interior light replacement (page 529) - Parent H1: None\n",
      "h2: Filter replacement (page 492) - Parent H1: None\n",
      "h2: Filter replacement (page 492) - Parent H1: None\n",
      "h2: Blade replacement (page 494) - Parent H1: None\n",
      "h2: Front windshield wiper blade replacement (page 495) - Parent H1: None\n",
      "h2: Rear window wiper blade replacement (page 496) - Parent H1: None\n",
      "h2: Tire replacement (page 503) - Parent H1: None\n",
      "h2: Compact spare tire replacement (page 504) - Parent H1: None\n",
      "h2: Wheel replacement (page 504) - Parent H1: None\n",
      "h2: Instrument panel fuse replacement (page 514) - Parent H1: None\n",
      "h2: Engine compartment panel fuse replacement (page 514) - Parent H1: None\n",
      "h2: Front light replacement (page 526) - Parent H1: None\n",
      "h2: Side repeater light replacement (page 527) - Parent H1: None\n",
      "h2: Rear combination light replacement (page 527) - Parent H1: None\n",
      "h2: High mounted stop light replacement (page 528) - Parent H1: None\n",
      "h2: License plate light replacement (page 528) - Parent H1: None\n",
      "h2: Interior light replacement (page 529) - Parent H1: None\n"
     ]
    }
   ],
   "source": [
    "def find_headers_in_html(doc, html_string, word):\n",
    "    \"\"\"Find headers in HTML that contain the given word, and include page info and parent H1 from doc.\"\"\"\n",
    "    try:\n",
    "        from bs4 import BeautifulSoup\n",
    "        from docling_core.types.doc.document import SectionHeaderItem\n",
    "        soup = BeautifulSoup(html_string, 'html.parser')\n",
    "        headers = []\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            text = tag.get_text().strip()\n",
    "            if word.lower() in text.lower():\n",
    "                # Find parent H1\n",
    "                parent_h1 = None\n",
    "                current = tag\n",
    "                while current:\n",
    "                    current = current.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "                    if current and current.name == 'h1':\n",
    "                        parent_h1 = current.get_text().strip()\n",
    "                        break\n",
    "                # Find corresponding SectionHeaderItem in doc\n",
    "                page = None\n",
    "                for item in doc.texts:\n",
    "                    if isinstance(item, SectionHeaderItem) and item.text.strip() == text:\n",
    "                        prov = getattr(item, \"prov\", None)\n",
    "                        if prov:\n",
    "                            for p in prov:\n",
    "                                pg = getattr(p, \"page_no\", None)\n",
    "                                if pg is not None:\n",
    "                                    page = int(pg)\n",
    "                                    break\n",
    "                        break\n",
    "                headers.append((tag.name, text, page, parent_h1))\n",
    "        return headers\n",
    "    except ImportError:\n",
    "        print(\"BeautifulSoup not available. Install with: pip install beautifulsoup4\")\n",
    "        return []\n",
    "\n",
    "# Call the function\n",
    "html = export_doc_html(doc)\n",
    "replacement_headers = find_headers_in_html(doc, html, 'replacement')\n",
    "for level, text, page, parent_h1 in replacement_headers:\n",
    "    print(f\"{level}: {text} (page {page}) - Parent H1: {parent_h1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054b38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_semantic_similarity(headers_list, query):\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between a list of headers and a query string using TF-IDF vectors.\n",
    "    \n",
    "    Args:\n",
    "        headers_list: List of tuples (level, text, page, parent_h1) from find_headers_in_html or similar.\n",
    "        query: The query string (phrase or word).\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with 'header', 'cosine_similarity', and 'euclidean_distance'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        \n",
    "        # Extract texts from headers\n",
    "        texts = [text for _, text, _, _ in headers_list]\n",
    "        texts.append(query)\n",
    "        \n",
    "        # Vectorize using TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Query vector is the last one\n",
    "        query_vec = tfidf_matrix[-1]\n",
    "        \n",
    "        similarities = []\n",
    "        for i, header in enumerate(headers_list):\n",
    "            header_vec = tfidf_matrix[i]\n",
    "            \n",
    "            # Cosine similarity\n",
    "            cos_sim = cosine_similarity(query_vec, header_vec)[0][0]\n",
    "            \n",
    "            # Euclidean distance\n",
    "            euclidean = np.linalg.norm(query_vec.toarray() - header_vec.toarray())\n",
    "            \n",
    "            similarities.append({\n",
    "                'header': header,\n",
    "                'cosine_similarity': cos_sim,\n",
    "                'euclidean_distance': euclidean\n",
    "            })\n",
    "        \n",
    "        return similarities\n",
    "    except ImportError as e:\n",
    "        print(f\"Required libraries not available: {e}. Install scikit-learn and numpy.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75eb48ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HTML to artifacts/doc.html\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "header",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cosine_similarity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "euclidean_distance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e54a9103-9d1e-4f9c-ba35-f9956dbb0c3f",
       "rows": [
        [
         "0",
         "('h2', 'Cabin air filter', 203, None)",
         "0.6705436749433179",
         "0.81173434700853"
        ],
        [
         "1",
         "('h2', 'Cabin air filter', 203, None)",
         "0.6705436749433179",
         "0.81173434700853"
        ],
        [
         "2",
         "('h2', 'Cabin Air Filter', 493, None)",
         "0.6705436749433179",
         "0.81173434700853"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/plain": [
       "[{'header': ('h2', 'Cabin air filter', 203, None),\n",
       "  'cosine_similarity': np.float64(0.6705436749433179),\n",
       "  'euclidean_distance': np.float64(0.81173434700853)},\n",
       " {'header': ('h2', 'Cabin air filter', 203, None),\n",
       "  'cosine_similarity': np.float64(0.6705436749433179),\n",
       "  'euclidean_distance': np.float64(0.81173434700853)},\n",
       " {'header': ('h2', 'Cabin Air Filter', 493, None),\n",
       "  'cosine_similarity': np.float64(0.6705436749433179),\n",
       "  'euclidean_distance': np.float64(0.81173434700853)}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = export_doc_html(doc)\n",
    "replacement_headers = find_headers_in_html(doc, html, 'cabin air filter')\n",
    "compute_semantic_similarity(replacement_headers, 'replace cabin air filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94332cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_ref='#/texts/9550' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=492, bbox=BoundingBox(l=36.734, t=514.1930122070312, r=138.19, b=503.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 18))] orig='Filter replacement' text='Filter replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9579' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=493, bbox=BoundingBox(l=235.158, t=393.1940122070312, r=336.374, b=382.7230122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 18))] orig='Filter replacement' text='Filter replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9602' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=494, bbox=BoundingBox(l=220.985, t=276.3510122070312, r=324.841, b=265.8810122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 17))] orig='Blade replacement' text='Blade replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9611' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=495, bbox=BoundingBox(l=50.907, t=547.1660122070313, r=184.713, b=528.4450122070313, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 40))] orig='Front windshield wiper blade replacement' text='Front windshield wiper blade replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9636' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=496, bbox=BoundingBox(l=220.985, t=547.1660122070313, r=337.751, b=528.4450122070313, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 35))] orig='Rear window wiper blade replacement' text='Rear window wiper blade replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9784' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=503, bbox=BoundingBox(l=235.158, t=547.1930122070312, r=328.694, b=536.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 16))] orig='Tire replacement' text='Tire replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9798' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=504, bbox=BoundingBox(l=36.734, t=367.33301220703123, r=183.008, b=358.6110122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 30))] orig='Compact spare tire replacement' text='Compact spare tire replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/9804' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=504, bbox=BoundingBox(l=220.985, t=547.1930122070312, r=328.441, b=536.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 17))] orig='Wheel replacement' text='Wheel replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10007' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=514, bbox=BoundingBox(l=36.734, t=547.1930122070312, r=159.729, b=524.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 33))] orig='Instrument panel fuse replacement' text='Instrument panel fuse replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10019' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=514, bbox=BoundingBox(l=220.985, t=448.1930122070312, r=373.019, b=425.7230122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 41))] orig='Engine compartment panel fuse replacement' text='Engine compartment panel fuse replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10108' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=526, bbox=BoundingBox(l=36.734, t=486.3510122070312, r=166.749, b=475.8800122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 23))] orig='Front light replacement' text='Front light replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10139' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=527, bbox=BoundingBox(l=235.158, t=547.1930122070312, r=338.954, b=524.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 31))] orig='Side repeater light replacement' text='Side repeater light replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10143' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=527, bbox=BoundingBox(l=235.158, t=308.6740122070312, r=362.953, b=286.2030122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 34))] orig='Rear combination light replacement' text='Rear combination light replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10154' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=528, bbox=BoundingBox(l=36.734, t=491.1930122070312, r=172.449, b=468.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 35))] orig='High mounted stop light replacement' text='High mounted stop light replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10158' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=528, bbox=BoundingBox(l=220.985, t=547.1930122070312, r=323.341, b=524.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 31))] orig='License plate light replacement' text='License plate light replacement' formatting=None hyperlink=None level=1\n",
      "self_ref='#/texts/10170' parent=RefItem(cref='#/body') children=[] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.SECTION_HEADER: 'section_header'> prov=[ProvenanceItem(page_no=529, bbox=BoundingBox(l=50.907, t=547.1930122070312, r=192.441, b=536.7220122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 26))] orig='Interior light replacement' text='Interior light replacement' formatting=None hyperlink=None level=1\n"
     ]
    }
   ],
   "source": [
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "def find_headers_with_word(doc, word):\n",
    "    \"\"\"Find all SectionHeaderItem that contain the given word in their text.\"\"\"\n",
    "    matches = []\n",
    "    for text in doc.texts:\n",
    "        if isinstance(text, SectionHeaderItem):\n",
    "            if word.lower() in text.text.lower():\n",
    "                matches.append(text)\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "replacement_headers = find_headers_with_word(doc, 'replacement')\n",
    "for header in replacement_headers:\n",
    "    print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567242d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8920d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline timings:\n",
      "             stage   time (s)\n",
      "0   pipeline_total  93.107376\n",
      "1        doc_build  87.146748\n",
      "7     doc_assemble   5.877417\n",
      "8    reading_order   1.764754\n",
      "4           layout   0.644790\n",
      "9       doc_enrich   0.082080\n",
      "3       page_parse   0.075277\n",
      "2        page_init   0.001723\n",
      "5  table_structure   0.000135\n",
      "6    page_assemble   0.000041\n"
     ]
    }
   ],
   "source": [
    "# The `profile_pipeline_timings` setting gives you a breakdown of the time spent in each step.\n",
    "# Let's print it out to see where the bottleneck is.\n",
    "import pandas as pd\n",
    "\n",
    "timings = []\n",
    "for stage, timing_info in conversion_result.timings.items():\n",
    "    timings.append({'stage': stage, 'time (s)': timing_info.times[0]})\n",
    "\n",
    "df_timings = pd.DataFrame(timings)\n",
    "df_timings = df_timings.sort_values(by='time (s)', ascending=False)\n",
    "print(\"Pipeline timings:\")\n",
    "print(df_timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097fbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23184a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: how do i change the cabin air filter?\n"
     ]
    }
   ],
   "source": [
    "# Parameters for header-only selection\n",
    "query = \"how do i change the cabin air filter?\"  # set this per user request\n",
    "print(\"Query:\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e493e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2133 body headers.\n",
      "Indexed 1911 headers with content.\n"
     ]
    }
   ],
   "source": [
    "# Header index with spaCy lemmatization and optional WordNet synonyms\n",
    "import re\n",
    "from typing import List, Any, Tuple, Dict, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "# Load spaCy model (lightweight) and optionally WordNet\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except Exception:\n",
    "        # Try to download if missing (comment out if offline)\n",
    "        import sys, subprocess\n",
    "        subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=False)\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    nlp = None\n",
    "    print(\"spaCy not available:\", e)\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    try:\n",
    "        _ = wn.synsets(\"test\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"omw-1.4\")\n",
    "except Exception as e:\n",
    "    wn = None\n",
    "    print(\"WordNet not available:\", e)\n",
    "\n",
    "\n",
    "def is_body(x: Any) -> bool:\n",
    "    v = getattr(x, \"content_layer\", None)\n",
    "    return getattr(v, \"value\", v) == \"body\"\n",
    "\n",
    "texts: List[Any] = list(doc.texts)\n",
    "headers: List[Tuple[int, SectionHeaderItem]] = [\n",
    "    (i, t) for i, t in enumerate(texts) if isinstance(t, SectionHeaderItem) and is_body(t)\n",
    "]\n",
    "print(f\"Found {len(headers)} body headers.\")\n",
    "\n",
    "# --- Page helpers ---\n",
    "\n",
    "def item_pages(obj: Any) -> Set[int]:\n",
    "    pages: Set[int] = set()\n",
    "    prov = getattr(obj, \"prov\", None)\n",
    "    if prov:\n",
    "        for p in prov:\n",
    "            pg = getattr(p, \"page_no\", None)\n",
    "            if pg is not None:\n",
    "                try:\n",
    "                    pages.add(int(pg))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    # Fallback if no provenance: use page_ref (0-indexed in many docling builds)\n",
    "    pr = getattr(obj, \"page_ref\", None)\n",
    "    if pr is not None and not pages:\n",
    "        try:\n",
    "            pages.add(int(pr) + 1)\n",
    "        except Exception:\n",
    "            pages.add(1)\n",
    "    return pages\n",
    "\n",
    "\n",
    "def nodes_pages(nodes: List[Any]) -> Set[int]:\n",
    "    ps: Set[int] = set()\n",
    "    for n in nodes:\n",
    "        ps |= item_pages(n)\n",
    "    return ps\n",
    "\n",
    "# Slice a section and filter out empty/TOC\n",
    "\n",
    "def slice_nodes(i: int) -> Tuple[SectionHeaderItem, List[Any]]:\n",
    "    h = texts[i]\n",
    "    lvl = getattr(h, \"level\", 3)\n",
    "    nodes = []\n",
    "    for j in range(i + 1, len(texts)):\n",
    "        t = texts[j]\n",
    "        if not is_body(t):\n",
    "            continue\n",
    "        if isinstance(t, SectionHeaderItem) and getattr(t, \"level\", 3) <= lvl:\n",
    "            break\n",
    "        nodes.append(t)\n",
    "    return h, nodes\n",
    "\n",
    "\n",
    "def has_content(nodes: List[Any]) -> bool:\n",
    "    textish = 0\n",
    "    structural = 0\n",
    "    for n in nodes:\n",
    "        name = n.__class__.__name__.lower()\n",
    "        if hasattr(n, \"text\") and name != \"sectionheaderitem\":\n",
    "            if re.search(r\"\\w\", getattr(n, \"text\", \"\") or \"\"):\n",
    "                textish += 1\n",
    "        if hasattr(n, \"items\") or hasattr(n, \"num_rows\") or hasattr(n, \"caption\"):\n",
    "            structural += 1\n",
    "    return textish >= 1 or structural >= 1\n",
    "\n",
    "# Build header index with POS-tagged lemmas and document pages\n",
    "IndexItem = Dict[str, Any]\n",
    "index: List[IndexItem] = []\n",
    "\n",
    "for i, h in headers:\n",
    "    title = getattr(h, \"text\", \"\") or \"\"\n",
    "    header_ps = item_pages(h)\n",
    "    nodes = slice_nodes(i)[1]\n",
    "    if not has_content(nodes):\n",
    "        continue\n",
    "    section_ps = nodes_pages(nodes)\n",
    "\n",
    "    nouns: Set[str] = set()\n",
    "    verbs: Set[str] = set()\n",
    "\n",
    "    if nlp is not None:\n",
    "        doc_h = nlp(title)\n",
    "        for tok in doc_h:\n",
    "            if tok.is_stop or not tok.is_alpha:\n",
    "                continue\n",
    "            lemma = tok.lemma_.lower()\n",
    "            if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "                nouns.add(lemma)\n",
    "            elif tok.pos_ in (\"VERB\",):\n",
    "                verbs.add(lemma)\n",
    "    else:\n",
    "        # Fallback: simple regex tokenization, no POS\n",
    "        for t in re.findall(r\"\\b[\\w-]+\\b\", title.lower()):\n",
    "            if len(t) >= 2:\n",
    "                nouns.add(t)\n",
    "\n",
    "    # Optional synonym expansion via WordNet\n",
    "    syns_n: Set[str] = set()\n",
    "    syns_v: Set[str] = set()\n",
    "    if wn is not None:\n",
    "        for n in nouns:\n",
    "            for s in wn.synsets(n, pos=wn.NOUN):\n",
    "                for l in s.lemma_names():\n",
    "                    syns_n.add(l.replace(\"_\", \" \").lower())\n",
    "        for v in verbs:\n",
    "            for s in wn.synsets(v, pos=wn.VERB):\n",
    "                for l in s.lemma_names():\n",
    "                    syns_v.add(l.replace(\"_\", \" \").lower())\n",
    "\n",
    "    index.append({\n",
    "        \"i\": i,\n",
    "        # First page where the header is located (doc page numbers)\n",
    "        \"header_pages\": sorted(header_ps),\n",
    "        # Pages covered by the section content\n",
    "        \"section_pages\": sorted(section_ps),\n",
    "        # Convenience union of all pages for this section (header + content)\n",
    "        \"doc_pages\": sorted((header_ps | section_ps)),\n",
    "        \"level\": getattr(h, \"level\", 3),\n",
    "        \"title\": title,\n",
    "        \"nouns\": nouns,\n",
    "        \"verbs\": verbs,\n",
    "        \"syn_nouns\": syns_n,\n",
    "        \"syn_verbs\": syns_v,\n",
    "    })\n",
    "\n",
    "print(f\"Indexed {len(index)} headers with content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2517245e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top headers:\n",
      " 1. p 203 h1 score=0.580 | noun=1.00 verb=0.00 n_syn=1.00 v_syn=0.00 fuzz=0.60 :: Cabin air filter  [pages: 203]\n",
      " 2. p 212 h1 score=0.580 | noun=1.00 verb=0.00 n_syn=1.00 v_syn=0.00 fuzz=0.60 :: Cabin air filter  [pages: 212]\n",
      " 3. p 483 h1 score=0.433 | noun=0.67 verb=0.00 n_syn=0.94 v_syn=0.00 fuzz=0.51 :: Air cleaner filter  [pages: 483]\n",
      " 4. p 321 h1 score=0.421 | noun=0.00 verb=1.00 n_syn=0.00 v_syn=1.00 fuzz=0.42 :: · Changing lanes  [pages: 321,322]\n",
      " 5. p 374 h1 score=0.421 | noun=0.00 verb=1.00 n_syn=0.00 v_syn=1.00 fuzz=0.42 :: · Changing lanes  [pages: 374]\n",
      "Chosen: doc pages [203] 'Cabin air filter' score=0.580\n",
      "Saved header_syn_lemma_best_section.html\n"
     ]
    }
   ],
   "source": [
    "# Rank headers using query nouns/verbs + WordNet synonyms; pick best and save section\n",
    "from html import escape\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "\n",
    "# Extract query nouns/verbs via spaCy (or fallback)\n",
    "q_nouns, q_verbs = set(), set()\n",
    "q_text = query\n",
    "if 'nlp' in globals() and nlp is not None:\n",
    "    qdoc = nlp(q_text)\n",
    "    for tok in qdoc:\n",
    "        if tok.is_stop or not tok.is_alpha:\n",
    "            continue\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            q_nouns.add(lemma)\n",
    "        elif tok.pos_ in (\"VERB\",):\n",
    "            q_verbs.add(lemma)\n",
    "else:\n",
    "    for t in re.findall(r\"\\b[\\w-]+\\b\", q_text.lower()):\n",
    "        if len(t) >= 2:\n",
    "            q_nouns.add(t)\n",
    "\n",
    "# Expand query with WordNet if available\n",
    "q_syn_n, q_syn_v = set(), set()\n",
    "if 'wn' in globals() and wn is not None:\n",
    "    for n in q_nouns:\n",
    "        for s in wn.synsets(n, pos=wn.NOUN):\n",
    "            for l in s.lemma_names():\n",
    "                q_syn_n.add(l.replace(\"_\", \" \").lower())\n",
    "    for v in q_verbs:\n",
    "        for s in wn.synsets(v, pos=wn.VERB):\n",
    "            for l in s.lemma_names():\n",
    "                q_syn_v.add(l.replace(\"_\", \" \").lower())\n",
    "\n",
    "# Score headers: prioritize noun/verb coverage, then synonyms, then fuzzy title\n",
    "cands = []\n",
    "for h in index:\n",
    "    hn = h['nouns']; hv = h['verbs']\n",
    "    syn_n = h['syn_nouns']; syn_v = h['syn_verbs']\n",
    "\n",
    "    # Coverage\n",
    "    noun_cov = len(q_nouns & hn) / max(1, len(q_nouns))\n",
    "    verb_cov = len(q_verbs & hv) / max(1, len(q_verbs))\n",
    "\n",
    "    # Synonym coverage\n",
    "    noun_syn_cov = len(q_syn_n & (hn | syn_n)) / max(1, len(q_syn_n)) if q_syn_n else 0.0\n",
    "    verb_syn_cov = len(q_syn_v & (hv | syn_v)) / max(1, len(q_syn_v)) if q_syn_v else 0.0\n",
    "\n",
    "    fuzzy = SequenceMatcher(None, q_text.lower(), h['title'].lower()).ratio()\n",
    "\n",
    "    score = (\n",
    "        0.40 * noun_cov +\n",
    "        0.30 * verb_cov +\n",
    "        0.15 * noun_syn_cov +\n",
    "        0.10 * verb_syn_cov +\n",
    "        0.05 * fuzzy\n",
    "    )\n",
    "\n",
    "    # Derive a representative document page for display: first page where this section appears\n",
    "    # Prefer header page if available, else earliest section content page\n",
    "    doc_pages = h.get('doc_pages', [])\n",
    "    first_doc_page = doc_pages[0] if doc_pages else (h.get('header_pages') or h.get('section_pages') or [1])[0]\n",
    "\n",
    "    cands.append({**h, 'score': score, 'fuzzy': fuzzy,\n",
    "                  'noun_cov': noun_cov, 'verb_cov': verb_cov,\n",
    "                  'noun_syn_cov': noun_syn_cov, 'verb_syn_cov': verb_syn_cov,\n",
    "                  'first_doc_page': first_doc_page})\n",
    "\n",
    "cands.sort(key=lambda x: x['score'], reverse=True)\n",
    "print(\"Top headers:\")\n",
    "for r, c in enumerate(cands[:5], start=1):\n",
    "    # Show the full list of document pages for each candidate\n",
    "    pages_str = ','.join(str(p) for p in c.get('doc_pages', []) or c.get('header_pages', []) or c.get('section_pages', []) or [\"?\"])\n",
    "    print(f\"{r:>2}. p{c['first_doc_page']:>4} h{c['level']} score={c['score']:.3f} | noun={c['noun_cov']:.2f} verb={c['verb_cov']:.2f} n_syn={c['noun_syn_cov']:.2f} v_syn={c['verb_syn_cov']:.2f} fuzz={c['fuzzy']:.2f} :: {c['title']}  [pages: {pages_str}]\")\n",
    "\n",
    "best = cands[0]\n",
    "print(f\"Chosen: doc pages {best.get('doc_pages', best.get('header_pages', best.get('section_pages', ['?'])))} '{best['title']}' score={best['score']:.3f}\")\n",
    "\n",
    "# Slice and save rendered HTML\n",
    "h, nodes = slice_nodes(best['i'])\n",
    "\n",
    "# Augment HTML header with page info\n",
    "page_info = ', '.join(str(p) for p in best.get('doc_pages') or best.get('header_pages') or best.get('section_pages') or [])\n",
    "\n",
    "\n",
    "def render_nodes(nodes: List[Any]) -> str:\n",
    "    parts = []\n",
    "    for n in nodes:\n",
    "        name = n.__class__.__name__.lower()\n",
    "        if hasattr(n, 'text') and name != 'sectionheaderitem':\n",
    "            t = getattr(n, 'text', '')\n",
    "            if t:\n",
    "                parts.append(f\"<p>{escape(t)}</p>\")\n",
    "        elif hasattr(n, 'items'):\n",
    "            parts.append('<ul>')\n",
    "            for it in getattr(n, 'items', []) or []:\n",
    "                parts.append(f\"<li>{escape(getattr(it, 'text', str(it)) or '')}</li>\")\n",
    "            parts.append('</ul>')\n",
    "        elif hasattr(n, 'num_rows') and hasattr(n, 'num_cols') and hasattr(n, 'cells'):\n",
    "            rows = []\n",
    "            for r in range(n.num_rows):\n",
    "                cells = []\n",
    "                for c in range(n.num_cols):\n",
    "                    cell = n.cells[r][c]\n",
    "                    cells.append(f\"<td>{escape(getattr(cell, 'text', '') or '')}</td>\")\n",
    "                rows.append('<tr>' + ''.join(cells) + '</tr>')\n",
    "            parts.append('<table>' + ''.join(rows) + '</table>')\n",
    "        elif hasattr(n, 'caption'):\n",
    "            cap = getattr(getattr(n, 'caption', None), 'text', '') or ''\n",
    "            if cap:\n",
    "                parts.append(f\"<figure><figcaption>{escape(cap)}</figcaption></figure>\")\n",
    "    return '\\n'.join(parts)\n",
    "\n",
    "html_out = f\"\"\"\n",
    "<!doctype html>\n",
    "<html><head><meta charset='utf-8'><title>{escape(best['title'])}</title>\n",
    "<style>body{{font-family:Segoe UI, Roboto, Arial, sans-serif; line-height:1.5; padding:1rem}} ul{{margin-left:1.25rem}}</style>\n",
    "</head><body>\n",
    "<h2>{escape(best['title'])}</h2>\n",
    "<p><em>Document pages: {escape(page_info)}</em></p>\n",
    "{render_nodes(nodes)}\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "out_dir = Path('artifacts/sections')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "(out_dir / 'header_syn_lemma_best_section.html').write_text(html_out, encoding='utf-8')\n",
    "print('Saved header_syn_lemma_best_section.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8cde0e",
   "metadata": {},
   "source": [
    "# Subtree-based selection: rank a section together with its subsections\n",
    "\n",
    "This section adds a minimal, reusable, and PEP8-compliant utility layer to:\n",
    "\n",
    "- Build a header tree from `doc.texts` (parent/children relationships by header levels)\n",
    "- Compute page coverage using provenance (preferred) and page_ref fallback\n",
    "- Extract linguistic features (noun/verb lemmas and optional WordNet synonyms) for titles\n",
    "- Construct a subtree index (parent + immediate children merged semantics)\n",
    "- Rank subtrees against a query and explore the top candidates with pages and child headers\n",
    "\n",
    "Outputs:\n",
    "- Console summary of the top subtree candidates (parent + children, doc pages, coverage)\n",
    "- Saved HTML for the best subtree slice: `artifacts/sections/subtree_best_section.html`\n",
    "\n",
    "Design notes:\n",
    "- We keep this independent from existing cells. Functions are documented and PEP8-compliant.\n",
    "- We limit subtree semantics to parent + immediate children to stay fast and deterministic.\n",
    "- Rendering uses the existing structural slice: from the parent header until the next header of the same or higher level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc45806",
   "metadata": {},
   "outputs": [],
   "source": [
    "    node = HeaderNode(index=i, level=level, title=title, parent=parent, children=[], header_pages=header_pages, section_pages=list(set().union(*section_pages)), doc_pages=doc_pages)\n",
    "    node.nodes = nodes\n",
    "    nodes_map[i] = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ab8f241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top subtrees (parent + immediate children):\n",
      " 1. p203 h1 score=0.589 :: Cabin air filter\n",
      "    subtree noun matches: air, cabin, filter\n",
      " 2. p212 h1 score=0.589 :: Cabin air filter\n",
      "    subtree noun matches: air, cabin, filter\n",
      " 3. p493 h1 score=0.589 :: Cabin Air Filter\n",
      "    subtree noun matches: air, cabin, filter\n",
      " 4. p483 h1 score=0.408 :: Air cleaner filter\n",
      "    subtree noun matches: air, filter\n",
      " 5. p139 h1 score=0.376 :: Replacing the battery\n",
      "    subtree noun matches: battery\n",
      "    subtree verb matches: replace\n",
      "Saved subtree_best_section.html\n"
     ]
    }
   ],
   "source": [
    "# Minimal runner for subtree selection and action bump\n",
    "from pathlib import Path\n",
    "\n",
    "def run_subtree_selection(query: str, top_k: int = 5, out_dir: Path = Path(\"artifacts/sections\"), out_name: str = \"subtree_best_section.html\"):\n",
    "    tree = build_header_tree(doc)\n",
    "    subtrees = build_subtree_index(tree)\n",
    "    scored = [(score_subtree(query, st), st) for st in subtrees]\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    print(\"Top subtrees (parent + immediate children):\")\n",
    "    for rank, (score, st) in enumerate(scored[:top_k], start=1):\n",
    "        pages_str = \",\".join(str(p) for p in st[\"doc_pages\"])\n",
    "        print(f\"{rank:>2}. p{pages_str} h{st['level']} score={score:.3f} :: {st['title']}\")\n",
    "        if st[\"child_titles\"]:\n",
    "            print(f\"    child titles: {st['child_titles']}\")\n",
    "        pn = \", \".join(sorted(st[\"subtree_nouns\"]))\n",
    "        pv = \", \".join(sorted(st[\"subtree_verbs\"]))\n",
    "        if pn:\n",
    "            print(f\"    subtree noun matches: {pn}\")\n",
    "        if pv:\n",
    "            print(f\"    subtree verb matches: {pv}\")\n",
    "        if st[\"child_action_hits\"]:\n",
    "            print(f\"    child action hits: {st['child_action_hits']}\")\n",
    "    # Save best subtree\n",
    "    if scored:\n",
    "        best_idx = scored[0][1][\"idx\"]\n",
    "        out_path = out_dir / out_name\n",
    "        save_best_subtree_html(tree, best_idx, out_path)\n",
    "        print(f\"Saved {out_path.name}\")\n",
    "\n",
    "# Example runner\n",
    "if 'query' in globals():\n",
    "    run_subtree_selection(query=query, top_k=5)\n",
    "else:\n",
    "    print(\"Set a `query variable (str) and re-run this cell to execute subtree selection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8ca9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I replace the cabin air filter?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
