{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Get the logger for docling and set its level\n",
    "logging.getLogger('docling').setLevel(logging.INFO)\n",
    "logging.getLogger('docling_core').setLevel(logging.INFO)\n",
    "log = logging.getLogger(__name__)  # This makes your script a logging-aware application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_headers_in_html(doc, html_string, word):\n",
    "    \"\"\"Find headers in HTML that contain the given word, and include page info and parent H1 from doc.\"\"\"\n",
    "    try:\n",
    "        from bs4 import BeautifulSoup\n",
    "        from docling_core.types.doc.document import SectionHeaderItem\n",
    "        soup = BeautifulSoup(html_string, 'html.parser')\n",
    "        headers = []\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            text = tag.get_text().strip()\n",
    "            if word.lower() in text.lower():\n",
    "                # Find parent H1\n",
    "                parent_h1 = None\n",
    "                current = tag\n",
    "                while current:\n",
    "                    current = current.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "                    if current and current.name == 'h1':\n",
    "                        parent_h1 = current.get_text().strip()\n",
    "                        break\n",
    "                # Find corresponding SectionHeaderItem in doc\n",
    "                page = None\n",
    "                for item in doc.texts:\n",
    "                    if isinstance(item, SectionHeaderItem) and item.text.strip() == text:\n",
    "                        prov = getattr(item, \"prov\", None)\n",
    "                        if prov:\n",
    "                            for p in prov:\n",
    "                                pg = getattr(p, \"page_no\", None)\n",
    "                                if pg is not None:\n",
    "                                    page = int(pg)\n",
    "                                    break\n",
    "                        break\n",
    "                headers.append((tag.name, text, page, parent_h1))\n",
    "        return headers\n",
    "    except ImportError:\n",
    "        print(\"BeautifulSoup not available. Install with: pip install beautifulsoup4\")\n",
    "        return []\n",
    "\n",
    "# Call the function\n",
    "html = export_doc_html(doc)\n",
    "replacement_headers = find_headers_in_html(doc, html, 'replacement')\n",
    "for level, text, page, parent_h1 in replacement_headers:\n",
    "    print(f\"{level}: {text} (page {page}) - Parent H1: {parent_h1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling_core.types.doc.document import DoclingDocument\n",
    "\n",
    "chunk = Path(r'data/temp_chunk_0-91_kona.json')\n",
    "doc = DoclingDocument.load_from_json(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eefd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "def compute_header_similarity(doc, query, top_n=5):\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between all headers in a DoclingDocument and a query string using TF-IDF vectors.\n",
    "Args:\n",
    "    doc: DoclingDocument object containing the document.\n",
    "    query: The query string (phrase or word).\n",
    "    top_n: Number of top similar headers to return (default: 5).\n",
    "\n",
    "Returns:\n",
    "    List of dicts with 'header_text', 'similarity_score', 'header_item', and 'page' for the top-N headers.\n",
    "    Each dict contains:\n",
    "    - 'header_text': The text of the header.\n",
    "    - 'similarity_score': Cosine similarity score (0-1).\n",
    "    - 'header_item': The SectionHeaderItem object.\n",
    "    - 'page': The page number where the header appears (if available).\n",
    "\"\"\"\n",
    "    try:\n",
    "        # Extract all headers from the document\n",
    "        headers = []\n",
    "        for item in doc.texts:\n",
    "            if isinstance(item, SectionHeaderItem):\n",
    "                # Get page info\n",
    "                page = None\n",
    "                if hasattr(item, 'prov') and item.prov:\n",
    "                    for p in item.prov:\n",
    "                        pg = getattr(p, 'page_no', None)\n",
    "                        if pg is not None:\n",
    "                            page = int(pg)\n",
    "                            break\n",
    "                \n",
    "                headers.append({\n",
    "                    'text': item.text,\n",
    "                    'item': item,\n",
    "                    'page': page\n",
    "                })\n",
    "        \n",
    "        if not headers:\n",
    "            return []\n",
    "        \n",
    "        # Extract texts for vectorization\n",
    "        texts = [h['text'] for h in headers]\n",
    "        texts.append(query)\n",
    "        \n",
    "        # Vectorize using TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Query vector is the last one\n",
    "        query_vec = tfidf_matrix[-1]\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = []\n",
    "        for i, header in enumerate(headers):\n",
    "            header_vec = tfidf_matrix[i]\n",
    "            \n",
    "            # Cosine similarity\n",
    "            cos_sim = cosine_similarity(query_vec, header_vec)[0][0]\n",
    "            \n",
    "            similarities.append({\n",
    "                'header_text': header['text'],\n",
    "                'similarity_score': cos_sim,\n",
    "                'header_item': header['item'],\n",
    "                'page': header['page']\n",
    "            })\n",
    "        \n",
    "        # Sort by similarity score descending and return top-N\n",
    "        similarities.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"Required libraries not available: {e}. Install scikit-learn and numpy.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing similarity: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where can I find the VIN?\"\n",
    "compute_header_similarity(doc, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce62b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Any, Tuple, Dict, Set\n",
    "from docling_core.types.doc.document import SectionHeaderItem, DoclingDocument\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from html import escape\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def rank_and_save_best_section_with_hdbscan(doc: DoclingDocument, query: str, top_n: int = 5, hdbscan_results: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rank headers in a DoclingDocument using query nouns/verbs + WordNet synonyms + HDBSCAN cluster similarity, and return top N matches.\n",
    "    \n",
    "    This function processes the document to extract headers, analyzes their linguistic features (nouns, verbs, synonyms),\n",
    "    scores them against the query based on coverage, fuzzy matching, and cluster similarity, and returns the top N ranked headers with all data.\n",
    "    \n",
    "    Args:\n",
    "        doc (DoclingDocument): The document object containing the text elements to process.\n",
    "        query (str): The query string to match against header titles.\n",
    "        top_n (int, optional): Number of top matches to return. Defaults to 5.\n",
    "        hdbscan_results (Dict[str, Any], optional): Precomputed HDBSCAN results from perform_hdbscan_on_headers. If None, clustering is skipped.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of dictionaries for the top N headers, each containing:\n",
    "            - 'title': Header text\n",
    "            - 'score': Matching score (0-1)\n",
    "            - 'first_doc_page': First page number\n",
    "            - 'doc_pages': List of all page numbers\n",
    "            - 'level': Header level\n",
    "            - 'nouns': Set of nouns in header\n",
    "            - 'verbs': Set of verbs in header\n",
    "            - 'syn_nouns': Set of synonym nouns\n",
    "            - 'syn_verbs': Set of synonym verbs\n",
    "            - 'noun_cov': Noun coverage score\n",
    "            - 'verb_cov': Verb coverage score\n",
    "            - 'noun_syn_cov': Synonym noun coverage\n",
    "            - 'verb_syn_cov': Synonym verb coverage\n",
    "            - 'fuzzy': Fuzzy match score\n",
    "            - 'cluster_sim': Cluster similarity score (if hdbscan_results provided)\n",
    "    \n",
    "    Raises:\n",
    "        Any exceptions from spaCy, NLTK, or file operations are not caught and will propagate.\n",
    "    \n",
    "    Example:\n",
    "        >>> from pathlib import Path\n",
    "        >>> from docling_core.types.doc.document import DoclingDocument\n",
    "        >>> doc = DoclingDocument.load_from_json(Path(\"data/temp_chunk_0-91_kona.json\"))\n",
    "        >>> header_data = extract_header_texts(doc)\n",
    "        >>> hdbscan_results = perform_hdbscan_on_headers(header_data)\n",
    "        >>> results = rank_and_save_best_section_with_hdbscan(doc, \"Where can I find the VIN?\", top_n=3, hdbscan_results=hdbscan_results)\n",
    "        >>> for r in results:\n",
    "        ...     print(f\"{r['title']} (page {r['first_doc_page']}) - Score: {r['score']:.3f}\")\n",
    "    \"\"\"\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    # Download WordNet if needed\n",
    "    # nltk.download(\"wordnet\")\n",
    "    # nltk.download(\"omw-1.4\")\n",
    "    \n",
    "    def is_body(x: Any) -> bool:\n",
    "        \"\"\"Check if the text element belongs to the body content layer.\"\"\"\n",
    "        v = getattr(x, \"content_layer\", None)\n",
    "        return getattr(v, \"value\", v) == \"body\"\n",
    "    \n",
    "    texts: List[Any] = list(doc.texts)\n",
    "    headers: List[Tuple[int, SectionHeaderItem]] = [\n",
    "        (i, t) for i, t in enumerate(texts) if isinstance(t, SectionHeaderItem) and is_body(t)\n",
    "    ]\n",
    "    \n",
    "    def item_pages(obj: Any) -> Set[int]:\n",
    "        \"\"\"Extract page numbers from a document object using provenance or fallback.\"\"\"\n",
    "        pages: Set[int] = set()\n",
    "        prov = getattr(obj, \"prov\", None)\n",
    "        if prov:\n",
    "            for p in prov:\n",
    "                pg = getattr(p, \"page_no\", None)\n",
    "                if pg is not None:\n",
    "                    try:\n",
    "                        pages.add(int(pg))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        pr = getattr(obj, \"page_ref\", None)\n",
    "        if pr is not None and not pages:\n",
    "            try:\n",
    "                pages.add(int(pr) + 1)\n",
    "            except Exception:\n",
    "                pages.add(1)\n",
    "        return pages\n",
    "    \n",
    "    def nodes_pages(nodes: List[Any]) -> Set[int]:\n",
    "        \"\"\"Collect all unique page numbers from a list of nodes.\"\"\"\n",
    "        ps: Set[int] = set()\n",
    "        for n in nodes:\n",
    "            ps |= item_pages(n)\n",
    "        return ps\n",
    "    \n",
    "    def slice_nodes(i: int) -> Tuple[SectionHeaderItem, List[Any]]:\n",
    "        \"\"\"Slice the document to get the section content under a header.\"\"\"\n",
    "        h = texts[i]\n",
    "        lvl = getattr(h, \"level\", 3)\n",
    "        nodes = []\n",
    "        for j in range(i + 1, len(texts)):\n",
    "            t = texts[j]\n",
    "            if not is_body(t):\n",
    "                continue\n",
    "            if isinstance(t, SectionHeaderItem) and getattr(t, \"level\", 3) <= lvl:\n",
    "                break\n",
    "            nodes.append(t)\n",
    "        return h, nodes\n",
    "    \n",
    "    def has_content(nodes: List[Any]) -> bool:\n",
    "        \"\"\"Check if the section nodes contain meaningful content.\"\"\"\n",
    "        textish = 0\n",
    "        structural = 0\n",
    "        for n in nodes:\n",
    "            name = n.__class__.__name__.lower()\n",
    "            if hasattr(n, \"text\") and name != \"sectionheaderitem\":\n",
    "                if re.search(r\"\\w\", getattr(n, \"text\", \"\") or \"\"):\n",
    "                    textish += 1\n",
    "            if hasattr(n, \"items\") or hasattr(n, \"num_rows\") or hasattr(n, \"caption\"):\n",
    "                structural += 1\n",
    "        return textish >= 1 or structural >= 1\n",
    "    \n",
    "    # Build header index\n",
    "    IndexItem = Dict[str, Any]\n",
    "    index: List[IndexItem] = []\n",
    "    \n",
    "    for i, h in headers:\n",
    "        title = getattr(h, \"text\", \"\") or \"\"\n",
    "        header_ps = item_pages(h)\n",
    "        nodes = slice_nodes(i)[1]\n",
    "        if not has_content(nodes):\n",
    "            continue\n",
    "        section_ps = nodes_pages(nodes)\n",
    "        \n",
    "        nouns: Set[str] = set()\n",
    "        verbs: Set[str] = set()\n",
    "        \n",
    "        doc_h = nlp(title)\n",
    "        for tok in doc_h:\n",
    "            if tok.is_stop or not tok.is_alpha:\n",
    "                continue\n",
    "            lemma = tok.lemma_.lower()\n",
    "            if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "                nouns.add(lemma)\n",
    "            elif tok.pos_ in (\"VERB\",):\n",
    "                verbs.add(lemma)\n",
    "        \n",
    "        syns_n: Set[str] = set()\n",
    "        syns_v: Set[str] = set()\n",
    "        for n in nouns:\n",
    "            for s in wn.synsets(n, pos=wn.NOUN):\n",
    "                if hasattr(s, 'lemma_names'):\n",
    "                    for l in s.lemma_names():\n",
    "                        syns_n.add(l.replace(\"_\", \" \").lower())\n",
    "        for v in verbs:\n",
    "            for s in wn.synsets(v, pos=wn.VERB):\n",
    "                if hasattr(s, 'lemma_names'):\n",
    "                    for l in s.lemma_names():\n",
    "                        syns_v.add(l.replace(\"_\", \" \").lower())\n",
    "        \n",
    "        index.append({\n",
    "            \"i\": i,\n",
    "            \"header_pages\": sorted(header_ps),\n",
    "            \"section_pages\": sorted(section_ps),\n",
    "            \"doc_pages\": sorted((header_ps | section_ps)),\n",
    "            \"level\": getattr(h, \"level\", 3),\n",
    "            \"title\": title,\n",
    "            \"nouns\": nouns,\n",
    "            \"verbs\": verbs,\n",
    "            \"syn_nouns\": syns_n,\n",
    "            \"syn_verbs\": syns_v,\n",
    "        })\n",
    "    \n",
    "    # Extract query nouns/verbs\n",
    "    q_nouns, q_verbs = set(), set()\n",
    "    q_text = query\n",
    "    qdoc = nlp(q_text)\n",
    "    for tok in qdoc:\n",
    "        if tok.is_stop or not tok.is_alpha:\n",
    "            continue\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            q_nouns.add(lemma)\n",
    "        elif tok.pos_ in (\"VERB\",):\n",
    "            q_verbs.add(lemma)\n",
    "    \n",
    "    # Expand query with WordNet\n",
    "    q_syn_n, q_syn_v = set(), set()\n",
    "    for n in q_nouns:\n",
    "        for s in wn.synsets(n, pos=wn.NOUN):\n",
    "            if hasattr(s, 'lemma_names'):\n",
    "                for l in s.lemma_names():\n",
    "                    q_syn_n.add(l.replace(\"_\", \" \").lower())\n",
    "    for v in q_verbs:\n",
    "        for s in wn.synsets(v, pos=wn.VERB):\n",
    "            if hasattr(s, 'lemma_names'):\n",
    "                for l in s.lemma_names():\n",
    "                    q_syn_v.add(l.replace(\"_\", \" \").lower())\n",
    "    \n",
    "    # Prepare cluster similarity if HDBSCAN results provided\n",
    "    cluster_sim_scores = {}\n",
    "    if hdbscan_results:\n",
    "        labels = hdbscan_results.get(\"labels\", [])\n",
    "        cluster_names = hdbscan_results.get(\"cluster_names\", {})\n",
    "        \n",
    "        # Vectorize cluster names and query for similarity\n",
    "        cluster_texts = list(cluster_names.values())\n",
    "        cluster_texts.append(q_text)\n",
    "        if cluster_texts:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            cluster_matrix = vectorizer.fit_transform(cluster_texts)\n",
    "            query_vec = cluster_matrix[-1]\n",
    "            \n",
    "            for idx, cluster_name in enumerate(cluster_names.values()):\n",
    "                cluster_vec = cluster_matrix[idx]\n",
    "                sim = cosine_similarity(query_vec, cluster_vec)[0][0]\n",
    "                cluster_sim_scores[cluster_name] = sim\n",
    "    \n",
    "    # Score headers\n",
    "    cands = []\n",
    "    for h_idx, h in enumerate(index):\n",
    "        hn = h['nouns']; hv = h['verbs']\n",
    "        syn_n = h['syn_nouns']; syn_v = h['syn_verbs']\n",
    "        \n",
    "        noun_cov = len(q_nouns & hn) / max(1, len(q_nouns))\n",
    "        verb_cov = len(q_verbs & hv) / max(1, len(q_verbs))\n",
    "        \n",
    "        noun_syn_cov = len(q_syn_n & (hn | syn_n)) / max(1, len(q_syn_n)) if q_syn_n else 0.0\n",
    "        verb_syn_cov = len(q_syn_v & (hv | syn_v)) / max(1, len(q_syn_v)) if q_syn_v else 0.0\n",
    "        \n",
    "        fuzzy = SequenceMatcher(None, q_text.lower(), h['title'].lower()).ratio()\n",
    "        \n",
    "        # Cluster similarity\n",
    "        cluster_sim = 0.0\n",
    "        if hdbscan_results and h_idx < len(labels):\n",
    "            cluster_label = labels[h_idx]\n",
    "            if cluster_label != -1:\n",
    "                cluster_name = cluster_names.get(f\"cluster_{cluster_label}\", \"\")\n",
    "                cluster_sim = cluster_sim_scores.get(cluster_name, 0.0)\n",
    "        \n",
    "        score = (\n",
    "            0.35 * noun_cov +\n",
    "            0.25 * verb_cov +\n",
    "            0.15 * noun_syn_cov +\n",
    "            0.10 * verb_syn_cov +\n",
    "            0.05 * fuzzy +\n",
    "            0.10 * cluster_sim  # Add cluster similarity\n",
    "        )\n",
    "        \n",
    "        doc_pages = h.get('doc_pages', [])\n",
    "        first_doc_page = doc_pages[0] if doc_pages else (h.get('header_pages') or h.get('section_pages') or [1])[0]\n",
    "        \n",
    "        cands.append({**h, 'score': score, 'fuzzy': fuzzy,\n",
    "                      'noun_cov': noun_cov, 'verb_cov': verb_cov,\n",
    "                      'noun_syn_cov': noun_syn_cov, 'verb_syn_cov': verb_syn_cov,\n",
    "                      'cluster_sim': cluster_sim, 'first_doc_page': first_doc_page})\n",
    "    \n",
    "    cands.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Print top results\n",
    "    print(\"Top headers:\")\n",
    "    for r, c in enumerate(cands[:top_n], start=1):\n",
    "        pages_str = ','.join(str(p) for p in c.get('doc_pages', []) or c.get('header_pages', []) or c.get('section_pages', []) or [\"?\"])\n",
    "        print(f\"{r:>2}. p{c['first_doc_page']:>4} h{c['level']} score={c['score']:.3f} | \"\n",
    "              f\"noun={c['noun_cov']:.2f} verb={c['verb_cov']:.2f} n_syn={c['noun_syn_cov']:.2f} \"\n",
    "              f\"v_syn={c['verb_syn_cov']:.2f} fuzz={c['fuzzy']:.2f} clust={c['cluster_sim']:.2f} :: {c['title']}  [pages: {pages_str}]\")\n",
    "    \n",
    "    return cands[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling_core.types.doc.document import DoclingDocument\n",
    "\n",
    "chunk = Path(r'data/temp_chunk_0-91_kona.json')\n",
    "doc = DoclingDocument.load_from_json(chunk)\n",
    "\n",
    "query = \"Where can I find the VIN number?\"\n",
    "results = rank_and_save_best_section_with_hdbscan(doc, query, top_n=5)\n",
    "print(\"\\nReturned data for top matches:\")\n",
    "for r in results:\n",
    "    print(f\"Title: {r['title']}, Page: {r['first_doc_page']}, Score: {r['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.cli.download import download\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "def compute_semantic_header_similarity(doc, query, top_n=5):\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between all headers in a DoclingDocument and a query string using spaCy word vectors.\n",
    "    Args:\n",
    "        doc: DoclingDocument object containing the document.\n",
    "        query: The query string.\n",
    "        top_n: Number of top similar headers to return (default: 5).\n",
    "    Returns:\n",
    "        List of dicts with 'header_text', 'similarity_score', 'header_item', and 'page' for the top-N headers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        # Extract all headers\n",
    "        headers = []\n",
    "        for item in doc.texts:\n",
    "            if isinstance(item, SectionHeaderItem):\n",
    "                page = None\n",
    "                if hasattr(item, 'prov') and item.prov:\n",
    "                    for p in item.prov:\n",
    "                        pg = getattr(p, 'page_no', None)\n",
    "                        if pg is not None:\n",
    "                            page = int(pg)\n",
    "                            break\n",
    "                headers.append({\n",
    "                    'text': item.text,\n",
    "                    'item': item,\n",
    "                    'page': page\n",
    "                })\n",
    "        \n",
    "        if not headers:\n",
    "            return []\n",
    "        \n",
    "        # Function to get vector for a text\n",
    "        def get_vector(text):\n",
    "            doc_nlp = nlp(text)\n",
    "            vectors = [token.vector for token in doc_nlp if token.has_vector and not token.is_stop]\n",
    "            if vectors:\n",
    "                return np.mean(vectors, axis=0)\n",
    "            else:\n",
    "                return np.zeros(300)  # en_core_web_lg has 300 dimensions\n",
    "        \n",
    "        # Get vectors for query and headers\n",
    "        query_vec = get_vector(query)\n",
    "        header_vecs = [get_vector(h['text']) for h in headers]\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        similarities = []\n",
    "        for i, header in enumerate(headers):\n",
    "            header_vec = header_vecs[i]\n",
    "            # Cosine similarity\n",
    "            cos_sim = cosine_similarity([query_vec], [header_vec])[0][0]\n",
    "            similarities.append({\n",
    "                'header_text': header['text'],\n",
    "                'similarity_score': cos_sim,\n",
    "                'header_item': header['item'],\n",
    "                'page': header['page']\n",
    "            })\n",
    "        \n",
    "        # Sort by similarity score descending\n",
    "        similarities.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error computing semantic similarity: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test the semantic similarity\n",
    "query = \"Where can I find the VIN?\"\n",
    "compute_semantic_header_similarity(doc, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the semantic similarity\n",
    "query = \"Where can I find the VIN?\"\n",
    "rank_and_save_best_section(doc, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "# Load spaCy transformer model for better semantic understanding\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def match_headers_to_query(doc: DoclingDocument, query: str, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Match headers in a DoclingDocument to a query using spaCy's transformer model for semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        doc (DoclingDocument): The document to search.\n",
    "        query (str): The query string.\n",
    "        top_n (int): Number of top matching headers to return.\n",
    "    \n",
    "    Returns:\n",
    "        List[dict]: List of dictionaries with header details and similarity scores.\n",
    "    \"\"\"\n",
    "    # Extract all headers\n",
    "    headers = []\n",
    "    for item in doc.texts:\n",
    "        if isinstance(item, SectionHeaderItem):\n",
    "            page = None\n",
    "            if hasattr(item, 'prov') and item.prov:\n",
    "                for p in item.prov:\n",
    "                    pg = getattr(p, 'page_no', None)\n",
    "                    if pg is not None:\n",
    "                        page = int(pg)\n",
    "                        break\n",
    "            headers.append({\n",
    "                'text': item.text,\n",
    "                'item': item,\n",
    "                'page': page\n",
    "            })\n",
    "    \n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Get query doc\n",
    "    query_doc = nlp(query)\n",
    "    \n",
    "    # Get header docs\n",
    "    header_docs = [nlp(h['text']) for h in headers]\n",
    "    \n",
    "    # Compute similarities using spaCy's similarity\n",
    "    similarities = []\n",
    "    for i, header in enumerate(headers):\n",
    "        header_doc = header_docs[i]\n",
    "        sim_score = query_doc.similarity(header_doc)\n",
    "        similarities.append({\n",
    "            'header_text': header['text'],\n",
    "            'similarity_score': sim_score,\n",
    "            'header_item': header['item'],\n",
    "            'page': header['page']\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Example usage\n",
    "query = \"Where can I find the VIN?\"\n",
    "results = match_headers_to_query(doc, query)\n",
    "for result in results:\n",
    "    print(f\"Score: {result['similarity_score']:.3f} | {result['header_text']} (page {result['page']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_semantic_similarity(headers_list, query):\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between a list of headers and a query string using TF-IDF vectors.\n",
    "    \n",
    "    Args:\n",
    "        headers_list: List of tuples (level, text, page, parent_h1) from find_headers_in_html or similar.\n",
    "        query: The query string (phrase or word).\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with 'header', 'cosine_similarity', and 'euclidean_distance'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        \n",
    "        # Extract texts from headers\n",
    "        texts = [text for _, text, _, _ in headers_list]\n",
    "        texts.append(query)\n",
    "        \n",
    "        # Vectorize using TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Query vector is the last one\n",
    "        query_vec = tfidf_matrix[-1]\n",
    "        \n",
    "        similarities = []\n",
    "        for i, header in enumerate(headers_list):\n",
    "            header_vec = tfidf_matrix[i]\n",
    "            \n",
    "            # Cosine similarity\n",
    "            cos_sim = cosine_similarity(query_vec, header_vec)[0][0]\n",
    "            \n",
    "            # Euclidean distance\n",
    "            euclidean = np.linalg.norm(query_vec.toarray() - header_vec.toarray())\n",
    "            \n",
    "            similarities.append({\n",
    "                'header': header,\n",
    "                'cosine_similarity': cos_sim,\n",
    "                'euclidean_distance': euclidean\n",
    "            })\n",
    "        \n",
    "        return similarities\n",
    "    except ImportError as e:\n",
    "        print(f\"Required libraries not available: {e}. Install scikit-learn and numpy.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = export_doc_html(doc)\n",
    "replacement_headers = find_headers_in_html(doc, html, 'cabin air filter')\n",
    "compute_semantic_similarity(replacement_headers, 'replace cabin air filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94332cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "\n",
    "def find_headers_with_word(doc, word):\n",
    "    \"\"\"Find all SectionHeaderItem that contain the given word in their text.\"\"\"\n",
    "    matches = []\n",
    "    for text in doc.texts:\n",
    "        if isinstance(text, SectionHeaderItem):\n",
    "            if word.lower() in text.text.lower():\n",
    "                matches.append(text)\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "replacement_headers = find_headers_with_word(doc, 'replacement')\n",
    "for header in replacement_headers:\n",
    "    print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23184a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for header-only selection\n",
    "query = \"Where can I find the VIN?\"  # set this per user request\n",
    "print(\"Query:\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = Path(r'data/temp_chunk_0-91_kona.json')\n",
    "doc = DoclingDocument.load_from_json(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "faf1ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.types.doc.document import SectionHeaderItem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def extract_header_texts(doc) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract all section header texts from a DoclingDocument with indices and pages.\"\"\"\n",
    "    headers = []\n",
    "    for i, item in enumerate(doc.texts):\n",
    "        if isinstance(item, SectionHeaderItem):\n",
    "            page = None\n",
    "            if hasattr(item, 'prov') and item.prov:\n",
    "                for p in item.prov:\n",
    "                    pg = getattr(p, 'page_no', None)\n",
    "                    if pg is not None:\n",
    "                        page = int(pg)\n",
    "                        break\n",
    "            headers.append({\n",
    "                'text': item.text.strip(),\n",
    "                'index': i,\n",
    "                'page': page\n",
    "            })\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aeb67276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lda_on_headers(header_data: List[Dict[str, Any]], n_topics: int = 10, max_iter: int = 10, random_state: int = 42) -> Dict[str, Any]:\n",
    "    \"\"\"Perform LDA topic modeling on header texts.\n",
    "    \n",
    "    Args:\n",
    "        header_data: List of dicts with 'text', 'index', 'page'.\n",
    "        n_topics: Number of topics to extract.\n",
    "        max_iter: Maximum iterations for LDA.\n",
    "        random_state: Random state for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'topics' (list of dicts with topic_id, top_words, top_headers), 'topic_distributions', and 'vectorizer'.\n",
    "    \"\"\"\n",
    "    if not header_data:\n",
    "        return {\"topics\": [], \"topic_distributions\": np.array([]), \"vectorizer\": None}\n",
    "    \n",
    "    header_texts = [h['text'] for h in header_data]\n",
    "    \n",
    "    # Vectorize\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "    tfidf_matrix = vectorizer.fit_transform(header_texts)\n",
    "    \n",
    "    # Fit LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=max_iter, random_state=random_state)\n",
    "    topic_distributions = lda.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Get top words per topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]  # Top 10 words\n",
    "        \n",
    "        # Get top 5 headers for this topic\n",
    "        topic_probs = topic_distributions[:, topic_idx]\n",
    "        top_header_indices = topic_probs.argsort()[-5:][::-1]\n",
    "        top_headers = []\n",
    "        for idx in top_header_indices:\n",
    "            header = header_data[idx]\n",
    "            top_headers.append({\n",
    "                'text': header['text'],\n",
    "                'page': header['page'],\n",
    "                'probability': topic_probs[idx]\n",
    "            })\n",
    "        \n",
    "        topics.append({\n",
    "            \"topic_id\": topic_idx, \n",
    "            \"top_words\": top_words,\n",
    "            \"top_headers\": top_headers\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"topics\": topics,\n",
    "        \"topic_distributions\": topic_distributions,\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"lda_model\": lda\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5eadd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hdbscan_on_headers(header_data: List[Dict[str, Any]], min_cluster_size: int = 5, min_samples: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"Perform HDBSCAN clustering on header texts using TF-IDF vectors with filtering of generic terms.\n",
    "\n",
    "    Args:\n",
    "        header_data: List of dicts with 'text', 'index', 'page'.\n",
    "        min_cluster_size: Minimum size of clusters.\n",
    "        min_samples: Minimum samples in neighborhood.\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'labels', 'probabilities', 'cluster_info', 'cluster_names', 'cluster_headers', and 'vectorizer'.\n",
    "    \"\"\"\n",
    "    if not header_data:\n",
    "        return {\"labels\": [], \"probabilities\": [], \"cluster_info\": {}, \"cluster_names\": {}, \"cluster_headers\": {}, \"vectorizer\": None}\n",
    "\n",
    "    # Filter out generic/irrelevant headers\n",
    "    generic_terms = {\n",
    "        'information', 'caution', 'warning', 'note', 'notice', 'important',\n",
    "        'the', 'illustration',\n",
    "        'shape', 'differ', 'actual', 'may', 'from', 'the', 'and', 'or', 'but',\n",
    "        'if', 'when', 'where', 'how', 'what', 'why', 'which', 'who', 'that',\n",
    "        'this', 'these', 'those', 'here', 'there', 'then', 'now', 'always',\n",
    "        'never', 'sometimes', 'often', 'usually', 'generally', 'specifically',\n",
    "        'particularly', 'especially', 'mainly', 'primarily', 'basically', 'your'\n",
    "    }\n",
    "\n",
    "    filtered_header_data = []\n",
    "    for header in header_data:\n",
    "        text_lower = header['text'].lower().strip()\n",
    "\n",
    "        # Skip headers that are just generic terms\n",
    "        if text_lower in generic_terms:\n",
    "            continue\n",
    "\n",
    "        # Skip headers that contain mostly generic terms\n",
    "        words = text_lower.split()\n",
    "        if len(words) <= 2:  # Very short headers\n",
    "            if any(word in generic_terms for word in words):\n",
    "                continue\n",
    "\n",
    "        # Skip headers that are too generic (contain only stop words or generic terms)\n",
    "        import re\n",
    "        meaningful_words = [re.sub(r'[^\\w\\s]', '', word) for word in words if re.sub(r'[^\\w\\s]', '', word) not in generic_terms and len(re.sub(r'[^\\w\\s]', '', word)) > 2]\n",
    "        if len(meaningful_words) < 1:\n",
    "            continue\n",
    "\n",
    "        filtered_header_data.append(header)\n",
    "\n",
    "    print(f\"Filtered {len(header_data) - len(filtered_header_data)} generic headers. Remaining: {len(filtered_header_data)}\")\n",
    "\n",
    "    if not filtered_header_data:\n",
    "        return {\"labels\": [], \"probabilities\": [], \"cluster_info\": {}, \"cluster_names\": {}, \"cluster_headers\": {}, \"vectorizer\": None}\n",
    "\n",
    "    header_texts = [h['text'] for h in filtered_header_data]\n",
    "\n",
    "    # Enhanced stop words for clustering\n",
    "    custom_stop_words = [\n",
    "        'information', 'caution', 'warning', 'note', 'notice', 'important',\n",
    "        'the', 'and', 'or', 'but', 'if', 'when', 'where', 'how', 'what', 'why',\n",
    "        'which', 'who', 'that', 'this', 'these', 'those', 'here', 'there',\n",
    "        'then', 'now', 'always', 'never', 'sometimes', 'often', 'usually',\n",
    "        'generally', 'specifically', 'particularly', 'especially', 'mainly',\n",
    "        'primarily', 'basically', 'may', 'can', 'will', 'should', 'would',\n",
    "        'could', 'might', 'must', 'shall', 'do', 'does', 'did', 'doing',\n",
    "        'done', 'have', 'has', 'had', 'having', 'be', 'is', 'am', 'are',\n",
    "        'was', 'were', 'being', 'been', 'to', 'of', 'in', 'on', 'at', 'by',\n",
    "        'for', 'with', 'as', 'from', 'into', 'through', 'during', 'before',\n",
    "        'after', 'above', 'below', 'between', 'among', 'within', 'without',\n",
    "        'against', 'along', 'around', 'behind', 'beside', 'besides', 'beyond',\n",
    "        'inside', 'outside', 'under', 'over', 'across', 'throughout', 'towards',\n",
    "        'shape', 'differ', 'illustration', 'actual', 'your', 'warmers'\n",
    "    ]\n",
    "\n",
    "    # Vectorize with enhanced stop words\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        ngram_range=(1, 2),  # Include bigrams for better context\n",
    "        token_pattern=r'(?u)\\b[a-zA-Z]{3,}\\b'  # Only words with 3+ characters\n",
    "    )\n",
    "\n",
    "    # Add custom stop words\n",
    "    if vectorizer.stop_words:\n",
    "        combined_stop_words = list(vectorizer.stop_words) + custom_stop_words\n",
    "    else:\n",
    "        combined_stop_words = custom_stop_words\n",
    "\n",
    "    vectorizer.set_params(stop_words=combined_stop_words)\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(header_texts)\n",
    "\n",
    "    # Convert to dense array for HDBSCAN\n",
    "    if sparse.issparse(tfidf_matrix):\n",
    "        tfidf_matrix = tfidf_matrix.toarray()\n",
    "\n",
    "    # Fit HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "    labels = clusterer.fit_predict(tfidf_matrix)\n",
    "    probabilities = clusterer.probabilities_\n",
    "\n",
    "    # Cluster info\n",
    "    unique_labels = set(labels)\n",
    "    cluster_info = {}\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            cluster_info[\"noise\"] = sum(labels == label)\n",
    "        else:\n",
    "            cluster_info[f\"cluster_{label}\"] = sum(labels == label)\n",
    "\n",
    "    # Name clusters by top words (excluding generic terms)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Filter feature names to remove generic terms\n",
    "    import re\n",
    "    meaningful_features = [f for f in feature_names if re.sub(r'[^\\w\\s]', '', f.lower()) not in generic_terms and len(re.sub(r'[^\\w\\s]', '', f)) > 2]\n",
    "\n",
    "    cluster_names = {}\n",
    "    cluster_headers = {}\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            cluster_names[\"noise\"] = \"Noise/Unclustered\"\n",
    "            cluster_headers[\"noise\"] = []\n",
    "        else:\n",
    "            # Get indices of headers in this cluster\n",
    "            cluster_indices = [i for i, l in enumerate(labels) if l == label]\n",
    "            cluster_headers_list = []\n",
    "            if cluster_indices:\n",
    "                # Average TF-IDF vectors for the cluster\n",
    "                cluster_vectors = tfidf_matrix[cluster_indices]\n",
    "                avg_vector = np.mean(cluster_vectors, axis=0)\n",
    "\n",
    "                # Get top meaningful words\n",
    "                # Map back to original feature indices\n",
    "                feature_indices = [i for i, f in enumerate(feature_names) if f in meaningful_features]\n",
    "                if feature_indices:\n",
    "                    meaningful_scores = avg_vector[feature_indices]\n",
    "                    top_indices = meaningful_scores.argsort()[-5:][::-1]\n",
    "                    top_words = [meaningful_features[i] for i in top_indices]\n",
    "                    cluster_names[f\"cluster_{label}\"] = \", \".join(top_words)\n",
    "                else:\n",
    "                    cluster_names[f\"cluster_{label}\"] = \"Generic Cluster\"\n",
    "\n",
    "                # Get headers in cluster\n",
    "                for idx in cluster_indices:\n",
    "                    header = filtered_header_data[idx]\n",
    "                    cluster_headers_list.append({\n",
    "                        'text': header['text'],\n",
    "                        'page': header['page'],\n",
    "                        'probability': probabilities[idx] if idx < len(probabilities) else None\n",
    "                    })\n",
    "            else:\n",
    "                cluster_names[f\"cluster_{label}\"] = \"Empty Cluster\"\n",
    "                cluster_headers_list = []\n",
    "            cluster_headers[f\"cluster_{label}\"] = cluster_headers_list\n",
    "\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "        \"probabilities\": probabilities,\n",
    "        \"cluster_info\": cluster_info,\n",
    "        \"cluster_names\": cluster_names,\n",
    "        \"cluster_headers\": cluster_headers,\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"clusterer\": clusterer,\n",
    "        \"filtered_headers\": filtered_header_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3ac33a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 261 headers.\n",
      "\n",
      "LDA Topics:\n",
      "Topic 0: consumer, safety, vehicle, information, reporting, defects, ventilation, support, srs, use\n",
      "  Top headers:\n",
      "    - 'Consumer Information' (page 40, prob: 0.353)\n",
      "    - '2.  Vehicle Information, Consumer Information and Reporting Safety Defects' (page 19, prob: 0.129)\n",
      "    - 'TWO-WAY RADIO OR CELLULAR' (page 2, prob: 0.033)\n",
      "    - 'Do not lie down' (page 66, prob: 0.033)\n",
      "    - 'Tires And Wheels' (page 32, prob: 0.033)\n",
      "\n",
      "Topic 1: genuine, parts, hyundai, light, srs, warning, belt, seat, ventilation, vehicle\n",
      "  Top headers:\n",
      "    - '2. Why HYUNDAI Genuine Parts?' (page 10, prob: 0.646)\n",
      "    - 'Guide To HYUNDAI Genuine Parts' (page 9, prob: 0.646)\n",
      "    - '3. How can you tell if you are purchasing HYUNDAI Genuine Parts?' (page 10, prob: 0.646)\n",
      "    - 'SRS warning light' (page 85, prob: 0.266)\n",
      "    - 'Seat belt warning light' (page 59, prob: 0.259)\n",
      "\n",
      "Topic 2: label, number, adjusting, height, locking, recommended, vehicle, engine, air, ventilation\n",
      "  Top headers:\n",
      "    - 'Recommended SAE viscosity number' (page 36, prob: 0.600)\n",
      "    - 'Adjusting the height up and down' (page 53, prob: 0.599)\n",
      "    - 'Adjusting the height up and down' (page 55, prob: 0.599)\n",
      "    - 'Tire Specification And Pressure Label' (page 38, prob: 0.517)\n",
      "    - 'Automatic locking mode' (page 74, prob: 0.517)\n",
      "\n",
      "Topic 3: hyundai, modifications, owner, vehicle, caution, use, tether, support, ventilation, warning\n",
      "  Top headers:\n",
      "    - 'HYUNDAI Motor America' (page 8, prob: 0.517)\n",
      "    - 'HYUNDAI VEHICLE OWNER PRIVACY POLICY' (page 4, prob: 0.221)\n",
      "    - 'CAUTION: MODIFICATIONS TO YOUR HYUNDAI' (page 2, prob: 0.215)\n",
      "    - 'Do not lie down' (page 66, prob: 0.033)\n",
      "    - 'Tires And Wheels' (page 32, prob: 0.033)\n",
      "\n",
      "Topic 4: rear, children, seats, control, containing, gasoline, reclining, point, tether, seat\n",
      "  Top headers:\n",
      "    - 'Gasoline containing MMT' (page 14, prob: 0.600)\n",
      "    - 'Gasoline containing alcohol or methanol' (page 13, prob: 0.600)\n",
      "    - 'Rear seats' (page 50, prob: 0.599)\n",
      "    - 'Folding the rear seats' (page 50, prob: 0.599)\n",
      "    - 'Children always in the rear' (page 67, prob: 0.599)\n",
      "\n",
      "Topic 5: belts, seat, angle, seatback, warmers, adjustment, passenger, rear, point, locking\n",
      "  Top headers:\n",
      "    - 'Seatback angle adjustment' (page 49, prob: 0.645)\n",
      "    - 'Seatback angle' (page 48, prob: 0.598)\n",
      "    - 'When to replace seat belts' (page 67, prob: 0.594)\n",
      "    - 'Care of seat belts' (page 67, prob: 0.594)\n",
      "    - 'Seat Belts' (page 58, prob: 0.594)\n",
      "\n",
      "Topic 6: belt, seat, use, release, remove, point, passenger, driver, children, warning\n",
      "  Top headers:\n",
      "    - 'Seat belt use during pregnancy' (page 65, prob: 0.641)\n",
      "    - 'Seat belt use and injured people' (page 66, prob: 0.641)\n",
      "    - 'To release your seat belt:' (page 61, prob: 0.638)\n",
      "    - 'To release your seat belt:' (page 62, prob: 0.638)\n",
      "    - 'To fasten your seat belt:' (page 62, prob: 0.599)\n",
      "\n",
      "Topic 7: anchors, latch, tether, children, ventilation, vehicle, use, support, srs, warning\n",
      "  Top headers:\n",
      "    - 'Lower Anchors and Tether for Children (LATCH System)' (page 71, prob: 0.677)\n",
      "    - 'TWO-WAY RADIO OR CELLULAR' (page 2, prob: 0.033)\n",
      "    - 'if equipped' (page 56, prob: 0.033)\n",
      "    - 'Tires And Wheels' (page 32, prob: 0.033)\n",
      "    - 'Smartstream G2.0 ATKINSON' (page 28, prob: 0.033)\n",
      "\n",
      "Topic 8: operation, restraints, head, recommended, seat, condition, occupant, classification, passenger, rear\n",
      "  Top headers:\n",
      "    - 'Condition and operation in the front passenger Occupant Classification System' (page 87, prob: 0.701)\n",
      "    - 'Front seat head restraints' (page 53, prob: 0.640)\n",
      "    - 'Operation in foreign countries' (page 14, prob: 0.517)\n",
      "    - 'Recommended Lubricants And Capacities' (page 34, prob: 0.517)\n",
      "    - 'Rear seat head restraints' (page 55, prob: 0.516)\n",
      "\n",
      "Topic 9: air, ocs, ventilation, seats, label, occupant, classification, tether, view, vehicle\n",
      "  Top headers:\n",
      "    - 'Air Ventilation Seats' (page 57, prob: 0.645)\n",
      "    - 'Front air ventilation seats' (page 57, prob: 0.645)\n",
      "    - 'Air Conditioning System' (page 33, prob: 0.517)\n",
      "    - 'Proper seated position for OCS' (page 90, prob: 0.517)\n",
      "    - 'Air Conditioner Compressor Label' (page 39, prob: 0.300)\n",
      "\n",
      "Topic 10: caution, seats, condition, warmers, modifications, safety, vehicle, hyundai, ventilation, view\n",
      "  Top headers:\n",
      "    - 'Seats Warmers' (page 56, prob: 0.597)\n",
      "    - 'CAUTION' (page 32, prob: 0.517)\n",
      "    - 'CAUTION' (page 63, prob: 0.517)\n",
      "    - 'CAUTION' (page 70, prob: 0.517)\n",
      "    - 'CAUTION' (page 12, prob: 0.517)\n",
      "\n",
      "Topic 11: overview, view, exterior, retractor, point, locking, rear, driver, passenger, seat\n",
      "  Top headers:\n",
      "    - 'Exterior Overview (Rear View)' (page 21, prob: 0.676)\n",
      "    - 'Exterior Overview (Front View)' (page 20, prob: 0.646)\n",
      "    - 'Driver's seat belt - 3-point system with emergency locking retractor' (page 60, prob: 0.546)\n",
      "    - 'Interior Overview' (page 22, prob: 0.517)\n",
      "    - 'Passenger and rear seat belts 3-point system with convertible locking retractor' (page 61, prob: 0.458)\n",
      "\n",
      "Topic 12: warning, warmers, view, ventilation, vehicle, use, tether, support, srs, shape\n",
      "  Top headers:\n",
      "    - 'TWO-WAY RADIO OR CELLULAR' (page 2, prob: 0.033)\n",
      "    - 'if equipped' (page 56, prob: 0.033)\n",
      "    - 'Never drink or take drugs and drive' (page 43, prob: 0.033)\n",
      "    - 'Smartstream G2.0 ATKINSON' (page 28, prob: 0.033)\n",
      "    - 'Tires And Wheels' (page 32, prob: 0.033)\n",
      "\n",
      "Topic 13: center, fuel, point, rear, overview, belt, seat, tether, vehicle, use\n",
      "  Top headers:\n",
      "    - 'Fuel Requirements' (page 13, prob: 0.517)\n",
      "    - 'Center Console Overview' (page 24, prob: 0.331)\n",
      "    - 'Rear center seat belt (3-point rear center seat belt)' (page 62, prob: 0.308)\n",
      "    - 'Tires And Wheels' (page 32, prob: 0.033)\n",
      "    - 'Periodic inspection' (page 67, prob: 0.033)\n",
      "\n",
      "Topic 14: engine, install, passenger, child, restraint, seat, tether, number, ventilation, vehicle\n",
      "  Top headers:\n",
      "    - 'Do not install a Child Restraint System on the Front Passenger's Seat' (page 91, prob: 0.697)\n",
      "    - 'Engine' (page 30, prob: 0.517)\n",
      "    - 'To install the tether anchor:' (page 73, prob: 0.321)\n",
      "    - 'Engine Number' (page 38, prob: 0.295)\n",
      "    - 'Do not lie down' (page 66, prob: 0.033)\n",
      "\n",
      "Topic 15: restraint, child, forward, rearward, tether, securing, raise, facing, adjustment, head\n",
      "  Top headers:\n",
      "    - 'Securing a Child Restraint System with the 'LATCH Anchors System'' (page 72, prob: 0.698)\n",
      "    - 'Rearward-facing Child Restraint System' (page 69, prob: 0.675)\n",
      "    - 'Forward-facing Child Restraint System' (page 69, prob: 0.675)\n",
      "    - 'Forward and rearward adjustment' (page 48, prob: 0.646)\n",
      "    - 'Forward and rearward adjustment' (page 47, prob: 0.646)\n",
      "\n",
      "Topic 16: seatback, recline, rear, reclining, vehicle, use, tether, support, warning, warmers\n",
      "  Top headers:\n",
      "    - 'To recline the seatback:' (page 49, prob: 0.598)\n",
      "    - 'To recline the seatback:' (page 50, prob: 0.598)\n",
      "    - 'To recline the seatback:' (page 48, prob: 0.598)\n",
      "    - 'Seatback pocket' (page 49, prob: 0.517)\n",
      "    - 'To unfold the rear seatback:' (page 51, prob: 0.319)\n",
      "\n",
      "Topic 17: restraint, head, reinstall, removal, remove, airbag, belt, seat, warning, vehicle\n",
      "  Top headers:\n",
      "    - 'To reinstall the head restraint:' (page 55, prob: 0.643)\n",
      "    - 'To reinstall the head restraint:' (page 54, prob: 0.643)\n",
      "    - 'Removal/Reinstall' (page 54, prob: 0.600)\n",
      "    - 'Head restraint' (page 52, prob: 0.599)\n",
      "    - 'Removal/Reinstallation' (page 55, prob: 0.517)\n",
      "\n",
      "Topic 18: compartment, engine, owner, vehicle, manual, overview, differ, actual, illustration, hyundai\n",
      "  Top headers:\n",
      "    - 'OWNER'S MANUAL' (page 1, prob: 0.600)\n",
      "    - 'Engine Compartment Overview' (page 27, prob: 0.445)\n",
      "    - 'HYUNDAI VEHICLE OWNER PRIVACY POLICY' (page 4, prob: 0.436)\n",
      "    - 'The actual engine compartment in the vehicle may differ from the illustration.' (page 28, prob: 0.365)\n",
      "    - 'The actual engine compartment in the vehicle may differ from the illustration.' (page 27, prob: 0.365)\n",
      "\n",
      "Topic 19: support, lumbar, view, warning, ventilation, vehicle, use, tether, srs, shape\n",
      "  Top headers:\n",
      "    - 'Lumbar support' (page 49, prob: 0.600)\n",
      "    - 'To adjust the lumbar support:' (page 49, prob: 0.600)\n",
      "    - 'TWO-WAY RADIO OR CELLULAR' (page 2, prob: 0.033)\n",
      "    - 'Do not lie down' (page 66, prob: 0.033)\n",
      "    - 'Periodic inspection' (page 67, prob: 0.033)\n",
      "\n",
      "Topic 20: airbags, passenger, danger, driver, seat, rear, warning, belt, point, locking\n",
      "  Top headers:\n",
      "    - 'Driver's and passenger's front airbags' (page 79, prob: 0.646)\n",
      "    - 'Airbags' (page 45, prob: 0.517)\n",
      "    - 'Curtain airbags' (page 81, prob: 0.517)\n",
      "    - 'Side airbags' (page 80, prob: 0.517)\n",
      "    - 'Where are the airbags?' (page 79, prob: 0.517)\n",
      "\n",
      "Topic 21: airbag, safety, precautions, collision, belt, restraint, seat, seats, vehicle, warning\n",
      "  Top headers:\n",
      "    - 'AIRBAG SAFETY PRECAUTIONS' (page 77, prob: 0.646)\n",
      "    - 'Important Safety Precautions' (page 42, prob: 0.599)\n",
      "    - 'Safety precautions' (page 45, prob: 0.599)\n",
      "    - 'Why didn't my airbag go off in a collision?' (page 91, prob: 0.598)\n",
      "    - 'Airbag collision sensors' (page 91, prob: 0.598)\n",
      "\n",
      "Topic 22: warning, light, srs, passenger, driver, safety, belt, seat, vehicle, rear\n",
      "  Top headers:\n",
      "    - 'WARNING' (page 13, prob: 0.517)\n",
      "    - 'WARNING' (page 77, prob: 0.517)\n",
      "    - 'WARNING' (page 79, prob: 0.517)\n",
      "    - 'WARNING' (page 80, prob: 0.517)\n",
      "    - 'WARNING' (page 81, prob: 0.517)\n",
      "\n",
      "Topic 23: introduction, controls, seat, manual, vehicle, use, tether, support, srs, warning\n",
      "  Top headers:\n",
      "    - 'Power seat controls' (page 48, prob: 0.590)\n",
      "    - '1.  Introduction' (page 7, prob: 0.517)\n",
      "    - 'Introduction' (page 8, prob: 0.517)\n",
      "    - 'Manual seat controls' (page 47, prob: 0.401)\n",
      "    - 'Periodic inspection' (page 67, prob: 0.033)\n",
      "\n",
      "Topic 24: information, consumer, safety, vehicle, reporting, defects, ventilation, support, srs, use\n",
      "  Top headers:\n",
      "    - 'Information' (page 45, prob: 0.517)\n",
      "    - 'Information' (page 59, prob: 0.517)\n",
      "    - 'Information' (page 45, prob: 0.517)\n",
      "    - 'Information' (page 62, prob: 0.517)\n",
      "    - 'Information' (page 62, prob: 0.517)\n",
      "\n",
      "Topic 25: height, driver, seat, srs, cushion, adjustment, reporting, defects, safety, components\n",
      "  Top headers:\n",
      "    - 'Seat cushion tilt/Seat height adjustment' (page 49, prob: 0.677)\n",
      "    - 'Reporting Safety Defects' (page 40, prob: 0.645)\n",
      "    - 'To change the height of the seat cushion:' (page 48, prob: 0.640)\n",
      "    - 'SRS components' (page 78, prob: 0.600)\n",
      "    - 'Height adjustment' (page 61, prob: 0.600)\n",
      "\n",
      "Topic 26: notice, warning, view, warmers, vehicle, use, tether, support, srs, shape\n",
      "  Top headers:\n",
      "    - 'NOTICE' (page 13, prob: 0.517)\n",
      "    - 'NOTICE' (page 12, prob: 0.517)\n",
      "    - 'NOTICE' (page 32, prob: 0.517)\n",
      "    - 'NOTICE' (page 64, prob: 0.517)\n",
      "    - 'NOTICE' (page 36, prob: 0.517)\n",
      "\n",
      "Topic 27: reclining, occupant, classification, modifications, components, ocs, seats, hyundai, seatback, caution\n",
      "  Top headers:\n",
      "    - 'Main components of the Occupant Classification System' (page 86, prob: 0.646)\n",
      "    - 'Occupant Classification System (OCS)' (page 85, prob: 0.427)\n",
      "    - 'Reclining seatback' (page 47, prob: 0.342)\n",
      "    - 'Reclining the rear seats' (page 50, prob: 0.267)\n",
      "    - 'CAUTION: MODIFICATIONS TO YOUR HYUNDAI' (page 2, prob: 0.252)\n",
      "\n",
      "Topic 28: vehicle, manual, additives, fuel, detergent, modifications, use, condition, number, safety\n",
      "  Top headers:\n",
      "    - 'Detergent Fuel Additives' (page 14, prob: 0.646)\n",
      "    - 'Using Fuel Additives (except Detergent Fuel Additives)' (page 14, prob: 0.638)\n",
      "    - 'How To Use This Manual' (page 11, prob: 0.600)\n",
      "    - 'Vehicle Modifications' (page 15, prob: 0.596)\n",
      "    - 'Vehicle Weight And Luggage Volume' (page 33, prob: 0.517)\n",
      "\n",
      "Topic 29: differ, actual, illustration, crs, child, shape, restraint, vehicle, engine, compartment\n",
      "  Top headers:\n",
      "    - 'The actual shape may differ from the illustration.' (page 21, prob: 0.678)\n",
      "    - 'The actual shape may differ from the illustration.' (page 22, prob: 0.678)\n",
      "    - 'The actual shape may differ from the illustration.' (page 24, prob: 0.678)\n",
      "    - 'The actual shape may differ from the illustration.' (page 20, prob: 0.678)\n",
      "    - 'Child Restraint System (CRS)' (page 68, prob: 0.644)\n",
      "\n",
      "Filtered 90 generic headers. Remaining: 171\n",
      "\n",
      "HDBSCAN Clusters:\n",
      "Cluster info: {'cluster_0': np.int64(4), 'cluster_1': np.int64(7), 'cluster_2': np.int64(4), 'cluster_3': np.int64(3), 'cluster_4': np.int64(4), 'cluster_5': np.int64(3), 'cluster_6': np.int64(3), 'cluster_7': np.int64(3), 'cluster_8': np.int64(4), 'cluster_9': np.int64(4), 'cluster_10': np.int64(3), 'cluster_11': np.int64(4), 'cluster_12': np.int64(3), 'cluster_13': np.int64(11), 'cluster_14': np.int64(5), 'cluster_15': np.int64(3), 'cluster_16': np.int64(6), 'cluster_17': np.int64(3), 'cluster_18': np.int64(6), 'cluster_19': np.int64(4), 'cluster_20': np.int64(3), 'cluster_21': np.int64(5), 'cluster_22': np.int64(3), 'cluster_23': np.int64(3), 'cluster_24': np.int64(3), 'cluster_25': np.int64(6), 'cluster_26': np.int64(5), 'cluster_27': np.int64(3), 'cluster_28': np.int64(5), 'noise': np.int64(48)}\n",
      "Labels: [-1 17 13 25 -1 -1 13 -1 -1 17]...\n",
      "\n",
      "Cluster Names and Headers:\n",
      "cluster_0: seat belts, belts, seat, ventilation seats, view (4 headers)\n",
      "  - 'Seat belts' (page 46)\n",
      "  - 'Seat Belts' (page 58)\n",
      "  - 'Care of seat belts' (page 67)\n",
      "  - 'When to replace seat belts' (page 67)\n",
      "\n",
      "cluster_1: head restraint, head, restraint, reinstall head, raise head (7 headers)\n",
      "  - 'Head restraint' (page 52)\n",
      "  - 'To raise the head restraint:' (page 53)\n",
      "  - 'To remove the head restraint:' (page 54)\n",
      "  - 'To reinstall the head restraint:' (page 54)\n",
      "  - 'To raise the head restraint:' (page 55)\n",
      "\n",
      "cluster_2: overview, control, center, ventilation seats, view (4 headers)\n",
      "  - 'Interior Overview' (page 22)\n",
      "  - 'Center Console Overview' (page 24)\n",
      "  - 'Steering Wheel Control Overview' (page 26)\n",
      "  - 'Control your speed' (page 43)\n",
      "\n",
      "cluster_3: compartment, engine compartment, engine, compartment vehicle, vehicle (3 headers)\n",
      "  - 'Engine Compartment Overview' (page 27)\n",
      "  - 'The actual engine compartment in the vehicle may differ from the illustration.' (page 27)\n",
      "  - 'The actual engine compartment in the vehicle may differ from the illustration.' (page 28)\n",
      "\n",
      "cluster_4: seat, front seat, rear seat, front, restraints (4 headers)\n",
      "  - 'Rear seat [C]' (page 44)\n",
      "  - 'Front seat head restraints' (page 53)\n",
      "  - 'Rear seat head restraints' (page 55)\n",
      "  - 'Front seat warmers' (page 56)\n",
      "\n",
      "cluster_5: fuel, fuel additives, additives, detergent, detergent fuel (3 headers)\n",
      "  - 'Fuel Requirements' (page 13)\n",
      "  - 'Using Fuel Additives (except Detergent Fuel Additives)' (page 14)\n",
      "  - 'Detergent Fuel Additives' (page 14)\n",
      "\n",
      "cluster_6: parts, hyundai genuine, genuine parts, genuine, hyundai (3 headers)\n",
      "  - 'Guide To HYUNDAI Genuine Parts' (page 9)\n",
      "  - '2. Why HYUNDAI Genuine Parts?' (page 10)\n",
      "  - '3. How can you tell if you are purchasing HYUNDAI Genuine Parts?' (page 10)\n",
      "\n",
      "cluster_7: occupant classification, classification system, classification, occupant, system (3 headers)\n",
      "  - 'Occupant Classification System (OCS)' (page 85)\n",
      "  - 'Main components of the Occupant Classification System' (page 86)\n",
      "  - 'Condition and operation in the front passenger Occupant Classification System' (page 87)\n",
      "\n",
      "cluster_8: seatback, rear seatback, rear, reclining, down (4 headers)\n",
      "  - 'Reclining seatback' (page 47)\n",
      "  - 'Seatback pocket' (page 49)\n",
      "  - 'To fold down the rear seatback:' (page 50)\n",
      "  - 'To unfold the rear seatback:' (page 51)\n",
      "\n",
      "cluster_9: children, rear, ventilation, ventilation seats, view (4 headers)\n",
      "  - 'Restrain all children' (page 42)\n",
      "  - 'Infant and small children' (page 65)\n",
      "  - 'Larger children' (page 66)\n",
      "  - 'Children always in the rear' (page 67)\n",
      "\n",
      "cluster_10: recline, recline seatback, seatback, ventilation seats, view (3 headers)\n",
      "  - 'To recline the seatback:' (page 48)\n",
      "  - 'To recline the seatback:' (page 49)\n",
      "  - 'To recline the seatback:' (page 50)\n",
      "\n",
      "cluster_11: height, seat height, seat, height adjustment, seat cushion (4 headers)\n",
      "  - 'Seat height' (page 48)\n",
      "  - 'To change the height of the seat cushion:' (page 48)\n",
      "  - 'Seat cushion tilt/Seat height adjustment' (page 49)\n",
      "  - 'Height adjustment' (page 61)\n",
      "\n",
      "cluster_12: forward rearward, rearward, forward, rearward adjustment, adjustment (3 headers)\n",
      "  - 'Forward and rearward adjustment' (page 47)\n",
      "  - 'Forward and rearward adjustment' (page 48)\n",
      "  - 'To move the seat forward or rearward:' (page 48)\n",
      "\n",
      "cluster_13: view, ventilation seats, ventilation, vehicle, use (11 headers)\n",
      "  - 'TWO-WAY RADIO OR CELLULAR' (page 2)\n",
      "  - 'Table of contents' (page 5)\n",
      "  - 'CALIFORNIA PROPOSITION 65 WARNING' (page 16)\n",
      "  - 'Smartstream G2.0 ATKINSON' (page 28)\n",
      "  - 'Dimensions' (page 29)\n",
      "\n",
      "cluster_14: safety precautions, precautions, safety, belt safety, seat belt (5 headers)\n",
      "  - 'Important Safety Precautions' (page 42)\n",
      "  - 'Safety precautions' (page 45)\n",
      "  - 'Seat belt safety precautions' (page 58)\n",
      "  - 'Additional seat belt safety precautions' (page 65)\n",
      "  - 'AIRBAG SAFETY PRECAUTIONS' (page 77)\n",
      "\n",
      "cluster_15: rear seats, seats, rear, reclining, view (3 headers)\n",
      "  - 'Rear seats' (page 50)\n",
      "  - 'Reclining the rear seats' (page 50)\n",
      "  - 'Folding the rear seats' (page 50)\n",
      "\n",
      "cluster_16: airbags, system, ventilation seats, vehicle, ventilation (6 headers)\n",
      "  - 'Infotainment system' (page 45)\n",
      "  - 'Airbags' (page 45)\n",
      "  - 'Where are the airbags?' (page 79)\n",
      "  - 'Side airbags' (page 80)\n",
      "  - 'Curtain airbags' (page 81)\n",
      "\n",
      "cluster_17: hyundai, modifications, vehicle, view, ventilation seats (3 headers)\n",
      "  - 'CAUTION: MODIFICATIONS TO YOUR HYUNDAI' (page 2)\n",
      "  - 'HYUNDAI Motor America' (page 8)\n",
      "  - 'Vehicle Modifications' (page 15)\n",
      "\n",
      "cluster_18: airbag, collision, restraint system, restraint, system (6 headers)\n",
      "  - 'Airbag hazards' (page 42)\n",
      "  - 'Airbag - Supplemental Restraint System' (page 76)\n",
      "  - 'What to expect after an airbag inflates' (page 84)\n",
      "  - 'Noise and smoke from inflating airbag' (page 85)\n",
      "  - 'Why didn't my airbag go off in a collision?' (page 91)\n",
      "\n",
      "cluster_19: passenger seat, passenger, front passenger, front, seat (4 headers)\n",
      "  - 'Front passenger's seat [B]' (page 44)\n",
      "  - 'Front passenger's seat belt warning' (page 59)\n",
      "  - 'Rear passenger's seat belt warning' (page 59)\n",
      "  - 'Do not install a Child Restraint System on the Front Passenger's Seat' (page 91)\n",
      "\n",
      "cluster_20: label, air, vehicle, view, ventilation seats (3 headers)\n",
      "  - 'Vehicle Certification Label' (page 37)\n",
      "  - 'Tire Specification And Pressure Label' (page 38)\n",
      "  - 'Air Conditioner Compressor Label' (page 39)\n",
      "\n",
      "cluster_21: child, child restraint, restraint system, crs, system crs (5 headers)\n",
      "  - 'Child Restraint System (CRS)' (page 67)\n",
      "  - 'Child Restraint System (CRS)' (page 68)\n",
      "  - 'Selecting a Child Restraint System (CRS)' (page 68)\n",
      "  - 'Child Restraint System types' (page 69)\n",
      "  - 'Installing a Child Restraint System (CRS)' (page 70)\n",
      "\n",
      "cluster_22: system, securing child, securing, child restraint, child (3 headers)\n",
      "  - 'Securing a Child Restraint System with the 'LATCH Anchors System'' (page 72)\n",
      "  - 'Securing a Child Restraint System seat with 'Top-tether Anchorage' system' (page 72)\n",
      "  - 'Securing a Child Restraint System with a lap/shoulder belt' (page 73)\n",
      "\n",
      "cluster_23: engine, number, vehicle, view, ventilation seats (3 headers)\n",
      "  - 'Engine' (page 30)\n",
      "  - 'Vehicle Identification Number (VIN)' (page 37)\n",
      "  - 'Engine Number' (page 38)\n",
      "\n",
      "cluster_24: belt use, use, seat belt, belt, seat (3 headers)\n",
      "  - 'Seat belt use during pregnancy' (page 65)\n",
      "  - 'Seat belt use and children' (page 65)\n",
      "  - 'Seat belt use and injured people' (page 66)\n",
      "\n",
      "cluster_25: vehicle, safety, view, ventilation, ventilation seats (6 headers)\n",
      "  - 'SAFETY AND VEHICLE DAMAGE WARNING' (page 3)\n",
      "  - 'Safety Messages' (page 12)\n",
      "  - 'Vehicle Handling Instructions' (page 15)\n",
      "  - 'Vehicle Break-in Process' (page 16)\n",
      "  - 'Vehicle Data Collection And Event Data Recorders' (page 17)\n",
      "\n",
      "cluster_26: seats, front, safety, system, ventilation seats (5 headers)\n",
      "  - '3.  Seats & Safety System' (page 41)\n",
      "  - 'Seats' (page 44)\n",
      "  - 'Front seats' (page 46)\n",
      "  - 'Seats Warmers' (page 56)\n",
      "  - 'Booster seats' (page 70)\n",
      "\n",
      "cluster_27: driver, driver seat, seat, seat belt, belt (3 headers)\n",
      "  - 'Driver distraction' (page 42)\n",
      "  - 'Driver's seat [A]' (page 44)\n",
      "  - 'Driver's seat belt warning' (page 59)\n",
      "\n",
      "cluster_28: seat belt, belt, seat, light, restraint system (5 headers)\n",
      "  - 'Always wear your seat belt' (page 42)\n",
      "  - 'Seat belt warning light' (page 59)\n",
      "  - 'Seat belt restraint system' (page 60)\n",
      "  - 'To fasten your seat belt:' (page 62)\n",
      "  - 'Pretensioner seat belt' (page 63)\n",
      "\n",
      "noise: Noise/Unclustered (48 headers)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dev\\Dropbox\\GitHub\\mlops\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Suppress hdbscan warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=SyntaxWarning, module=\"hdbscan\")\n",
    "\n",
    "# Experiment with LDA on headers\n",
    "header_data = extract_header_texts(doc)\n",
    "print(f\"Extracted {len(header_data)} headers.\")\n",
    "\n",
    "# LDA Experiment\n",
    "lda_results = perform_lda_on_headers(header_data, n_topics=30, max_iter=20)\n",
    "print(\"\\nLDA Topics:\")\n",
    "for topic in lda_results[\"topics\"]:\n",
    "    print(f\"Topic {topic['topic_id']}: {', '.join(topic['top_words'])}\")\n",
    "    print(\"  Top headers:\")\n",
    "    for header in topic['top_headers']:\n",
    "        print(f\"    - '{header['text']}' (page {header['page']}, prob: {header['probability']:.3f})\")\n",
    "    print()\n",
    "\n",
    "# HDBSCAN Experiment\n",
    "hdbscan_results = perform_hdbscan_on_headers(header_data, min_cluster_size=3, min_samples=1)\n",
    "print(\"\\nHDBSCAN Clusters:\")\n",
    "print(f\"Cluster info: {hdbscan_results['cluster_info']}\")\n",
    "print(f\"Labels: {hdbscan_results['labels'][:10]}...\")  # First 10 labels\n",
    "print(\"\\nCluster Names and Headers:\")\n",
    "for cluster, name in hdbscan_results[\"cluster_names\"].items():\n",
    "    count = hdbscan_results[\"cluster_info\"].get(cluster, 0)\n",
    "    print(f\"{cluster}: {name} ({count} headers)\")\n",
    "    if cluster in hdbscan_results[\"cluster_headers\"]:\n",
    "        for header in hdbscan_results[\"cluster_headers\"][cluster][:5]:  # Show first 5 headers\n",
    "            print(f\"  - '{header['text']}' (page {header['page']})\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "19ed0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing improved ranking for query: 'Where can I find the VIN number?'\n",
      "Top headers:\n",
      " 1. p  37 h1 score=0.521 | noun=1.00 verb=0.00 n_syn=1.00 v_syn=0.00 fuzz=0.42 clust=0.00 :: Vehicle Identification Number (VIN)  [pages: 37]\n",
      " 2. p  38 h1 score=0.352 | noun=0.50 verb=0.00 n_syn=1.00 v_syn=0.00 fuzz=0.53 clust=0.00 :: Engine Number  [pages: 38]\n",
      " 3. p  33 h1 score=0.040 | noun=0.00 verb=0.00 n_syn=0.00 v_syn=0.00 fuzz=0.40 clust=0.20 :: Air Conditioning System  [pages: 33]\n",
      " 4. p  20 h1 score=0.036 | noun=0.00 verb=0.00 n_syn=0.00 v_syn=0.00 fuzz=0.32 clust=0.20 :: Exterior Overview (Front View)  [pages: 20]\n",
      " 5. p  17 h1 score=0.030 | noun=0.00 verb=0.00 n_syn=0.09 v_syn=0.00 fuzz=0.33 clust=0.00 :: Vehicle Data Collection And Event Data Recorders  [pages: 17]\n",
      "\n",
      "Improved results with HDBSCAN:\n",
      "Title: Vehicle Identification Number (VIN), Page: 37, Score: 0.521, Cluster Sim: 0.000\n",
      "Title: Engine Number, Page: 38, Score: 0.352, Cluster Sim: 0.000\n",
      "Title: Air Conditioning System, Page: 33, Score: 0.040, Cluster Sim: 0.197\n",
      "Title: Exterior Overview (Front View), Page: 20, Score: 0.036, Cluster Sim: 0.197\n",
      "Title: Vehicle Data Collection And Event Data Recorders, Page: 17, Score: 0.030, Cluster Sim: 0.000\n",
      "Top headers:\n",
      " 1. p  37 h1 score=0.521 | noun=1.00 verb=0.00 n_syn=1.00 v_syn=0.00 fuzz=0.42 clust=0.00 :: Vehicle Identification Number (VIN)  [pages: 37]\n",
      " 2. p  38 h1 score=0.352 | noun=0.50 verb=0.00 n_syn=1.00 v_syn=0.00 fuzz=0.53 clust=0.00 :: Engine Number  [pages: 38]\n",
      " 3. p  33 h1 score=0.040 | noun=0.00 verb=0.00 n_syn=0.00 v_syn=0.00 fuzz=0.40 clust=0.20 :: Air Conditioning System  [pages: 33]\n",
      " 4. p  20 h1 score=0.036 | noun=0.00 verb=0.00 n_syn=0.00 v_syn=0.00 fuzz=0.32 clust=0.20 :: Exterior Overview (Front View)  [pages: 20]\n",
      " 5. p  17 h1 score=0.030 | noun=0.00 verb=0.00 n_syn=0.09 v_syn=0.00 fuzz=0.33 clust=0.00 :: Vehicle Data Collection And Event Data Recorders  [pages: 17]\n",
      "\n",
      "Improved results with HDBSCAN:\n",
      "Title: Vehicle Identification Number (VIN), Page: 37, Score: 0.521, Cluster Sim: 0.000\n",
      "Title: Engine Number, Page: 38, Score: 0.352, Cluster Sim: 0.000\n",
      "Title: Air Conditioning System, Page: 33, Score: 0.040, Cluster Sim: 0.197\n",
      "Title: Exterior Overview (Front View), Page: 20, Score: 0.036, Cluster Sim: 0.197\n",
      "Title: Vehicle Data Collection And Event Data Recorders, Page: 17, Score: 0.030, Cluster Sim: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Test improved ranking with HDBSCAN\n",
    "query = \"Where can I find the VIN number?\"\n",
    "print(f\"\\nTesting improved ranking for query: '{query}'\")\n",
    "improved_results = rank_and_save_best_section_with_hdbscan(doc, query, top_n=5, hdbscan_results=hdbscan_results)\n",
    "print(\"\\nImproved results with HDBSCAN:\")\n",
    "for r in improved_results:\n",
    "    print(f\"Title: {r['title']}, Page: {r['first_doc_page']}, Score: {r['score']:.3f}, Cluster Sim: {r['cluster_sim']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3fa52c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster_for_query(query: str, hdbscan_results: Dict[str, Any], header_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Find which cluster a query belongs to using semantic similarity with enhanced query processing.\n",
    "\n",
    "    Args:\n",
    "        query: The query string to classify.\n",
    "        hdbscan_results: Results from perform_hdbscan_on_headers containing the trained model.\n",
    "        header_data: The original header data used for clustering.\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'predicted_cluster', 'cluster_name', 'cluster_headers', 'similarity_score', and 'all_cluster_similarities'.\n",
    "    \"\"\"\n",
    "    if not hdbscan_results or not hdbscan_results.get(\"vectorizer\"):\n",
    "        return {\"error\": \"No HDBSCAN results or vectorizer found\"}\n",
    "\n",
    "    # Enhanced query processing\n",
    "    import spacy\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "    # Extract key terms from query\n",
    "    q_nouns = set()\n",
    "    qdoc = nlp(query)\n",
    "    for tok in qdoc:\n",
    "        if tok.is_stop or not tok.is_alpha:\n",
    "            continue\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if tok.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            q_nouns.add(lemma)\n",
    "\n",
    "    # Expand query with synonyms\n",
    "    expanded_terms = set()\n",
    "    for noun in q_nouns:\n",
    "        expanded_terms.add(noun)\n",
    "        for synset in wn.synsets(noun, pos=wn.NOUN):\n",
    "            for lemma in synset.lemma_names():\n",
    "                expanded_terms.add(lemma.replace(\"_\", \" \").lower())\n",
    "\n",
    "    # Create expanded query\n",
    "    expanded_query = query + \" \" + \" \".join(expanded_terms)\n",
    "    print(f\"Original query: '{query}'\")\n",
    "    print(f\"Expanded query: '{expanded_query}'\")\n",
    "\n",
    "    # Use semantic similarity instead of TF-IDF\n",
    "    cluster_names = hdbscan_results[\"cluster_names\"]\n",
    "    cluster_headers = hdbscan_results[\"cluster_headers\"]\n",
    "\n",
    "    # Get vector for expanded query\n",
    "    query_doc = nlp(expanded_query)\n",
    "\n",
    "    # Compute semantic similarities to cluster names\n",
    "    similarities = {}\n",
    "    for cluster_key, cluster_name in cluster_names.items():\n",
    "        if cluster_key != \"noise\":\n",
    "            cluster_doc = nlp(cluster_name)\n",
    "            similarity = query_doc.similarity(cluster_doc)\n",
    "            similarities[int(cluster_key.split(\"_\")[1])] = similarity\n",
    "\n",
    "    # Find the most similar cluster\n",
    "    if similarities:\n",
    "        best_cluster = max(similarities, key=similarities.get)\n",
    "        best_similarity = similarities[best_cluster]\n",
    "    else:\n",
    "        best_cluster = -1\n",
    "        best_similarity = 0.0\n",
    "\n",
    "    # Get cluster info\n",
    "    cluster_key = f\"cluster_{best_cluster}\" if best_cluster != -1 else \"noise\"\n",
    "    cluster_name = cluster_names.get(cluster_key, \"Unknown\")\n",
    "    headers_in_cluster = cluster_headers.get(cluster_key, [])\n",
    "\n",
    "    # Sort similarities\n",
    "    sorted_similarities = {cluster_names.get(f\"cluster_{k}\", f\"cluster_{k}\"): v for k, v in similarities.items()}\n",
    "    sorted_similarities = dict(sorted(sorted_similarities.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # Print formatted output\n",
    "    print(\"== Testing Cluster Prediction ===\")\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Predicted Cluster: {cluster_key} - {cluster_name}\")\n",
    "    print(f\"Similarity Score: {best_similarity:.3f}\")\n",
    "    print(f\"Headers in cluster: {len(headers_in_cluster)}\")\n",
    "    print(\"Top headers in cluster:\")\n",
    "    for header in headers_in_cluster[:5]:  # Show top 5 headers\n",
    "        page = header.get('page', 'N/A')\n",
    "        print(f\"  - '{header['text']}' (page {page})\")\n",
    "    print(\"Similarities to other clusters:\")\n",
    "    for cluster, sim in list(sorted_similarities.items())[:5]:  # Show top 5 similarities\n",
    "        print(f\"  - {cluster}: {sim:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"predicted_cluster\": cluster_key,\n",
    "        \"cluster_name\": cluster_name,\n",
    "        \"cluster_headers\": headers_in_cluster,\n",
    "        \"similarity_score\": best_similarity,\n",
    "        \"all_cluster_similarities\": sorted_similarities,\n",
    "        \"query\": query,\n",
    "        \"expanded_query\": expanded_query\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d0ef5d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: 'where can I find the VIN number'\n",
      "Expanded query: 'where can I find the VIN number figure numeral turn vin act number bit telephone number issue phone number identification number routine'\n",
      "== Testing Cluster Prediction ===\n",
      "\n",
      "Query: 'where can I find the VIN number'\n",
      "Predicted Cluster: cluster_23 - engine, number, vehicle, view, ventilation seats\n",
      "Similarity Score: 0.636\n",
      "Headers in cluster: 3\n",
      "Top headers in cluster:\n",
      "  - 'Engine' (page 30)\n",
      "  - 'Vehicle Identification Number (VIN)' (page 37)\n",
      "  - 'Engine Number' (page 38)\n",
      "Similarities to other clusters:\n",
      "  - engine, number, vehicle, view, ventilation seats: 0.636\n",
      "  - system, securing child, securing, child restraint, child: 0.588\n",
      "  - label, air, vehicle, view, ventilation seats: 0.579\n",
      "  - belt use, use, seat belt, belt, seat: 0.558\n",
      "  - overview, control, center, ventilation seats, view: 0.556\n",
      "Query belongs to cluster: engine, number, vehicle, view, ventilation seats (similarity: 0.636)\n"
     ]
    }
   ],
   "source": [
    "query = \"where can I find the VIN number\"\n",
    "result = find_cluster_for_query(query, hdbscan_results, header_data)\n",
    "print(f\"Query belongs to cluster: {result['cluster_name']} (similarity: {result['similarity_score']:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
